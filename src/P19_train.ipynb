{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170499072it [00:56, 3043713.77it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\cifar-10-python.tar.gz to ../data\n",
      "Files already downloaded and verified\n",
      "训练集的长度：50000\n",
      "测试集的长度：10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(root='../data',train=True,transform=torchvision.transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(root='../data',train=False,transform=torchvision.transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "\n",
    "# ----> len dataset\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "print(\"训练集的长度：{}\".format(train_data_size))\n",
    "print(\"测试集的长度：{}\".format(test_data_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data,batch_size=64)\n",
    "test_dataloader = DataLoader(test_data,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FanNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FanNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, padding=2)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 5, padding=2)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        self.maxpool3 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(1024, 64)\n",
    "        self.linear2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fanfan = FanNet()\n",
    "    input = torch.ones((64,3,32,32))\n",
    "    output = fanfan(input)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------第0轮训练开始----------\n",
      "训练次数：1, loss:4.677282810211182\n",
      "训练次数：2, loss:102.27971649169922\n",
      "训练次数：3, loss:32.91666793823242\n",
      "训练次数：4, loss:101.23944854736328\n",
      "训练次数：5, loss:85.64004516601562\n",
      "训练次数：6, loss:17.29160499572754\n",
      "训练次数：7, loss:34.875160217285156\n",
      "训练次数：8, loss:31.705135345458984\n",
      "训练次数：9, loss:48.421817779541016\n",
      "训练次数：10, loss:30.37312889099121\n",
      "训练次数：11, loss:52.78559494018555\n",
      "训练次数：12, loss:35.105491638183594\n",
      "训练次数：13, loss:25.28563117980957\n",
      "训练次数：14, loss:32.901119232177734\n",
      "训练次数：15, loss:193.61170959472656\n",
      "训练次数：16, loss:252.36770629882812\n",
      "训练次数：17, loss:104.58283233642578\n",
      "训练次数：18, loss:38.1594123840332\n",
      "训练次数：19, loss:20.039886474609375\n",
      "训练次数：20, loss:17.333147048950195\n",
      "训练次数：21, loss:32.87016677856445\n",
      "训练次数：22, loss:89.71215057373047\n",
      "训练次数：23, loss:32.59584426879883\n",
      "训练次数：24, loss:52.150474548339844\n",
      "训练次数：25, loss:14.243913650512695\n",
      "训练次数：26, loss:63.50657653808594\n",
      "训练次数：27, loss:18.99108123779297\n",
      "训练次数：28, loss:14.867518424987793\n",
      "训练次数：29, loss:7.814864158630371\n",
      "训练次数：30, loss:10.393378257751465\n",
      "训练次数：31, loss:15.68023681640625\n",
      "训练次数：32, loss:15.39531421661377\n",
      "训练次数：33, loss:20.277151107788086\n",
      "训练次数：34, loss:11.44466781616211\n",
      "训练次数：35, loss:8.049607276916504\n",
      "训练次数：36, loss:8.930073738098145\n",
      "训练次数：37, loss:13.745634078979492\n",
      "训练次数：38, loss:8.985060691833496\n",
      "训练次数：39, loss:14.008163452148438\n",
      "训练次数：40, loss:10.517414093017578\n",
      "训练次数：41, loss:5.577223777770996\n",
      "训练次数：42, loss:9.016529083251953\n",
      "训练次数：43, loss:6.719254016876221\n",
      "训练次数：44, loss:5.443317890167236\n",
      "训练次数：45, loss:4.731158256530762\n",
      "训练次数：46, loss:5.486793041229248\n",
      "训练次数：47, loss:7.407367706298828\n",
      "训练次数：48, loss:3.7691173553466797\n",
      "训练次数：49, loss:14.974555969238281\n",
      "训练次数：50, loss:3.8543691635131836\n",
      "训练次数：51, loss:6.374973297119141\n",
      "训练次数：52, loss:4.543893814086914\n",
      "训练次数：53, loss:3.7999045848846436\n",
      "训练次数：54, loss:4.4730095863342285\n",
      "训练次数：55, loss:3.846421003341675\n",
      "训练次数：56, loss:3.7385401725769043\n",
      "训练次数：57, loss:6.806858539581299\n",
      "训练次数：58, loss:4.765782833099365\n",
      "训练次数：59, loss:7.837489128112793\n",
      "训练次数：60, loss:6.256326198577881\n",
      "训练次数：61, loss:7.4703216552734375\n",
      "训练次数：62, loss:6.549781799316406\n",
      "训练次数：63, loss:3.668776512145996\n",
      "训练次数：64, loss:3.2374751567840576\n",
      "训练次数：65, loss:3.016887903213501\n",
      "训练次数：66, loss:2.3605902194976807\n",
      "训练次数：67, loss:2.2837533950805664\n",
      "训练次数：68, loss:2.304497480392456\n",
      "训练次数：69, loss:2.6739885807037354\n",
      "训练次数：70, loss:2.6488544940948486\n",
      "训练次数：71, loss:2.5891544818878174\n",
      "训练次数：72, loss:2.6203503608703613\n",
      "训练次数：73, loss:2.330404758453369\n",
      "训练次数：74, loss:2.318774461746216\n",
      "训练次数：75, loss:2.347665786743164\n",
      "训练次数：76, loss:2.1309027671813965\n",
      "训练次数：77, loss:2.4002528190612793\n",
      "训练次数：78, loss:2.2797980308532715\n",
      "训练次数：79, loss:2.2528586387634277\n",
      "训练次数：80, loss:2.391584634780884\n",
      "训练次数：81, loss:2.2960658073425293\n",
      "训练次数：82, loss:2.414823293685913\n",
      "训练次数：83, loss:2.3602118492126465\n",
      "训练次数：84, loss:2.03633975982666\n",
      "训练次数：85, loss:2.1196088790893555\n",
      "训练次数：86, loss:2.152303457260132\n",
      "训练次数：87, loss:2.0514743328094482\n",
      "训练次数：88, loss:2.043780565261841\n",
      "训练次数：89, loss:2.084841251373291\n",
      "训练次数：90, loss:2.192859649658203\n",
      "训练次数：91, loss:2.155198574066162\n",
      "训练次数：92, loss:2.041234016418457\n",
      "训练次数：93, loss:2.106205940246582\n",
      "训练次数：94, loss:1.9199466705322266\n",
      "训练次数：95, loss:1.993171215057373\n",
      "训练次数：96, loss:1.982167363166809\n",
      "训练次数：97, loss:1.911685585975647\n",
      "训练次数：98, loss:2.0698111057281494\n",
      "训练次数：99, loss:1.961504340171814\n",
      "训练次数：100, loss:1.9428216218948364\n",
      "训练次数：101, loss:2.0695645809173584\n",
      "训练次数：102, loss:1.9563931226730347\n",
      "训练次数：103, loss:2.0578322410583496\n",
      "训练次数：104, loss:2.1076838970184326\n",
      "训练次数：105, loss:2.0255253314971924\n",
      "训练次数：106, loss:1.8538727760314941\n",
      "训练次数：107, loss:1.9376978874206543\n",
      "训练次数：108, loss:2.160257339477539\n",
      "训练次数：109, loss:1.9974653720855713\n",
      "训练次数：110, loss:1.8896417617797852\n",
      "训练次数：111, loss:1.8842782974243164\n",
      "训练次数：112, loss:2.021977663040161\n",
      "训练次数：113, loss:2.0452542304992676\n",
      "训练次数：114, loss:1.820082426071167\n",
      "训练次数：115, loss:2.078640937805176\n",
      "训练次数：116, loss:2.049762725830078\n",
      "训练次数：117, loss:2.0043275356292725\n",
      "训练次数：118, loss:1.9925086498260498\n",
      "训练次数：119, loss:2.181640863418579\n",
      "训练次数：120, loss:2.0955116748809814\n",
      "训练次数：121, loss:2.2206497192382812\n",
      "训练次数：122, loss:1.9591283798217773\n",
      "训练次数：123, loss:2.1827616691589355\n",
      "训练次数：124, loss:2.1182901859283447\n",
      "训练次数：125, loss:1.7072886228561401\n",
      "训练次数：126, loss:1.9673393964767456\n",
      "训练次数：127, loss:2.0734479427337646\n",
      "训练次数：128, loss:2.018099784851074\n",
      "训练次数：129, loss:1.9202886819839478\n",
      "训练次数：130, loss:1.9129709005355835\n",
      "训练次数：131, loss:2.1035051345825195\n",
      "训练次数：132, loss:1.9281654357910156\n",
      "训练次数：133, loss:2.119676351547241\n",
      "训练次数：134, loss:1.801236867904663\n",
      "训练次数：135, loss:2.0004281997680664\n",
      "训练次数：136, loss:1.930700421333313\n",
      "训练次数：137, loss:2.013252019882202\n",
      "训练次数：138, loss:1.9833097457885742\n",
      "训练次数：139, loss:1.8817826509475708\n",
      "训练次数：140, loss:2.0129055976867676\n",
      "训练次数：141, loss:2.0984151363372803\n",
      "训练次数：142, loss:2.0068798065185547\n",
      "训练次数：143, loss:1.9482145309448242\n",
      "训练次数：144, loss:1.7783170938491821\n",
      "训练次数：145, loss:1.8147375583648682\n",
      "训练次数：146, loss:1.9749959707260132\n",
      "训练次数：147, loss:1.890318751335144\n",
      "训练次数：148, loss:2.0495917797088623\n",
      "训练次数：149, loss:1.7955421209335327\n",
      "训练次数：150, loss:1.896889090538025\n",
      "训练次数：151, loss:1.8241996765136719\n",
      "训练次数：152, loss:1.76396906375885\n",
      "训练次数：153, loss:1.9221538305282593\n",
      "训练次数：154, loss:1.805118203163147\n",
      "训练次数：155, loss:1.8251837491989136\n",
      "训练次数：156, loss:1.8099398612976074\n",
      "训练次数：157, loss:1.9435337781906128\n",
      "训练次数：158, loss:1.8475767374038696\n",
      "训练次数：159, loss:2.061208486557007\n",
      "训练次数：160, loss:2.030022621154785\n",
      "训练次数：161, loss:1.9515308141708374\n",
      "训练次数：162, loss:1.8454978466033936\n",
      "训练次数：163, loss:1.8730846643447876\n",
      "训练次数：164, loss:1.939558506011963\n",
      "训练次数：165, loss:1.8580641746520996\n",
      "训练次数：166, loss:1.9640657901763916\n",
      "训练次数：167, loss:1.785243034362793\n",
      "训练次数：168, loss:1.7352523803710938\n",
      "训练次数：169, loss:1.803005576133728\n",
      "训练次数：170, loss:1.8022493124008179\n",
      "训练次数：171, loss:1.8697360754013062\n",
      "训练次数：172, loss:1.779375433921814\n",
      "训练次数：173, loss:1.7964555025100708\n",
      "训练次数：174, loss:2.15389084815979\n",
      "训练次数：175, loss:1.815626859664917\n",
      "训练次数：176, loss:1.8409453630447388\n",
      "训练次数：177, loss:1.757501482963562\n",
      "训练次数：178, loss:2.003406286239624\n",
      "训练次数：179, loss:1.8670401573181152\n",
      "训练次数：180, loss:1.9532999992370605\n",
      "训练次数：181, loss:1.8091886043548584\n",
      "训练次数：182, loss:1.7620784044265747\n",
      "训练次数：183, loss:1.7067134380340576\n",
      "训练次数：184, loss:1.6815131902694702\n",
      "训练次数：185, loss:1.8111653327941895\n",
      "训练次数：186, loss:2.084792137145996\n",
      "训练次数：187, loss:1.879694938659668\n",
      "训练次数：188, loss:1.853442668914795\n",
      "训练次数：189, loss:1.9494580030441284\n",
      "训练次数：190, loss:1.9496231079101562\n",
      "训练次数：191, loss:1.6060895919799805\n",
      "训练次数：192, loss:1.931857705116272\n",
      "训练次数：193, loss:1.9462367296218872\n",
      "训练次数：194, loss:1.8916347026824951\n",
      "训练次数：195, loss:1.8693727254867554\n",
      "训练次数：196, loss:1.8527770042419434\n",
      "训练次数：197, loss:1.914244294166565\n",
      "训练次数：198, loss:1.9465713500976562\n",
      "训练次数：199, loss:1.8794193267822266\n",
      "训练次数：200, loss:1.8336470127105713\n",
      "训练次数：201, loss:1.750183343887329\n",
      "训练次数：202, loss:1.8798812627792358\n",
      "训练次数：203, loss:1.9233596324920654\n",
      "训练次数：204, loss:1.8993498086929321\n",
      "训练次数：205, loss:1.8301562070846558\n",
      "训练次数：206, loss:1.8692861795425415\n",
      "训练次数：207, loss:1.8895263671875\n",
      "训练次数：208, loss:1.7998825311660767\n",
      "训练次数：209, loss:1.8687403202056885\n",
      "训练次数：210, loss:1.9654349088668823\n",
      "训练次数：211, loss:1.7647687196731567\n",
      "训练次数：212, loss:1.812056541442871\n",
      "训练次数：213, loss:1.7090431451797485\n",
      "训练次数：214, loss:1.8768302202224731\n",
      "训练次数：215, loss:1.8896684646606445\n",
      "训练次数：216, loss:1.8963532447814941\n",
      "训练次数：217, loss:1.68966805934906\n",
      "训练次数：218, loss:1.7889457941055298\n",
      "训练次数：219, loss:1.7674132585525513\n",
      "训练次数：220, loss:1.7742234468460083\n",
      "训练次数：221, loss:1.9194681644439697\n",
      "训练次数：222, loss:1.7987216711044312\n",
      "训练次数：223, loss:1.714835524559021\n",
      "训练次数：224, loss:1.8823508024215698\n",
      "训练次数：225, loss:2.0124268531799316\n",
      "训练次数：226, loss:1.839895486831665\n",
      "训练次数：227, loss:1.673203468322754\n",
      "训练次数：228, loss:1.637997031211853\n",
      "训练次数：229, loss:1.7870564460754395\n",
      "训练次数：230, loss:1.9542758464813232\n",
      "训练次数：231, loss:1.8988993167877197\n",
      "训练次数：232, loss:1.744008183479309\n",
      "训练次数：233, loss:1.8686587810516357\n",
      "训练次数：234, loss:1.8051810264587402\n",
      "训练次数：235, loss:1.9239698648452759\n",
      "训练次数：236, loss:2.057509422302246\n",
      "训练次数：237, loss:2.1390531063079834\n",
      "训练次数：238, loss:1.7889853715896606\n",
      "训练次数：239, loss:1.6799496412277222\n",
      "训练次数：240, loss:1.7644983530044556\n",
      "训练次数：241, loss:1.914158821105957\n",
      "训练次数：242, loss:1.7627840042114258\n",
      "训练次数：243, loss:1.7916457653045654\n",
      "训练次数：244, loss:1.9644757509231567\n",
      "训练次数：245, loss:1.6826434135437012\n",
      "训练次数：246, loss:1.7014060020446777\n",
      "训练次数：247, loss:1.7717384099960327\n",
      "训练次数：248, loss:1.8014293909072876\n",
      "训练次数：249, loss:1.9405760765075684\n",
      "训练次数：250, loss:1.9115605354309082\n",
      "训练次数：251, loss:1.7549529075622559\n",
      "训练次数：252, loss:1.6858222484588623\n",
      "训练次数：253, loss:1.7173720598220825\n",
      "训练次数：254, loss:1.7446494102478027\n",
      "训练次数：255, loss:1.8390507698059082\n",
      "训练次数：256, loss:1.6588712930679321\n",
      "训练次数：257, loss:1.782588243484497\n",
      "训练次数：258, loss:1.7009892463684082\n",
      "训练次数：259, loss:1.8996002674102783\n",
      "训练次数：260, loss:1.960038185119629\n",
      "训练次数：261, loss:1.7316207885742188\n",
      "训练次数：262, loss:1.817962884902954\n",
      "训练次数：263, loss:1.8095927238464355\n",
      "训练次数：264, loss:1.7903807163238525\n",
      "训练次数：265, loss:1.6045376062393188\n",
      "训练次数：266, loss:1.5730960369110107\n",
      "训练次数：267, loss:1.7555263042449951\n",
      "训练次数：268, loss:1.667051076889038\n",
      "训练次数：269, loss:1.7268922328948975\n",
      "训练次数：270, loss:1.789538025856018\n",
      "训练次数：271, loss:1.8393430709838867\n",
      "训练次数：272, loss:1.8432228565216064\n",
      "训练次数：273, loss:1.6537331342697144\n",
      "训练次数：274, loss:1.736046552658081\n",
      "训练次数：275, loss:1.7206484079360962\n",
      "训练次数：276, loss:1.669609785079956\n",
      "训练次数：277, loss:1.9654096364974976\n",
      "训练次数：278, loss:1.9465441703796387\n",
      "训练次数：279, loss:1.8108099699020386\n",
      "训练次数：280, loss:1.640342354774475\n",
      "训练次数：281, loss:1.6412302255630493\n",
      "训练次数：282, loss:1.6299629211425781\n",
      "训练次数：283, loss:1.6121093034744263\n",
      "训练次数：284, loss:1.7554831504821777\n",
      "训练次数：285, loss:1.6437925100326538\n",
      "训练次数：286, loss:1.6495405435562134\n",
      "训练次数：287, loss:1.8287224769592285\n",
      "训练次数：288, loss:1.6526360511779785\n",
      "训练次数：289, loss:1.7620707750320435\n",
      "训练次数：290, loss:1.8162740468978882\n",
      "训练次数：291, loss:1.5668931007385254\n",
      "训练次数：292, loss:1.799936056137085\n",
      "训练次数：293, loss:1.7253268957138062\n",
      "训练次数：294, loss:1.6184357404708862\n",
      "训练次数：295, loss:1.7103124856948853\n",
      "训练次数：296, loss:1.7409318685531616\n",
      "训练次数：297, loss:1.9912762641906738\n",
      "训练次数：298, loss:1.6631423234939575\n",
      "训练次数：299, loss:1.7694530487060547\n",
      "训练次数：300, loss:1.781116247177124\n",
      "训练次数：301, loss:1.6556802988052368\n",
      "训练次数：302, loss:1.6696573495864868\n",
      "训练次数：303, loss:1.6367733478546143\n",
      "训练次数：304, loss:1.4778136014938354\n",
      "训练次数：305, loss:1.7060585021972656\n",
      "训练次数：306, loss:1.8242275714874268\n",
      "训练次数：307, loss:1.5817514657974243\n",
      "训练次数：308, loss:1.6918003559112549\n",
      "训练次数：309, loss:1.7330191135406494\n",
      "训练次数：310, loss:1.6027084589004517\n",
      "训练次数：311, loss:1.5196019411087036\n",
      "训练次数：312, loss:1.6404324769973755\n",
      "训练次数：313, loss:1.9458314180374146\n",
      "训练次数：314, loss:1.6640267372131348\n",
      "训练次数：315, loss:1.6072194576263428\n",
      "训练次数：316, loss:1.6814857721328735\n",
      "训练次数：317, loss:1.6219569444656372\n",
      "训练次数：318, loss:1.8251210451126099\n",
      "训练次数：319, loss:1.5725594758987427\n",
      "训练次数：320, loss:1.675378680229187\n",
      "训练次数：321, loss:1.6392159461975098\n",
      "训练次数：322, loss:1.839288592338562\n",
      "训练次数：323, loss:1.7350051403045654\n",
      "训练次数：324, loss:1.7936290502548218\n",
      "训练次数：325, loss:1.517150640487671\n",
      "训练次数：326, loss:1.5317000150680542\n",
      "训练次数：327, loss:1.5886636972427368\n",
      "训练次数：328, loss:1.6255497932434082\n",
      "训练次数：329, loss:1.583836317062378\n",
      "训练次数：330, loss:1.7364574670791626\n",
      "训练次数：331, loss:1.5410144329071045\n",
      "训练次数：332, loss:1.6270376443862915\n",
      "训练次数：333, loss:1.6474076509475708\n",
      "训练次数：334, loss:1.8009238243103027\n",
      "训练次数：335, loss:1.772780179977417\n",
      "训练次数：336, loss:1.7260937690734863\n",
      "训练次数：337, loss:1.6037626266479492\n",
      "训练次数：338, loss:1.5014848709106445\n",
      "训练次数：339, loss:1.589254379272461\n",
      "训练次数：340, loss:1.5198031663894653\n",
      "训练次数：341, loss:1.7711856365203857\n",
      "训练次数：342, loss:1.6742228269577026\n",
      "训练次数：343, loss:1.5787664651870728\n",
      "训练次数：344, loss:1.6354315280914307\n",
      "训练次数：345, loss:1.6869548559188843\n",
      "训练次数：346, loss:1.4247022867202759\n",
      "训练次数：347, loss:1.6181892156600952\n",
      "训练次数：348, loss:1.6976103782653809\n",
      "训练次数：349, loss:1.6571422815322876\n",
      "训练次数：350, loss:1.5628232955932617\n",
      "训练次数：351, loss:1.5046520233154297\n",
      "训练次数：352, loss:1.63376784324646\n",
      "训练次数：353, loss:1.5286626815795898\n",
      "训练次数：354, loss:1.6888585090637207\n",
      "训练次数：355, loss:1.4180824756622314\n",
      "训练次数：356, loss:1.6900519132614136\n",
      "训练次数：357, loss:1.732448935508728\n",
      "训练次数：358, loss:1.74472975730896\n",
      "训练次数：359, loss:1.6994781494140625\n",
      "训练次数：360, loss:1.7980283498764038\n",
      "训练次数：361, loss:1.6011918783187866\n",
      "训练次数：362, loss:1.3676098585128784\n",
      "训练次数：363, loss:1.7750835418701172\n",
      "训练次数：364, loss:1.6528068780899048\n",
      "训练次数：365, loss:1.3421787023544312\n",
      "训练次数：366, loss:1.464157223701477\n",
      "训练次数：367, loss:1.7813106775283813\n",
      "训练次数：368, loss:1.704724907875061\n",
      "训练次数：369, loss:1.368820309638977\n",
      "训练次数：370, loss:1.5434116125106812\n",
      "训练次数：371, loss:1.2876425981521606\n",
      "训练次数：372, loss:1.6552125215530396\n",
      "训练次数：373, loss:1.582697868347168\n",
      "训练次数：374, loss:1.6226361989974976\n",
      "训练次数：375, loss:1.7219769954681396\n",
      "训练次数：376, loss:1.438337802886963\n",
      "训练次数：377, loss:1.5146980285644531\n",
      "训练次数：378, loss:1.596030354499817\n",
      "训练次数：379, loss:1.6601523160934448\n",
      "训练次数：380, loss:1.6398764848709106\n",
      "训练次数：381, loss:1.6905497312545776\n",
      "训练次数：382, loss:1.5249940156936646\n",
      "训练次数：383, loss:1.5807996988296509\n",
      "训练次数：384, loss:1.5429491996765137\n",
      "训练次数：385, loss:1.534609079360962\n",
      "训练次数：386, loss:1.4400850534439087\n",
      "训练次数：387, loss:1.543191909790039\n",
      "训练次数：388, loss:1.7493388652801514\n",
      "训练次数：389, loss:1.6776574850082397\n",
      "训练次数：390, loss:1.4679749011993408\n",
      "训练次数：391, loss:1.6379451751708984\n",
      "训练次数：392, loss:1.3742899894714355\n",
      "训练次数：393, loss:1.6050437688827515\n",
      "训练次数：394, loss:1.6757303476333618\n",
      "训练次数：395, loss:1.617150902748108\n",
      "训练次数：396, loss:1.6168259382247925\n",
      "训练次数：397, loss:1.618151307106018\n",
      "训练次数：398, loss:1.778542399406433\n",
      "训练次数：399, loss:1.535131573677063\n",
      "训练次数：400, loss:1.4728257656097412\n",
      "训练次数：401, loss:1.6601839065551758\n",
      "训练次数：402, loss:1.8479454517364502\n",
      "训练次数：403, loss:1.4950010776519775\n",
      "训练次数：404, loss:1.475221872329712\n",
      "训练次数：405, loss:1.7968084812164307\n",
      "训练次数：406, loss:1.5565651655197144\n",
      "训练次数：407, loss:1.49569833278656\n",
      "训练次数：408, loss:1.4150276184082031\n",
      "训练次数：409, loss:1.6327743530273438\n",
      "训练次数：410, loss:1.5538409948349\n",
      "训练次数：411, loss:1.7907330989837646\n",
      "训练次数：412, loss:1.4988049268722534\n",
      "训练次数：413, loss:1.6942235231399536\n",
      "训练次数：414, loss:1.5984026193618774\n",
      "训练次数：415, loss:1.756550908088684\n",
      "训练次数：416, loss:1.4104622602462769\n",
      "训练次数：417, loss:1.6852370500564575\n",
      "训练次数：418, loss:1.4746417999267578\n",
      "训练次数：419, loss:1.5655003786087036\n",
      "训练次数：420, loss:1.656589150428772\n",
      "训练次数：421, loss:1.8873980045318604\n",
      "训练次数：422, loss:1.5636743307113647\n",
      "训练次数：423, loss:1.7419219017028809\n",
      "训练次数：424, loss:1.614510178565979\n",
      "训练次数：425, loss:1.4683763980865479\n",
      "训练次数：426, loss:1.6182993650436401\n",
      "训练次数：427, loss:1.606557846069336\n",
      "训练次数：428, loss:1.5657495260238647\n",
      "训练次数：429, loss:1.6609070301055908\n",
      "训练次数：430, loss:1.637794017791748\n",
      "训练次数：431, loss:1.5904953479766846\n",
      "训练次数：432, loss:1.394281268119812\n",
      "训练次数：433, loss:1.699059009552002\n",
      "训练次数：434, loss:1.566533088684082\n",
      "训练次数：435, loss:1.5479660034179688\n",
      "训练次数：436, loss:1.7929683923721313\n",
      "训练次数：437, loss:1.617913007736206\n",
      "训练次数：438, loss:1.5467948913574219\n",
      "训练次数：439, loss:1.4834145307540894\n",
      "训练次数：440, loss:1.6054691076278687\n",
      "训练次数：441, loss:1.7649308443069458\n",
      "训练次数：442, loss:1.529353141784668\n",
      "训练次数：443, loss:1.739823579788208\n",
      "训练次数：444, loss:1.4055498838424683\n",
      "训练次数：445, loss:1.5787009000778198\n",
      "训练次数：446, loss:1.5327221155166626\n",
      "训练次数：447, loss:1.7039837837219238\n",
      "训练次数：448, loss:1.5899499654769897\n",
      "训练次数：449, loss:1.4972690343856812\n",
      "训练次数：450, loss:1.6129592657089233\n",
      "训练次数：451, loss:1.2861640453338623\n",
      "训练次数：452, loss:1.3310329914093018\n",
      "训练次数：453, loss:1.4801719188690186\n",
      "训练次数：454, loss:1.4920746088027954\n",
      "训练次数：455, loss:1.6334943771362305\n",
      "训练次数：456, loss:1.464496374130249\n",
      "训练次数：457, loss:1.6722943782806396\n",
      "训练次数：458, loss:1.344639778137207\n",
      "训练次数：459, loss:1.397702693939209\n",
      "训练次数：460, loss:1.5530763864517212\n",
      "训练次数：461, loss:1.6855294704437256\n",
      "训练次数：462, loss:1.5447099208831787\n",
      "训练次数：463, loss:1.424595594406128\n",
      "训练次数：464, loss:1.5648066997528076\n",
      "训练次数：465, loss:1.7085200548171997\n",
      "训练次数：466, loss:1.522739291191101\n",
      "训练次数：467, loss:1.6144330501556396\n",
      "训练次数：468, loss:1.460528016090393\n",
      "训练次数：469, loss:1.524192452430725\n",
      "训练次数：470, loss:1.629088044166565\n",
      "训练次数：471, loss:1.3938632011413574\n",
      "训练次数：472, loss:1.6522659063339233\n",
      "训练次数：473, loss:1.489547848701477\n",
      "训练次数：474, loss:1.7039306163787842\n",
      "训练次数：475, loss:1.3053231239318848\n",
      "训练次数：476, loss:1.8952410221099854\n",
      "训练次数：477, loss:1.4208029508590698\n",
      "训练次数：478, loss:1.6324704885482788\n",
      "训练次数：479, loss:1.7289361953735352\n",
      "训练次数：480, loss:1.519734263420105\n",
      "训练次数：481, loss:1.7449679374694824\n",
      "训练次数：482, loss:1.5840545892715454\n",
      "训练次数：483, loss:1.5821329355239868\n",
      "训练次数：484, loss:1.571420431137085\n",
      "训练次数：485, loss:1.47089684009552\n",
      "训练次数：486, loss:1.3768976926803589\n",
      "训练次数：487, loss:1.5532561540603638\n",
      "训练次数：488, loss:1.6077163219451904\n",
      "训练次数：489, loss:1.5497825145721436\n",
      "训练次数：490, loss:1.4576225280761719\n",
      "训练次数：491, loss:1.7129319906234741\n",
      "训练次数：492, loss:1.656213641166687\n",
      "训练次数：493, loss:1.6087822914123535\n",
      "训练次数：494, loss:1.3531081676483154\n",
      "训练次数：495, loss:1.6370925903320312\n",
      "训练次数：496, loss:1.636951208114624\n",
      "训练次数：497, loss:1.5096999406814575\n",
      "训练次数：498, loss:1.6506778001785278\n",
      "训练次数：499, loss:1.7999154329299927\n",
      "训练次数：500, loss:1.5051772594451904\n",
      "训练次数：501, loss:1.5792828798294067\n",
      "训练次数：502, loss:1.5177990198135376\n",
      "训练次数：503, loss:1.4892529249191284\n",
      "训练次数：504, loss:1.797161340713501\n",
      "训练次数：505, loss:1.5558942556381226\n",
      "训练次数：506, loss:1.6538820266723633\n",
      "训练次数：507, loss:1.5174223184585571\n",
      "训练次数：508, loss:1.5809959173202515\n",
      "训练次数：509, loss:1.5570194721221924\n",
      "训练次数：510, loss:1.5683813095092773\n",
      "训练次数：511, loss:1.798888087272644\n",
      "训练次数：512, loss:1.4847979545593262\n",
      "训练次数：513, loss:1.6259435415267944\n",
      "训练次数：514, loss:1.715514898300171\n",
      "训练次数：515, loss:1.7394697666168213\n",
      "训练次数：516, loss:1.5362188816070557\n",
      "训练次数：517, loss:1.7232599258422852\n",
      "训练次数：518, loss:1.5043292045593262\n",
      "训练次数：519, loss:1.7482205629348755\n",
      "训练次数：520, loss:1.4888747930526733\n",
      "训练次数：521, loss:1.9078593254089355\n",
      "训练次数：522, loss:1.5126633644104004\n",
      "训练次数：523, loss:1.678828239440918\n",
      "训练次数：524, loss:1.7032479047775269\n",
      "训练次数：525, loss:1.8070250749588013\n",
      "训练次数：526, loss:1.8488690853118896\n",
      "训练次数：527, loss:1.6914105415344238\n",
      "训练次数：528, loss:1.5130596160888672\n",
      "训练次数：529, loss:1.5698869228363037\n",
      "训练次数：530, loss:1.6625131368637085\n",
      "训练次数：531, loss:1.5880688428878784\n",
      "训练次数：532, loss:1.5514683723449707\n",
      "训练次数：533, loss:1.5820281505584717\n",
      "训练次数：534, loss:1.29820716381073\n",
      "训练次数：535, loss:1.440345287322998\n",
      "训练次数：536, loss:1.4947190284729004\n",
      "训练次数：537, loss:1.7135131359100342\n",
      "训练次数：538, loss:1.6121578216552734\n",
      "训练次数：539, loss:1.5555623769760132\n",
      "训练次数：540, loss:1.5202889442443848\n",
      "训练次数：541, loss:1.5026483535766602\n",
      "训练次数：542, loss:1.640924096107483\n",
      "训练次数：543, loss:1.551709771156311\n",
      "训练次数：544, loss:1.5678417682647705\n",
      "训练次数：545, loss:1.5968319177627563\n",
      "训练次数：546, loss:1.461565375328064\n",
      "训练次数：547, loss:1.6372593641281128\n",
      "训练次数：548, loss:1.5463323593139648\n",
      "训练次数：549, loss:1.478390097618103\n",
      "训练次数：550, loss:1.5670021772384644\n",
      "训练次数：551, loss:1.4823893308639526\n",
      "训练次数：552, loss:1.6812138557434082\n",
      "训练次数：553, loss:1.4788521528244019\n",
      "训练次数：554, loss:1.7395646572113037\n",
      "训练次数：555, loss:1.3193191289901733\n",
      "训练次数：556, loss:1.425349235534668\n",
      "训练次数：557, loss:1.8645309209823608\n",
      "训练次数：558, loss:1.4966702461242676\n",
      "训练次数：559, loss:1.52156400680542\n",
      "训练次数：560, loss:1.6181341409683228\n",
      "训练次数：561, loss:1.4315122365951538\n",
      "训练次数：562, loss:1.4395004510879517\n",
      "训练次数：563, loss:1.7504419088363647\n",
      "训练次数：564, loss:1.5750964879989624\n",
      "训练次数：565, loss:1.6291006803512573\n",
      "训练次数：566, loss:1.6147503852844238\n",
      "训练次数：567, loss:1.505196452140808\n",
      "训练次数：568, loss:1.5762745141983032\n",
      "训练次数：569, loss:1.4380964040756226\n",
      "训练次数：570, loss:1.4015995264053345\n",
      "训练次数：571, loss:1.4930908679962158\n",
      "训练次数：572, loss:1.5680391788482666\n",
      "训练次数：573, loss:1.3833056688308716\n",
      "训练次数：574, loss:1.6858264207839966\n",
      "训练次数：575, loss:1.6110172271728516\n",
      "训练次数：576, loss:1.4668751955032349\n",
      "训练次数：577, loss:1.5446850061416626\n",
      "训练次数：578, loss:1.8017222881317139\n",
      "训练次数：579, loss:1.316243052482605\n",
      "训练次数：580, loss:1.4588249921798706\n",
      "训练次数：581, loss:1.482895851135254\n",
      "训练次数：582, loss:1.5745230913162231\n",
      "训练次数：583, loss:1.3725508451461792\n",
      "训练次数：584, loss:1.685996174812317\n",
      "训练次数：585, loss:1.5677282810211182\n",
      "训练次数：586, loss:1.4159377813339233\n",
      "训练次数：587, loss:1.5895878076553345\n",
      "训练次数：588, loss:1.531043529510498\n",
      "训练次数：589, loss:1.5990842580795288\n",
      "训练次数：590, loss:1.418020486831665\n",
      "训练次数：591, loss:1.5350216627120972\n",
      "训练次数：592, loss:1.380690574645996\n",
      "训练次数：593, loss:1.575221061706543\n",
      "训练次数：594, loss:1.6262397766113281\n",
      "训练次数：595, loss:1.6274322271347046\n",
      "训练次数：596, loss:1.8721177577972412\n",
      "训练次数：597, loss:1.497259497642517\n",
      "训练次数：598, loss:1.5786277055740356\n",
      "训练次数：599, loss:1.532043218612671\n",
      "训练次数：600, loss:1.641851544380188\n",
      "训练次数：601, loss:1.6155730485916138\n",
      "训练次数：602, loss:1.3195054531097412\n",
      "训练次数：603, loss:1.255301833152771\n",
      "训练次数：604, loss:1.6541870832443237\n",
      "训练次数：605, loss:1.5711338520050049\n",
      "训练次数：606, loss:1.3882888555526733\n",
      "训练次数：607, loss:1.2998648881912231\n",
      "训练次数：608, loss:1.4106172323226929\n",
      "训练次数：609, loss:1.543447494506836\n",
      "训练次数：610, loss:1.5347375869750977\n",
      "训练次数：611, loss:1.4867734909057617\n",
      "训练次数：612, loss:1.6461542844772339\n",
      "训练次数：613, loss:1.499056339263916\n",
      "训练次数：614, loss:1.511659026145935\n",
      "训练次数：615, loss:1.5867853164672852\n",
      "训练次数：616, loss:1.3922348022460938\n",
      "训练次数：617, loss:1.455773949623108\n",
      "训练次数：618, loss:1.458261251449585\n",
      "训练次数：619, loss:1.5819597244262695\n",
      "训练次数：620, loss:1.6784881353378296\n",
      "训练次数：621, loss:1.441031575202942\n",
      "训练次数：622, loss:1.6007570028305054\n",
      "训练次数：623, loss:1.4825643301010132\n",
      "训练次数：624, loss:1.451214075088501\n",
      "训练次数：625, loss:1.579126238822937\n",
      "训练次数：626, loss:1.342576265335083\n",
      "训练次数：627, loss:1.511526107788086\n",
      "训练次数：628, loss:1.7789251804351807\n",
      "训练次数：629, loss:1.650777816772461\n",
      "训练次数：630, loss:1.4560273885726929\n",
      "训练次数：631, loss:1.520149827003479\n",
      "训练次数：632, loss:1.6532039642333984\n",
      "训练次数：633, loss:1.5540140867233276\n",
      "训练次数：634, loss:1.3015011548995972\n",
      "训练次数：635, loss:1.3956832885742188\n",
      "训练次数：636, loss:1.2521601915359497\n",
      "训练次数：637, loss:1.6251063346862793\n",
      "训练次数：638, loss:1.7083938121795654\n",
      "训练次数：639, loss:1.4250706434249878\n",
      "训练次数：640, loss:1.684167742729187\n",
      "训练次数：641, loss:1.7090590000152588\n",
      "训练次数：642, loss:1.5354928970336914\n",
      "训练次数：643, loss:1.5284581184387207\n",
      "训练次数：644, loss:1.4367492198944092\n",
      "训练次数：645, loss:1.5014890432357788\n",
      "训练次数：646, loss:1.710755467414856\n",
      "训练次数：647, loss:1.3989579677581787\n",
      "训练次数：648, loss:1.391083002090454\n",
      "训练次数：649, loss:1.5301851034164429\n",
      "训练次数：650, loss:1.756851315498352\n",
      "训练次数：651, loss:1.411747932434082\n",
      "训练次数：652, loss:1.4555883407592773\n",
      "训练次数：653, loss:1.327149510383606\n",
      "训练次数：654, loss:1.4601186513900757\n",
      "训练次数：655, loss:1.4862022399902344\n",
      "训练次数：656, loss:1.575533390045166\n",
      "训练次数：657, loss:1.6381727457046509\n",
      "训练次数：658, loss:1.3580070734024048\n",
      "训练次数：659, loss:1.752326250076294\n",
      "训练次数：660, loss:1.4754396677017212\n",
      "训练次数：661, loss:1.4114389419555664\n",
      "训练次数：662, loss:1.1935949325561523\n",
      "训练次数：663, loss:1.3629796504974365\n",
      "训练次数：664, loss:1.4143604040145874\n",
      "训练次数：665, loss:1.7287226915359497\n",
      "训练次数：666, loss:1.8122743368148804\n",
      "训练次数：667, loss:1.39067542552948\n",
      "训练次数：668, loss:1.8179131746292114\n",
      "训练次数：669, loss:1.6553046703338623\n",
      "训练次数：670, loss:1.4825116395950317\n",
      "训练次数：671, loss:1.7288713455200195\n",
      "训练次数：672, loss:1.5862808227539062\n",
      "训练次数：673, loss:1.527951955795288\n",
      "训练次数：674, loss:1.4405189752578735\n",
      "训练次数：675, loss:1.571592926979065\n",
      "训练次数：676, loss:1.735953450202942\n",
      "训练次数：677, loss:1.6249316930770874\n",
      "训练次数：678, loss:1.4948011636734009\n",
      "训练次数：679, loss:1.7538714408874512\n",
      "训练次数：680, loss:1.7050055265426636\n",
      "训练次数：681, loss:1.6698580980300903\n",
      "训练次数：682, loss:1.2879304885864258\n",
      "训练次数：683, loss:1.3665616512298584\n",
      "训练次数：684, loss:1.5637537240982056\n",
      "训练次数：685, loss:1.6991121768951416\n",
      "训练次数：686, loss:1.8488318920135498\n",
      "训练次数：687, loss:1.4638113975524902\n",
      "训练次数：688, loss:1.633972406387329\n",
      "训练次数：689, loss:1.66817307472229\n",
      "训练次数：690, loss:1.566808819770813\n",
      "训练次数：691, loss:1.675959825515747\n",
      "训练次数：692, loss:1.6122691631317139\n",
      "训练次数：693, loss:1.941374659538269\n",
      "训练次数：694, loss:1.5145121812820435\n",
      "训练次数：695, loss:1.5797879695892334\n",
      "训练次数：696, loss:1.4131324291229248\n",
      "训练次数：697, loss:1.583154559135437\n",
      "训练次数：698, loss:1.5605729818344116\n",
      "训练次数：699, loss:1.4044569730758667\n",
      "训练次数：700, loss:1.562545657157898\n",
      "训练次数：701, loss:1.5367106199264526\n",
      "训练次数：702, loss:1.5626044273376465\n",
      "训练次数：703, loss:1.4254066944122314\n",
      "训练次数：704, loss:1.245481014251709\n",
      "训练次数：705, loss:1.4411096572875977\n",
      "训练次数：706, loss:1.4981471300125122\n",
      "训练次数：707, loss:1.6585745811462402\n",
      "训练次数：708, loss:1.642001748085022\n",
      "训练次数：709, loss:1.3626534938812256\n",
      "训练次数：710, loss:1.2951501607894897\n",
      "训练次数：711, loss:1.7112643718719482\n",
      "训练次数：712, loss:1.2044522762298584\n",
      "训练次数：713, loss:1.4167429208755493\n",
      "训练次数：714, loss:1.4060970544815063\n",
      "训练次数：715, loss:1.5216445922851562\n",
      "训练次数：716, loss:1.6932072639465332\n",
      "训练次数：717, loss:1.3441855907440186\n",
      "训练次数：718, loss:1.4487124681472778\n",
      "训练次数：719, loss:1.4451991319656372\n",
      "训练次数：720, loss:1.432145357131958\n",
      "训练次数：721, loss:1.2430459260940552\n",
      "训练次数：722, loss:1.3036495447158813\n",
      "训练次数：723, loss:1.205603837966919\n",
      "训练次数：724, loss:1.693259596824646\n",
      "训练次数：725, loss:1.5750737190246582\n",
      "训练次数：726, loss:1.4591130018234253\n",
      "训练次数：727, loss:1.7255256175994873\n",
      "训练次数：728, loss:1.787532091140747\n",
      "训练次数：729, loss:1.5648139715194702\n",
      "训练次数：730, loss:1.459804654121399\n",
      "训练次数：731, loss:1.2691229581832886\n",
      "训练次数：732, loss:1.4302653074264526\n",
      "训练次数：733, loss:1.535348653793335\n",
      "训练次数：734, loss:1.4576531648635864\n",
      "训练次数：735, loss:1.394700527191162\n",
      "训练次数：736, loss:1.5529625415802002\n",
      "训练次数：737, loss:1.3338810205459595\n",
      "训练次数：738, loss:1.535313606262207\n",
      "训练次数：739, loss:1.2938398122787476\n",
      "训练次数：740, loss:1.2854101657867432\n",
      "训练次数：741, loss:1.5048255920410156\n",
      "训练次数：742, loss:1.495958685874939\n",
      "训练次数：743, loss:1.8380043506622314\n",
      "训练次数：744, loss:1.3195562362670898\n",
      "训练次数：745, loss:1.2551389932632446\n",
      "训练次数：746, loss:1.559903621673584\n",
      "训练次数：747, loss:1.3808385133743286\n",
      "训练次数：748, loss:1.5711312294006348\n",
      "训练次数：749, loss:1.554024338722229\n",
      "训练次数：750, loss:1.309085488319397\n",
      "训练次数：751, loss:1.8872441053390503\n",
      "训练次数：752, loss:1.7489073276519775\n",
      "训练次数：753, loss:1.4763275384902954\n",
      "训练次数：754, loss:1.502726674079895\n",
      "训练次数：755, loss:1.5845084190368652\n",
      "训练次数：756, loss:1.4520400762557983\n",
      "训练次数：757, loss:1.5932040214538574\n",
      "训练次数：758, loss:1.5168200731277466\n",
      "训练次数：759, loss:1.4749857187271118\n",
      "训练次数：760, loss:1.5727368593215942\n",
      "训练次数：761, loss:1.4496572017669678\n",
      "训练次数：762, loss:1.5913162231445312\n",
      "训练次数：763, loss:1.5094611644744873\n",
      "训练次数：764, loss:1.4683582782745361\n",
      "训练次数：765, loss:1.3202191591262817\n",
      "训练次数：766, loss:1.5364470481872559\n",
      "训练次数：767, loss:1.3407843112945557\n",
      "训练次数：768, loss:1.3768759965896606\n",
      "训练次数：769, loss:1.3467837572097778\n",
      "训练次数：770, loss:1.3790823221206665\n",
      "训练次数：771, loss:1.488135814666748\n",
      "训练次数：772, loss:1.5259369611740112\n",
      "训练次数：773, loss:1.474244475364685\n",
      "训练次数：774, loss:1.338066577911377\n",
      "训练次数：775, loss:1.4205104112625122\n",
      "训练次数：776, loss:1.3001078367233276\n",
      "训练次数：777, loss:1.4983395338058472\n",
      "训练次数：778, loss:1.4163941144943237\n",
      "训练次数：779, loss:1.6345232725143433\n",
      "训练次数：780, loss:1.37409508228302\n",
      "训练次数：781, loss:1.5281906127929688\n",
      "训练次数：782, loss:1.995816946029663\n",
      "----------第1轮训练开始----------\n",
      "训练次数：783, loss:1.5664794445037842\n",
      "训练次数：784, loss:1.240524172782898\n",
      "训练次数：785, loss:1.5703762769699097\n",
      "训练次数：786, loss:1.2653005123138428\n",
      "训练次数：787, loss:1.406240463256836\n",
      "训练次数：788, loss:1.2928366661071777\n",
      "训练次数：789, loss:1.3352887630462646\n",
      "训练次数：790, loss:1.5125759840011597\n",
      "训练次数：791, loss:1.3820468187332153\n",
      "训练次数：792, loss:1.4831206798553467\n",
      "训练次数：793, loss:1.5681471824645996\n",
      "训练次数：794, loss:1.5109140872955322\n",
      "训练次数：795, loss:1.5852161645889282\n",
      "训练次数：796, loss:1.7158900499343872\n",
      "训练次数：797, loss:1.3639695644378662\n",
      "训练次数：798, loss:1.3988635540008545\n",
      "训练次数：799, loss:1.590151071548462\n",
      "训练次数：800, loss:1.287057638168335\n",
      "训练次数：801, loss:1.2876193523406982\n",
      "训练次数：802, loss:1.356428623199463\n",
      "训练次数：803, loss:1.6971445083618164\n",
      "训练次数：804, loss:1.460124135017395\n",
      "训练次数：805, loss:1.4559476375579834\n",
      "训练次数：806, loss:1.4599636793136597\n",
      "训练次数：807, loss:1.3481907844543457\n",
      "训练次数：808, loss:1.5393863916397095\n",
      "训练次数：809, loss:1.653979778289795\n",
      "训练次数：810, loss:1.334646463394165\n",
      "训练次数：811, loss:1.315138578414917\n",
      "训练次数：812, loss:1.2187442779541016\n",
      "训练次数：813, loss:1.7320542335510254\n",
      "训练次数：814, loss:1.4848581552505493\n",
      "训练次数：815, loss:1.2786816358566284\n",
      "训练次数：816, loss:1.2925562858581543\n",
      "训练次数：817, loss:1.421182632446289\n",
      "训练次数：818, loss:1.4381606578826904\n",
      "训练次数：819, loss:1.2744417190551758\n",
      "训练次数：820, loss:1.52428138256073\n",
      "训练次数：821, loss:1.2648826837539673\n",
      "训练次数：822, loss:1.5053300857543945\n",
      "训练次数：823, loss:1.355994701385498\n",
      "训练次数：824, loss:1.2904552221298218\n",
      "训练次数：825, loss:1.4972772598266602\n",
      "训练次数：826, loss:1.5521084070205688\n",
      "训练次数：827, loss:1.2114187479019165\n",
      "训练次数：828, loss:1.3467295169830322\n",
      "训练次数：829, loss:1.4169596433639526\n",
      "训练次数：830, loss:1.525808572769165\n",
      "训练次数：831, loss:1.5157800912857056\n",
      "训练次数：832, loss:1.4475902318954468\n",
      "训练次数：833, loss:1.3814234733581543\n",
      "训练次数：834, loss:1.5244511365890503\n",
      "训练次数：835, loss:1.2926137447357178\n",
      "训练次数：836, loss:1.5072373151779175\n",
      "训练次数：837, loss:1.1801986694335938\n",
      "训练次数：838, loss:1.2509057521820068\n",
      "训练次数：839, loss:1.4348962306976318\n",
      "训练次数：840, loss:1.2519464492797852\n",
      "训练次数：841, loss:1.2291022539138794\n",
      "训练次数：842, loss:1.3466347455978394\n",
      "训练次数：843, loss:1.4612928628921509\n",
      "训练次数：844, loss:1.5062304735183716\n",
      "训练次数：845, loss:1.4245418310165405\n",
      "训练次数：846, loss:1.3748539686203003\n",
      "训练次数：847, loss:1.5057454109191895\n",
      "训练次数：848, loss:1.307900071144104\n",
      "训练次数：849, loss:1.3726167678833008\n",
      "训练次数：850, loss:1.4753214120864868\n",
      "训练次数：851, loss:1.1935133934020996\n",
      "训练次数：852, loss:1.2928677797317505\n",
      "训练次数：853, loss:1.7810882329940796\n",
      "训练次数：854, loss:1.111864686012268\n",
      "训练次数：855, loss:1.4414935111999512\n",
      "训练次数：856, loss:1.5431594848632812\n",
      "训练次数：857, loss:1.3866491317749023\n",
      "训练次数：858, loss:1.6252776384353638\n",
      "训练次数：859, loss:1.47756028175354\n",
      "训练次数：860, loss:1.620105504989624\n",
      "训练次数：861, loss:1.34709632396698\n",
      "训练次数：862, loss:1.6987420320510864\n",
      "训练次数：863, loss:1.5504701137542725\n",
      "训练次数：864, loss:1.504071593284607\n",
      "训练次数：865, loss:1.5769352912902832\n",
      "训练次数：866, loss:1.4630552530288696\n",
      "训练次数：867, loss:1.5326210260391235\n",
      "训练次数：868, loss:1.4627940654754639\n",
      "训练次数：869, loss:1.4752192497253418\n",
      "训练次数：870, loss:1.254280924797058\n",
      "训练次数：871, loss:1.4246944189071655\n",
      "训练次数：872, loss:1.298315167427063\n",
      "训练次数：873, loss:1.5310006141662598\n",
      "训练次数：874, loss:1.4477283954620361\n",
      "训练次数：875, loss:1.2397191524505615\n",
      "训练次数：876, loss:1.3340696096420288\n",
      "训练次数：877, loss:1.5253859758377075\n",
      "训练次数：878, loss:1.4955635070800781\n",
      "训练次数：879, loss:1.115217685699463\n",
      "训练次数：880, loss:1.5430103540420532\n",
      "训练次数：881, loss:1.3003969192504883\n",
      "训练次数：882, loss:1.3228243589401245\n",
      "训练次数：883, loss:1.5610451698303223\n",
      "训练次数：884, loss:1.3327832221984863\n",
      "训练次数：885, loss:1.312291145324707\n",
      "训练次数：886, loss:1.404179334640503\n",
      "训练次数：887, loss:1.5987907648086548\n",
      "训练次数：888, loss:1.2955293655395508\n",
      "训练次数：889, loss:1.4237983226776123\n",
      "训练次数：890, loss:1.5141512155532837\n",
      "训练次数：891, loss:1.378116488456726\n",
      "训练次数：892, loss:1.3019648790359497\n",
      "训练次数：893, loss:1.4830034971237183\n",
      "训练次数：894, loss:1.2014367580413818\n",
      "训练次数：895, loss:1.4798612594604492\n",
      "训练次数：896, loss:1.209385633468628\n",
      "训练次数：897, loss:1.4651405811309814\n",
      "训练次数：898, loss:1.5812376737594604\n",
      "训练次数：899, loss:1.4001986980438232\n",
      "训练次数：900, loss:1.3825256824493408\n",
      "训练次数：901, loss:1.5563730001449585\n",
      "训练次数：902, loss:1.5557925701141357\n",
      "训练次数：903, loss:1.6438114643096924\n",
      "训练次数：904, loss:1.3604251146316528\n",
      "训练次数：905, loss:1.496813416481018\n",
      "训练次数：906, loss:1.4685696363449097\n",
      "训练次数：907, loss:1.4838504791259766\n",
      "训练次数：908, loss:1.2842495441436768\n",
      "训练次数：909, loss:1.362626552581787\n",
      "训练次数：910, loss:1.4038690328598022\n",
      "训练次数：911, loss:1.630003809928894\n",
      "训练次数：912, loss:1.329716682434082\n",
      "训练次数：913, loss:1.2395814657211304\n",
      "训练次数：914, loss:1.3109010457992554\n",
      "训练次数：915, loss:1.6592310667037964\n",
      "训练次数：916, loss:1.3478275537490845\n",
      "训练次数：917, loss:1.2304093837738037\n",
      "训练次数：918, loss:1.3885358572006226\n",
      "训练次数：919, loss:1.520159363746643\n",
      "训练次数：920, loss:1.5344222784042358\n",
      "训练次数：921, loss:1.2803837060928345\n",
      "训练次数：922, loss:1.563985824584961\n",
      "训练次数：923, loss:1.8719817399978638\n",
      "训练次数：924, loss:1.5316189527511597\n",
      "训练次数：925, loss:1.3718273639678955\n",
      "训练次数：926, loss:1.3434146642684937\n",
      "训练次数：927, loss:1.2993227243423462\n",
      "训练次数：928, loss:1.283101201057434\n",
      "训练次数：929, loss:1.3020776510238647\n",
      "训练次数：930, loss:1.4539412260055542\n",
      "训练次数：931, loss:1.1546589136123657\n",
      "训练次数：932, loss:1.1807814836502075\n",
      "训练次数：933, loss:1.3884638547897339\n",
      "训练次数：934, loss:1.4457225799560547\n",
      "训练次数：935, loss:1.3763424158096313\n",
      "训练次数：936, loss:1.1504169702529907\n",
      "训练次数：937, loss:1.3935863971710205\n",
      "训练次数：938, loss:1.2866108417510986\n",
      "训练次数：939, loss:1.5137240886688232\n",
      "训练次数：940, loss:1.190116286277771\n",
      "训练次数：941, loss:1.5299584865570068\n",
      "训练次数：942, loss:1.6555746793746948\n",
      "训练次数：943, loss:1.4673850536346436\n",
      "训练次数：944, loss:1.2396399974822998\n",
      "训练次数：945, loss:1.5243414640426636\n",
      "训练次数：946, loss:1.6511671543121338\n",
      "训练次数：947, loss:1.3223516941070557\n",
      "训练次数：948, loss:1.703809142112732\n",
      "训练次数：949, loss:1.413137674331665\n",
      "训练次数：950, loss:1.3069883584976196\n",
      "训练次数：951, loss:1.3027701377868652\n",
      "训练次数：952, loss:1.396790623664856\n",
      "训练次数：953, loss:1.5506551265716553\n",
      "训练次数：954, loss:1.2656919956207275\n",
      "训练次数：955, loss:1.2743563652038574\n",
      "训练次数：956, loss:1.692966103553772\n",
      "训练次数：957, loss:1.3100717067718506\n",
      "训练次数：958, loss:1.438712477684021\n",
      "训练次数：959, loss:1.238539457321167\n",
      "训练次数：960, loss:1.4454562664031982\n",
      "训练次数：961, loss:1.3928769826889038\n",
      "训练次数：962, loss:1.3484660387039185\n",
      "训练次数：963, loss:1.4339925050735474\n",
      "训练次数：964, loss:1.317462682723999\n",
      "训练次数：965, loss:1.349164366722107\n",
      "训练次数：966, loss:1.2130794525146484\n",
      "训练次数：967, loss:1.4458253383636475\n",
      "训练次数：968, loss:1.380400538444519\n",
      "训练次数：969, loss:1.5038220882415771\n",
      "训练次数：970, loss:1.4363946914672852\n",
      "训练次数：971, loss:1.6081881523132324\n",
      "训练次数：972, loss:1.3194907903671265\n",
      "训练次数：973, loss:1.2347638607025146\n",
      "训练次数：974, loss:1.5251879692077637\n",
      "训练次数：975, loss:1.755229115486145\n",
      "训练次数：976, loss:1.4149279594421387\n",
      "训练次数：977, loss:1.4576455354690552\n",
      "训练次数：978, loss:1.4310762882232666\n",
      "训练次数：979, loss:1.390944004058838\n",
      "训练次数：980, loss:1.6349238157272339\n",
      "训练次数：981, loss:1.6184425354003906\n",
      "训练次数：982, loss:1.4314484596252441\n",
      "训练次数：983, loss:1.110633373260498\n",
      "训练次数：984, loss:1.4409675598144531\n",
      "训练次数：985, loss:1.4988718032836914\n",
      "训练次数：986, loss:1.4840993881225586\n",
      "训练次数：987, loss:1.3086134195327759\n",
      "训练次数：988, loss:1.414925217628479\n",
      "训练次数：989, loss:1.259840726852417\n",
      "训练次数：990, loss:1.455807089805603\n",
      "训练次数：991, loss:1.2888895273208618\n",
      "训练次数：992, loss:1.5676157474517822\n",
      "训练次数：993, loss:1.3415557146072388\n",
      "训练次数：994, loss:1.2294968366622925\n",
      "训练次数：995, loss:1.214259147644043\n",
      "训练次数：996, loss:1.3829289674758911\n",
      "训练次数：997, loss:1.5297597646713257\n",
      "训练次数：998, loss:1.493053674697876\n",
      "训练次数：999, loss:1.335897445678711\n",
      "训练次数：1000, loss:1.2715164422988892\n",
      "训练次数：1001, loss:1.430749773979187\n",
      "训练次数：1002, loss:1.3459347486495972\n",
      "训练次数：1003, loss:1.3448717594146729\n",
      "训练次数：1004, loss:1.347023844718933\n",
      "训练次数：1005, loss:1.2724528312683105\n",
      "训练次数：1006, loss:1.3877798318862915\n",
      "训练次数：1007, loss:1.4428164958953857\n",
      "训练次数：1008, loss:1.4325673580169678\n",
      "训练次数：1009, loss:1.359882116317749\n",
      "训练次数：1010, loss:1.2318605184555054\n",
      "训练次数：1011, loss:1.262524962425232\n",
      "训练次数：1012, loss:1.5196033716201782\n",
      "训练次数：1013, loss:1.3208872079849243\n",
      "训练次数：1014, loss:1.402879238128662\n",
      "训练次数：1015, loss:1.5478864908218384\n",
      "训练次数：1016, loss:1.5041956901550293\n",
      "训练次数：1017, loss:1.3210129737854004\n",
      "训练次数：1018, loss:1.492222785949707\n",
      "训练次数：1019, loss:1.6642637252807617\n",
      "训练次数：1020, loss:1.3773159980773926\n",
      "训练次数：1021, loss:1.3956106901168823\n",
      "训练次数：1022, loss:1.296525001525879\n",
      "训练次数：1023, loss:1.3184363842010498\n",
      "训练次数：1024, loss:1.3707818984985352\n",
      "训练次数：1025, loss:1.3853230476379395\n",
      "训练次数：1026, loss:1.7602121829986572\n",
      "训练次数：1027, loss:1.1721445322036743\n",
      "训练次数：1028, loss:1.5803354978561401\n",
      "训练次数：1029, loss:1.320509910583496\n",
      "训练次数：1030, loss:1.3748245239257812\n",
      "训练次数：1031, loss:1.5180902481079102\n",
      "训练次数：1032, loss:1.407954454421997\n",
      "训练次数：1033, loss:1.3247073888778687\n",
      "训练次数：1034, loss:1.4165618419647217\n",
      "训练次数：1035, loss:1.3755271434783936\n",
      "训练次数：1036, loss:1.5397398471832275\n",
      "训练次数：1037, loss:1.2450143098831177\n",
      "训练次数：1038, loss:1.2094584703445435\n",
      "训练次数：1039, loss:1.35116708278656\n",
      "训练次数：1040, loss:1.2268580198287964\n",
      "训练次数：1041, loss:1.4671690464019775\n",
      "训练次数：1042, loss:1.5200103521347046\n",
      "训练次数：1043, loss:1.4261095523834229\n",
      "训练次数：1044, loss:1.337376356124878\n",
      "训练次数：1045, loss:1.4403488636016846\n",
      "训练次数：1046, loss:1.4322611093521118\n",
      "训练次数：1047, loss:1.1399810314178467\n",
      "训练次数：1048, loss:1.2515754699707031\n",
      "训练次数：1049, loss:1.5532338619232178\n",
      "训练次数：1050, loss:1.488535761833191\n",
      "训练次数：1051, loss:1.1717458963394165\n",
      "训练次数：1052, loss:1.5201109647750854\n",
      "训练次数：1053, loss:1.4778625965118408\n",
      "训练次数：1054, loss:1.300478219985962\n",
      "训练次数：1055, loss:1.2741743326187134\n",
      "训练次数：1056, loss:1.3020458221435547\n",
      "训练次数：1057, loss:1.261899709701538\n",
      "训练次数：1058, loss:1.2695441246032715\n",
      "训练次数：1059, loss:1.6161034107208252\n",
      "训练次数：1060, loss:1.4035124778747559\n",
      "训练次数：1061, loss:1.3826380968093872\n",
      "训练次数：1062, loss:1.2755018472671509\n",
      "训练次数：1063, loss:1.2239351272583008\n",
      "训练次数：1064, loss:1.4290791749954224\n",
      "训练次数：1065, loss:1.2849551439285278\n",
      "训练次数：1066, loss:1.3968801498413086\n",
      "训练次数：1067, loss:1.453540325164795\n",
      "训练次数：1068, loss:1.321781873703003\n",
      "训练次数：1069, loss:1.2631415128707886\n",
      "训练次数：1070, loss:1.288684368133545\n",
      "训练次数：1071, loss:1.5079165697097778\n",
      "训练次数：1072, loss:1.5326318740844727\n",
      "训练次数：1073, loss:1.3478517532348633\n",
      "训练次数：1074, loss:1.5091108083724976\n",
      "训练次数：1075, loss:1.161751389503479\n",
      "训练次数：1076, loss:1.3029980659484863\n",
      "训练次数：1077, loss:1.3428646326065063\n",
      "训练次数：1078, loss:1.4139586687088013\n",
      "训练次数：1079, loss:1.5445164442062378\n",
      "训练次数：1080, loss:1.3042776584625244\n",
      "训练次数：1081, loss:1.2417830228805542\n",
      "训练次数：1082, loss:1.4946995973587036\n",
      "训练次数：1083, loss:1.2871496677398682\n",
      "训练次数：1084, loss:1.2967668771743774\n",
      "训练次数：1085, loss:1.3225228786468506\n",
      "训练次数：1086, loss:1.2319176197052002\n",
      "训练次数：1087, loss:1.2239060401916504\n",
      "训练次数：1088, loss:1.486098051071167\n",
      "训练次数：1089, loss:1.2931106090545654\n",
      "训练次数：1090, loss:1.5162831544876099\n",
      "训练次数：1091, loss:1.6263959407806396\n",
      "训练次数：1092, loss:1.2558754682540894\n",
      "训练次数：1093, loss:1.2834993600845337\n",
      "训练次数：1094, loss:1.2490509748458862\n",
      "训练次数：1095, loss:1.6852052211761475\n",
      "训练次数：1096, loss:1.375320553779602\n",
      "训练次数：1097, loss:1.2923060655593872\n",
      "训练次数：1098, loss:1.3466700315475464\n",
      "训练次数：1099, loss:1.335607647895813\n",
      "训练次数：1100, loss:1.5372576713562012\n",
      "训练次数：1101, loss:1.2462164163589478\n",
      "训练次数：1102, loss:1.302274227142334\n",
      "训练次数：1103, loss:1.4337443113327026\n",
      "训练次数：1104, loss:1.390071153640747\n",
      "训练次数：1105, loss:1.3294730186462402\n",
      "训练次数：1106, loss:1.374879002571106\n",
      "训练次数：1107, loss:1.2074049711227417\n",
      "训练次数：1108, loss:1.3768677711486816\n",
      "训练次数：1109, loss:1.131002426147461\n",
      "训练次数：1110, loss:1.3615796566009521\n",
      "训练次数：1111, loss:1.2925622463226318\n",
      "训练次数：1112, loss:1.2643649578094482\n",
      "训练次数：1113, loss:1.313759684562683\n",
      "训练次数：1114, loss:1.209484577178955\n",
      "训练次数：1115, loss:1.3179700374603271\n",
      "训练次数：1116, loss:1.4310919046401978\n",
      "训练次数：1117, loss:1.4196511507034302\n",
      "训练次数：1118, loss:1.5090092420578003\n",
      "训练次数：1119, loss:1.2633209228515625\n",
      "训练次数：1120, loss:1.1553587913513184\n",
      "训练次数：1121, loss:1.2475783824920654\n",
      "训练次数：1122, loss:1.3193305730819702\n",
      "训练次数：1123, loss:1.3856446743011475\n",
      "训练次数：1124, loss:1.561644434928894\n",
      "训练次数：1125, loss:1.2468923330307007\n",
      "训练次数：1126, loss:1.2500171661376953\n",
      "训练次数：1127, loss:1.3714560270309448\n",
      "训练次数：1128, loss:1.299342155456543\n",
      "训练次数：1129, loss:1.2406959533691406\n",
      "训练次数：1130, loss:1.4816844463348389\n",
      "训练次数：1131, loss:1.297415018081665\n",
      "训练次数：1132, loss:1.3211948871612549\n",
      "训练次数：1133, loss:1.250700831413269\n",
      "训练次数：1134, loss:1.4226572513580322\n",
      "训练次数：1135, loss:1.179071307182312\n",
      "训练次数：1136, loss:1.22599196434021\n",
      "训练次数：1137, loss:1.2959002256393433\n",
      "训练次数：1138, loss:1.2641950845718384\n",
      "训练次数：1139, loss:1.3710248470306396\n",
      "训练次数：1140, loss:1.307567834854126\n",
      "训练次数：1141, loss:1.3447247743606567\n",
      "训练次数：1142, loss:1.3785691261291504\n",
      "训练次数：1143, loss:1.2806607484817505\n",
      "训练次数：1144, loss:0.9385806918144226\n",
      "训练次数：1145, loss:1.2761114835739136\n",
      "训练次数：1146, loss:1.2592525482177734\n",
      "训练次数：1147, loss:0.9625698924064636\n",
      "训练次数：1148, loss:1.0639674663543701\n",
      "训练次数：1149, loss:1.4099899530410767\n",
      "训练次数：1150, loss:1.404068112373352\n",
      "训练次数：1151, loss:1.1241796016693115\n",
      "训练次数：1152, loss:1.3173878192901611\n",
      "训练次数：1153, loss:0.9862397313117981\n",
      "训练次数：1154, loss:1.4058607816696167\n",
      "训练次数：1155, loss:1.221436858177185\n",
      "训练次数：1156, loss:1.2907683849334717\n",
      "训练次数：1157, loss:1.416301965713501\n",
      "训练次数：1158, loss:1.1491730213165283\n",
      "训练次数：1159, loss:1.1944694519042969\n",
      "训练次数：1160, loss:1.2288224697113037\n",
      "训练次数：1161, loss:1.4196608066558838\n",
      "训练次数：1162, loss:1.330994963645935\n",
      "训练次数：1163, loss:1.4952677488327026\n",
      "训练次数：1164, loss:1.2644881010055542\n",
      "训练次数：1165, loss:1.2334017753601074\n",
      "训练次数：1166, loss:1.1597002744674683\n",
      "训练次数：1167, loss:1.1319563388824463\n",
      "训练次数：1168, loss:1.068192481994629\n",
      "训练次数：1169, loss:1.2842776775360107\n",
      "训练次数：1170, loss:1.551346778869629\n",
      "训练次数：1171, loss:1.4321223497390747\n",
      "训练次数：1172, loss:1.229426383972168\n",
      "训练次数：1173, loss:1.173358678817749\n",
      "训练次数：1174, loss:0.9675053358078003\n",
      "训练次数：1175, loss:1.2586867809295654\n",
      "训练次数：1176, loss:1.5729271173477173\n",
      "训练次数：1177, loss:1.4346892833709717\n",
      "训练次数：1178, loss:1.2049190998077393\n",
      "训练次数：1179, loss:1.333676815032959\n",
      "训练次数：1180, loss:1.574297308921814\n",
      "训练次数：1181, loss:1.3286715745925903\n",
      "训练次数：1182, loss:1.1491541862487793\n",
      "训练次数：1183, loss:1.3306267261505127\n",
      "训练次数：1184, loss:1.527241587638855\n",
      "训练次数：1185, loss:1.3130195140838623\n",
      "训练次数：1186, loss:1.0906506776809692\n",
      "训练次数：1187, loss:1.2758466005325317\n",
      "训练次数：1188, loss:1.183042049407959\n",
      "训练次数：1189, loss:1.2894489765167236\n",
      "训练次数：1190, loss:0.9414712190628052\n",
      "训练次数：1191, loss:1.2816712856292725\n",
      "训练次数：1192, loss:1.0571260452270508\n",
      "训练次数：1193, loss:1.4829683303833008\n",
      "训练次数：1194, loss:1.2283401489257812\n",
      "训练次数：1195, loss:1.3663074970245361\n",
      "训练次数：1196, loss:1.3040577173233032\n",
      "训练次数：1197, loss:1.375734567642212\n",
      "训练次数：1198, loss:1.181075930595398\n",
      "训练次数：1199, loss:1.261869192123413\n",
      "训练次数：1200, loss:1.3365384340286255\n",
      "训练次数：1201, loss:1.296345829963684\n",
      "训练次数：1202, loss:1.265681266784668\n",
      "训练次数：1203, loss:1.2947748899459839\n",
      "训练次数：1204, loss:1.3202905654907227\n",
      "训练次数：1205, loss:1.379186987876892\n",
      "训练次数：1206, loss:1.2558661699295044\n",
      "训练次数：1207, loss:1.180207371711731\n",
      "训练次数：1208, loss:1.325247049331665\n",
      "训练次数：1209, loss:1.3246034383773804\n",
      "训练次数：1210, loss:1.117911458015442\n",
      "训练次数：1211, loss:1.2112189531326294\n",
      "训练次数：1212, loss:1.211974859237671\n",
      "训练次数：1213, loss:1.3813925981521606\n",
      "训练次数：1214, loss:1.1006039381027222\n",
      "训练次数：1215, loss:1.3032509088516235\n",
      "训练次数：1216, loss:1.2225124835968018\n",
      "训练次数：1217, loss:1.2781063318252563\n",
      "训练次数：1218, loss:1.5478482246398926\n",
      "训练次数：1219, loss:1.201216220855713\n",
      "训练次数：1220, loss:1.16200590133667\n",
      "训练次数：1221, loss:1.3054264783859253\n",
      "训练次数：1222, loss:1.3546879291534424\n",
      "训练次数：1223, loss:1.257807970046997\n",
      "训练次数：1224, loss:1.1217204332351685\n",
      "训练次数：1225, loss:1.4818971157073975\n",
      "训练次数：1226, loss:1.2153940200805664\n",
      "训练次数：1227, loss:1.4232606887817383\n",
      "训练次数：1228, loss:1.241847038269043\n",
      "训练次数：1229, loss:1.397721290588379\n",
      "训练次数：1230, loss:1.189753770828247\n",
      "训练次数：1231, loss:1.2496825456619263\n",
      "训练次数：1232, loss:1.449756145477295\n",
      "训练次数：1233, loss:1.0766355991363525\n",
      "训练次数：1234, loss:1.036081314086914\n",
      "训练次数：1235, loss:1.0890882015228271\n",
      "训练次数：1236, loss:1.2253912687301636\n",
      "训练次数：1237, loss:1.3100638389587402\n",
      "训练次数：1238, loss:1.3317147493362427\n",
      "训练次数：1239, loss:1.4216456413269043\n",
      "训练次数：1240, loss:1.215142846107483\n",
      "训练次数：1241, loss:1.0573437213897705\n",
      "训练次数：1242, loss:1.2350561618804932\n",
      "训练次数：1243, loss:1.2991468906402588\n",
      "训练次数：1244, loss:1.2745517492294312\n",
      "训练次数：1245, loss:1.2057064771652222\n",
      "训练次数：1246, loss:1.32905113697052\n",
      "训练次数：1247, loss:1.3161895275115967\n",
      "训练次数：1248, loss:0.9528892040252686\n",
      "训练次数：1249, loss:1.3293628692626953\n",
      "训练次数：1250, loss:1.1822320222854614\n",
      "训练次数：1251, loss:1.2679197788238525\n",
      "训练次数：1252, loss:1.460189938545227\n",
      "训练次数：1253, loss:1.19157874584198\n",
      "训练次数：1254, loss:1.2745925188064575\n",
      "训练次数：1255, loss:1.136240005493164\n",
      "训练次数：1256, loss:1.322971224784851\n",
      "训练次数：1257, loss:1.0630786418914795\n",
      "训练次数：1258, loss:1.3150818347930908\n",
      "训练次数：1259, loss:1.4165092706680298\n",
      "训练次数：1260, loss:1.2669464349746704\n",
      "训练次数：1261, loss:1.4353790283203125\n",
      "训练次数：1262, loss:1.3290811777114868\n",
      "训练次数：1263, loss:1.5184880495071411\n",
      "训练次数：1264, loss:1.4206740856170654\n",
      "训练次数：1265, loss:1.345701813697815\n",
      "训练次数：1266, loss:1.2172937393188477\n",
      "训练次数：1267, loss:1.2653330564498901\n",
      "训练次数：1268, loss:1.0454450845718384\n",
      "训练次数：1269, loss:1.4948195219039917\n",
      "训练次数：1270, loss:1.2944536209106445\n",
      "训练次数：1271, loss:1.2568485736846924\n",
      "训练次数：1272, loss:0.9484913945198059\n",
      "训练次数：1273, loss:1.8207309246063232\n",
      "训练次数：1274, loss:1.2352116107940674\n",
      "训练次数：1275, loss:1.3432468175888062\n",
      "训练次数：1276, loss:1.3306289911270142\n",
      "训练次数：1277, loss:1.333397626876831\n",
      "训练次数：1278, loss:1.3416436910629272\n",
      "训练次数：1279, loss:1.2175816297531128\n",
      "训练次数：1280, loss:1.4660184383392334\n",
      "训练次数：1281, loss:1.4662524461746216\n",
      "训练次数：1282, loss:1.111855387687683\n",
      "训练次数：1283, loss:1.4859100580215454\n",
      "训练次数：1284, loss:1.190519094467163\n",
      "训练次数：1285, loss:1.1979726552963257\n",
      "训练次数：1286, loss:1.4571828842163086\n",
      "训练次数：1287, loss:1.3657890558242798\n",
      "训练次数：1288, loss:1.3943390846252441\n",
      "训练次数：1289, loss:1.2460135221481323\n",
      "训练次数：1290, loss:1.3042372465133667\n",
      "训练次数：1291, loss:1.3238067626953125\n",
      "训练次数：1292, loss:1.3587485551834106\n",
      "训练次数：1293, loss:1.283683180809021\n",
      "训练次数：1294, loss:1.310112476348877\n",
      "训练次数：1295, loss:1.3227980136871338\n",
      "训练次数：1296, loss:1.4885114431381226\n",
      "训练次数：1297, loss:1.461159110069275\n",
      "训练次数：1298, loss:1.2905793190002441\n",
      "训练次数：1299, loss:1.3983852863311768\n",
      "训练次数：1300, loss:1.142353892326355\n",
      "训练次数：1301, loss:1.5311542749404907\n",
      "训练次数：1302, loss:1.28508460521698\n",
      "训练次数：1303, loss:1.3950297832489014\n",
      "训练次数：1304, loss:1.417800784111023\n",
      "训练次数：1305, loss:1.3179264068603516\n",
      "训练次数：1306, loss:1.4535921812057495\n",
      "训练次数：1307, loss:1.4466718435287476\n",
      "训练次数：1308, loss:1.54481840133667\n",
      "训练次数：1309, loss:1.3710849285125732\n",
      "训练次数：1310, loss:1.1073191165924072\n",
      "训练次数：1311, loss:1.2651408910751343\n",
      "训练次数：1312, loss:1.624380111694336\n",
      "训练次数：1313, loss:1.2878082990646362\n",
      "训练次数：1314, loss:1.3174165487289429\n",
      "训练次数：1315, loss:1.3276128768920898\n",
      "训练次数：1316, loss:0.9918920397758484\n",
      "训练次数：1317, loss:1.3623909950256348\n",
      "训练次数：1318, loss:1.222255825996399\n",
      "训练次数：1319, loss:1.550767421722412\n",
      "训练次数：1320, loss:1.4503203630447388\n",
      "训练次数：1321, loss:1.317704439163208\n",
      "训练次数：1322, loss:1.0547538995742798\n",
      "训练次数：1323, loss:1.172285556793213\n",
      "训练次数：1324, loss:1.4637447595596313\n",
      "训练次数：1325, loss:1.3231717348098755\n",
      "训练次数：1326, loss:1.341884732246399\n",
      "训练次数：1327, loss:1.2990117073059082\n",
      "训练次数：1328, loss:1.2267495393753052\n",
      "训练次数：1329, loss:1.49481999874115\n",
      "训练次数：1330, loss:1.2585432529449463\n",
      "训练次数：1331, loss:1.1581770181655884\n",
      "训练次数：1332, loss:1.3979848623275757\n",
      "训练次数：1333, loss:1.192487120628357\n",
      "训练次数：1334, loss:1.4828768968582153\n",
      "训练次数：1335, loss:1.1604691743850708\n",
      "训练次数：1336, loss:1.3053048849105835\n",
      "训练次数：1337, loss:1.3053780794143677\n",
      "训练次数：1338, loss:1.1824883222579956\n",
      "训练次数：1339, loss:1.419227957725525\n",
      "训练次数：1340, loss:1.1846041679382324\n",
      "训练次数：1341, loss:1.207304835319519\n",
      "训练次数：1342, loss:1.4065711498260498\n",
      "训练次数：1343, loss:1.2269285917282104\n",
      "训练次数：1344, loss:1.4589869976043701\n",
      "训练次数：1345, loss:1.4944273233413696\n",
      "训练次数：1346, loss:1.169097661972046\n",
      "训练次数：1347, loss:1.3334119319915771\n",
      "训练次数：1348, loss:1.2124141454696655\n",
      "训练次数：1349, loss:1.1384273767471313\n",
      "训练次数：1350, loss:1.3282406330108643\n",
      "训练次数：1351, loss:1.2530285120010376\n",
      "训练次数：1352, loss:1.188832402229309\n",
      "训练次数：1353, loss:1.1293344497680664\n",
      "训练次数：1354, loss:1.0720330476760864\n",
      "训练次数：1355, loss:1.1558595895767212\n",
      "训练次数：1356, loss:1.5345282554626465\n",
      "训练次数：1357, loss:1.3026835918426514\n",
      "训练次数：1358, loss:1.134875774383545\n",
      "训练次数：1359, loss:1.3233178853988647\n",
      "训练次数：1360, loss:1.5449610948562622\n",
      "训练次数：1361, loss:1.1680116653442383\n",
      "训练次数：1362, loss:1.2365236282348633\n",
      "训练次数：1363, loss:1.3862309455871582\n",
      "训练次数：1364, loss:1.3379141092300415\n",
      "训练次数：1365, loss:1.1371760368347168\n",
      "训练次数：1366, loss:1.4812097549438477\n",
      "训练次数：1367, loss:1.326301097869873\n",
      "训练次数：1368, loss:1.1602591276168823\n",
      "训练次数：1369, loss:1.3992395401000977\n",
      "训练次数：1370, loss:1.1468123197555542\n",
      "训练次数：1371, loss:1.4177230596542358\n",
      "训练次数：1372, loss:1.3089784383773804\n",
      "训练次数：1373, loss:1.3180640935897827\n",
      "训练次数：1374, loss:1.0435669422149658\n",
      "训练次数：1375, loss:1.44368577003479\n",
      "训练次数：1376, loss:1.2648186683654785\n",
      "训练次数：1377, loss:1.4606188535690308\n",
      "训练次数：1378, loss:1.5339478254318237\n",
      "训练次数：1379, loss:1.2300376892089844\n",
      "训练次数：1380, loss:1.262113332748413\n",
      "训练次数：1381, loss:1.451632022857666\n",
      "训练次数：1382, loss:1.2766979932785034\n",
      "训练次数：1383, loss:1.4171994924545288\n",
      "训练次数：1384, loss:1.1828643083572388\n",
      "训练次数：1385, loss:0.9943241477012634\n",
      "训练次数：1386, loss:1.5563037395477295\n",
      "训练次数：1387, loss:1.1845818758010864\n",
      "训练次数：1388, loss:1.0417335033416748\n",
      "训练次数：1389, loss:0.973199188709259\n",
      "训练次数：1390, loss:1.1860965490341187\n",
      "训练次数：1391, loss:1.2889524698257446\n",
      "训练次数：1392, loss:1.2798727750778198\n",
      "训练次数：1393, loss:1.220595121383667\n",
      "训练次数：1394, loss:1.449076533317566\n",
      "训练次数：1395, loss:1.2446017265319824\n",
      "训练次数：1396, loss:1.060948133468628\n",
      "训练次数：1397, loss:1.243043303489685\n",
      "训练次数：1398, loss:1.0536695718765259\n",
      "训练次数：1399, loss:1.3172873258590698\n",
      "训练次数：1400, loss:1.1805979013442993\n",
      "训练次数：1401, loss:1.4155899286270142\n",
      "训练次数：1402, loss:1.5884469747543335\n",
      "训练次数：1403, loss:1.395824909210205\n",
      "训练次数：1404, loss:1.4552775621414185\n",
      "训练次数：1405, loss:1.1195462942123413\n",
      "训练次数：1406, loss:1.0701179504394531\n",
      "训练次数：1407, loss:1.2581170797348022\n",
      "训练次数：1408, loss:1.0771756172180176\n",
      "训练次数：1409, loss:1.2759941816329956\n",
      "训练次数：1410, loss:1.4932156801223755\n",
      "训练次数：1411, loss:1.2897799015045166\n",
      "训练次数：1412, loss:1.172349214553833\n",
      "训练次数：1413, loss:1.2713861465454102\n",
      "训练次数：1414, loss:1.4479137659072876\n",
      "训练次数：1415, loss:1.4331437349319458\n",
      "训练次数：1416, loss:0.9944167733192444\n",
      "训练次数：1417, loss:1.1841233968734741\n",
      "训练次数：1418, loss:1.0114357471466064\n",
      "训练次数：1419, loss:1.274175763130188\n",
      "训练次数：1420, loss:1.358413577079773\n",
      "训练次数：1421, loss:1.2859081029891968\n",
      "训练次数：1422, loss:1.4838941097259521\n",
      "训练次数：1423, loss:1.3038166761398315\n",
      "训练次数：1424, loss:1.349481463432312\n",
      "训练次数：1425, loss:1.374412178993225\n",
      "训练次数：1426, loss:1.1288594007492065\n",
      "训练次数：1427, loss:1.131618618965149\n",
      "训练次数：1428, loss:1.4655534029006958\n",
      "训练次数：1429, loss:1.296098232269287\n",
      "训练次数：1430, loss:1.1447018384933472\n",
      "训练次数：1431, loss:1.2829535007476807\n",
      "训练次数：1432, loss:1.65428626537323\n",
      "训练次数：1433, loss:1.1854264736175537\n",
      "训练次数：1434, loss:1.1715329885482788\n",
      "训练次数：1435, loss:1.0465977191925049\n",
      "训练次数：1436, loss:1.223134160041809\n",
      "训练次数：1437, loss:1.356226921081543\n",
      "训练次数：1438, loss:1.4055707454681396\n",
      "训练次数：1439, loss:1.3128247261047363\n",
      "训练次数：1440, loss:1.2376611232757568\n",
      "训练次数：1441, loss:1.5085755586624146\n",
      "训练次数：1442, loss:1.274173617362976\n",
      "训练次数：1443, loss:1.131192684173584\n",
      "训练次数：1444, loss:1.122676134109497\n",
      "训练次数：1445, loss:1.1779049634933472\n",
      "训练次数：1446, loss:1.3138805627822876\n",
      "训练次数：1447, loss:1.4130089282989502\n",
      "训练次数：1448, loss:1.562256097793579\n",
      "训练次数：1449, loss:1.229785680770874\n",
      "训练次数：1450, loss:1.4990028142929077\n",
      "训练次数：1451, loss:1.246159315109253\n",
      "训练次数：1452, loss:1.2465492486953735\n",
      "训练次数：1453, loss:1.3349406719207764\n",
      "训练次数：1454, loss:1.3337939977645874\n",
      "训练次数：1455, loss:1.2056138515472412\n",
      "训练次数：1456, loss:1.2838594913482666\n",
      "训练次数：1457, loss:1.331864595413208\n",
      "训练次数：1458, loss:1.551992654800415\n",
      "训练次数：1459, loss:1.3782801628112793\n",
      "训练次数：1460, loss:1.168421745300293\n",
      "训练次数：1461, loss:1.259228229522705\n",
      "训练次数：1462, loss:1.2345056533813477\n",
      "训练次数：1463, loss:1.3791922330856323\n",
      "训练次数：1464, loss:1.0184561014175415\n",
      "训练次数：1465, loss:1.0282953977584839\n",
      "训练次数：1466, loss:1.4454740285873413\n",
      "训练次数：1467, loss:1.3270560503005981\n",
      "训练次数：1468, loss:1.5392149686813354\n",
      "训练次数：1469, loss:1.1393299102783203\n",
      "训练次数：1470, loss:1.1932772397994995\n",
      "训练次数：1471, loss:1.247852087020874\n",
      "训练次数：1472, loss:1.3882365226745605\n",
      "训练次数：1473, loss:1.3300387859344482\n",
      "训练次数：1474, loss:1.3417561054229736\n",
      "训练次数：1475, loss:1.459530234336853\n",
      "训练次数：1476, loss:1.2670092582702637\n",
      "训练次数：1477, loss:1.1453132629394531\n",
      "训练次数：1478, loss:1.3267629146575928\n",
      "训练次数：1479, loss:1.2652517557144165\n",
      "训练次数：1480, loss:1.3218350410461426\n",
      "训练次数：1481, loss:1.1096800565719604\n",
      "训练次数：1482, loss:1.3069013357162476\n",
      "训练次数：1483, loss:1.2944782972335815\n",
      "训练次数：1484, loss:1.287961721420288\n",
      "训练次数：1485, loss:1.2567780017852783\n",
      "训练次数：1486, loss:1.0469403266906738\n",
      "训练次数：1487, loss:1.070404291152954\n",
      "训练次数：1488, loss:1.1864362955093384\n",
      "训练次数：1489, loss:1.4826421737670898\n",
      "训练次数：1490, loss:1.2033555507659912\n",
      "训练次数：1491, loss:1.0821903944015503\n",
      "训练次数：1492, loss:1.0621092319488525\n",
      "训练次数：1493, loss:1.4119828939437866\n",
      "训练次数：1494, loss:1.0490479469299316\n",
      "训练次数：1495, loss:1.1806981563568115\n",
      "训练次数：1496, loss:1.2056281566619873\n",
      "训练次数：1497, loss:1.2283247709274292\n",
      "训练次数：1498, loss:1.1591176986694336\n",
      "训练次数：1499, loss:1.119569182395935\n",
      "训练次数：1500, loss:1.2209739685058594\n",
      "训练次数：1501, loss:1.1447982788085938\n",
      "训练次数：1502, loss:1.085554838180542\n",
      "训练次数：1503, loss:1.0693618059158325\n",
      "训练次数：1504, loss:1.1383236646652222\n",
      "训练次数：1505, loss:1.0148720741271973\n",
      "训练次数：1506, loss:1.7427798509597778\n",
      "训练次数：1507, loss:1.3645976781845093\n",
      "训练次数：1508, loss:1.0505943298339844\n",
      "训练次数：1509, loss:1.4786542654037476\n",
      "训练次数：1510, loss:1.5122251510620117\n",
      "训练次数：1511, loss:1.1614934206008911\n",
      "训练次数：1512, loss:1.428159236907959\n",
      "训练次数：1513, loss:1.0205174684524536\n",
      "训练次数：1514, loss:1.1282117366790771\n",
      "训练次数：1515, loss:1.1412914991378784\n",
      "训练次数：1516, loss:1.1922094821929932\n",
      "训练次数：1517, loss:1.1608402729034424\n",
      "训练次数：1518, loss:1.3692487478256226\n",
      "训练次数：1519, loss:1.0243281126022339\n",
      "训练次数：1520, loss:1.1778264045715332\n",
      "训练次数：1521, loss:1.06356680393219\n",
      "训练次数：1522, loss:1.142457365989685\n",
      "训练次数：1523, loss:1.2246376276016235\n",
      "训练次数：1524, loss:1.2432498931884766\n",
      "训练次数：1525, loss:1.434903860092163\n",
      "训练次数：1526, loss:0.8790062069892883\n",
      "训练次数：1527, loss:1.1041241884231567\n",
      "训练次数：1528, loss:1.233543038368225\n",
      "训练次数：1529, loss:1.0796247720718384\n",
      "训练次数：1530, loss:1.389251708984375\n",
      "训练次数：1531, loss:1.3969720602035522\n",
      "训练次数：1532, loss:1.053727388381958\n",
      "训练次数：1533, loss:1.7271004915237427\n",
      "训练次数：1534, loss:1.2856932878494263\n",
      "训练次数：1535, loss:1.2227803468704224\n",
      "训练次数：1536, loss:1.2926688194274902\n",
      "训练次数：1537, loss:1.099801778793335\n",
      "训练次数：1538, loss:1.1888232231140137\n",
      "训练次数：1539, loss:1.3064345121383667\n",
      "训练次数：1540, loss:1.1477513313293457\n",
      "训练次数：1541, loss:1.1156829595565796\n",
      "训练次数：1542, loss:1.3158643245697021\n",
      "训练次数：1543, loss:1.1336710453033447\n",
      "训练次数：1544, loss:1.2805392742156982\n",
      "训练次数：1545, loss:1.2004891633987427\n",
      "训练次数：1546, loss:1.1652015447616577\n",
      "训练次数：1547, loss:1.1762561798095703\n",
      "训练次数：1548, loss:1.2864649295806885\n",
      "训练次数：1549, loss:0.9858160018920898\n",
      "训练次数：1550, loss:1.1613023281097412\n",
      "训练次数：1551, loss:1.0976136922836304\n",
      "训练次数：1552, loss:1.0593745708465576\n",
      "训练次数：1553, loss:1.3461380004882812\n",
      "训练次数：1554, loss:1.3194183111190796\n",
      "训练次数：1555, loss:1.2605494260787964\n",
      "训练次数：1556, loss:1.0203516483306885\n",
      "训练次数：1557, loss:1.313646912574768\n",
      "训练次数：1558, loss:1.1050955057144165\n",
      "训练次数：1559, loss:1.30978262424469\n",
      "训练次数：1560, loss:1.1575303077697754\n",
      "训练次数：1561, loss:1.1651567220687866\n",
      "训练次数：1562, loss:1.0276509523391724\n",
      "训练次数：1563, loss:1.2469682693481445\n",
      "训练次数：1564, loss:1.7893078327178955\n",
      "----------第2轮训练开始----------\n",
      "训练次数：1565, loss:1.4059078693389893\n",
      "训练次数：1566, loss:1.0529720783233643\n",
      "训练次数：1567, loss:1.3719090223312378\n",
      "训练次数：1568, loss:0.9610034823417664\n",
      "训练次数：1569, loss:1.1057119369506836\n",
      "训练次数：1570, loss:1.1829397678375244\n",
      "训练次数：1571, loss:1.3015193939208984\n",
      "训练次数：1572, loss:1.3920024633407593\n",
      "训练次数：1573, loss:1.108446478843689\n",
      "训练次数：1574, loss:1.4748674631118774\n",
      "训练次数：1575, loss:1.4226617813110352\n",
      "训练次数：1576, loss:1.2442982196807861\n",
      "训练次数：1577, loss:1.464510440826416\n",
      "训练次数：1578, loss:1.3933286666870117\n",
      "训练次数：1579, loss:1.230180025100708\n",
      "训练次数：1580, loss:1.055801272392273\n",
      "训练次数：1581, loss:1.2437986135482788\n",
      "训练次数：1582, loss:1.1793854236602783\n",
      "训练次数：1583, loss:1.048143744468689\n",
      "训练次数：1584, loss:1.0794280767440796\n",
      "训练次数：1585, loss:1.5967618227005005\n",
      "训练次数：1586, loss:1.4782754182815552\n",
      "训练次数：1587, loss:1.3485956192016602\n",
      "训练次数：1588, loss:1.203573226928711\n",
      "训练次数：1589, loss:1.228850245475769\n",
      "训练次数：1590, loss:1.4247270822525024\n",
      "训练次数：1591, loss:1.2728163003921509\n",
      "训练次数：1592, loss:1.218957543373108\n",
      "训练次数：1593, loss:1.1435604095458984\n",
      "训练次数：1594, loss:1.1547996997833252\n",
      "训练次数：1595, loss:1.484785556793213\n",
      "训练次数：1596, loss:1.3190038204193115\n",
      "训练次数：1597, loss:1.0262610912322998\n",
      "训练次数：1598, loss:1.0438538789749146\n",
      "训练次数：1599, loss:1.1294455528259277\n",
      "训练次数：1600, loss:1.0992151498794556\n",
      "训练次数：1601, loss:0.9903625249862671\n",
      "训练次数：1602, loss:1.1959905624389648\n",
      "训练次数：1603, loss:1.0290791988372803\n",
      "训练次数：1604, loss:1.2618114948272705\n",
      "训练次数：1605, loss:0.9161272644996643\n",
      "训练次数：1606, loss:0.9917697310447693\n",
      "训练次数：1607, loss:1.2343618869781494\n",
      "训练次数：1608, loss:1.4575722217559814\n",
      "训练次数：1609, loss:0.9596143364906311\n",
      "训练次数：1610, loss:1.0931445360183716\n",
      "训练次数：1611, loss:1.2781281471252441\n",
      "训练次数：1612, loss:1.2297940254211426\n",
      "训练次数：1613, loss:1.1727601289749146\n",
      "训练次数：1614, loss:1.3266366720199585\n",
      "训练次数：1615, loss:1.1778969764709473\n",
      "训练次数：1616, loss:1.185368537902832\n",
      "训练次数：1617, loss:1.1068419218063354\n",
      "训练次数：1618, loss:1.2509428262710571\n",
      "训练次数：1619, loss:1.129930853843689\n",
      "训练次数：1620, loss:1.023484468460083\n",
      "训练次数：1621, loss:1.1804002523422241\n",
      "训练次数：1622, loss:0.9677935242652893\n",
      "训练次数：1623, loss:0.9035170674324036\n",
      "训练次数：1624, loss:1.0657076835632324\n",
      "训练次数：1625, loss:1.263971209526062\n",
      "训练次数：1626, loss:1.1995903253555298\n",
      "训练次数：1627, loss:1.2711502313613892\n",
      "训练次数：1628, loss:1.1576482057571411\n",
      "训练次数：1629, loss:1.2129946947097778\n",
      "训练次数：1630, loss:1.1929404735565186\n",
      "训练次数：1631, loss:0.9863539338111877\n",
      "训练次数：1632, loss:1.1490590572357178\n",
      "训练次数：1633, loss:1.0248748064041138\n",
      "训练次数：1634, loss:1.0352153778076172\n",
      "训练次数：1635, loss:1.4832978248596191\n",
      "训练次数：1636, loss:0.9175475239753723\n",
      "训练次数：1637, loss:1.165916085243225\n",
      "训练次数：1638, loss:1.1913102865219116\n",
      "训练次数：1639, loss:1.3339611291885376\n",
      "训练次数：1640, loss:1.3370217084884644\n",
      "训练次数：1641, loss:1.196923851966858\n",
      "训练次数：1642, loss:1.361454963684082\n",
      "训练次数：1643, loss:1.105290412902832\n",
      "训练次数：1644, loss:1.4159070253372192\n",
      "训练次数：1645, loss:1.3820626735687256\n",
      "训练次数：1646, loss:1.1515758037567139\n",
      "训练次数：1647, loss:1.2329503297805786\n",
      "训练次数：1648, loss:1.0415704250335693\n",
      "训练次数：1649, loss:1.4480438232421875\n",
      "训练次数：1650, loss:1.1634690761566162\n",
      "训练次数：1651, loss:1.1400337219238281\n",
      "训练次数：1652, loss:1.0625321865081787\n",
      "训练次数：1653, loss:1.2128429412841797\n",
      "训练次数：1654, loss:1.100225567817688\n",
      "训练次数：1655, loss:1.261838436126709\n",
      "训练次数：1656, loss:1.27265465259552\n",
      "训练次数：1657, loss:1.0830568075180054\n",
      "训练次数：1658, loss:1.0707499980926514\n",
      "训练次数：1659, loss:1.2827329635620117\n",
      "训练次数：1660, loss:1.248935341835022\n",
      "训练次数：1661, loss:0.899853527545929\n",
      "训练次数：1662, loss:1.1384536027908325\n",
      "训练次数：1663, loss:1.1566048860549927\n",
      "训练次数：1664, loss:1.1087876558303833\n",
      "训练次数：1665, loss:1.1221415996551514\n",
      "训练次数：1666, loss:0.9838069677352905\n",
      "训练次数：1667, loss:0.9934828877449036\n",
      "训练次数：1668, loss:1.194591760635376\n",
      "训练次数：1669, loss:1.1776891946792603\n",
      "训练次数：1670, loss:0.9888336062431335\n",
      "训练次数：1671, loss:1.1783961057662964\n",
      "训练次数：1672, loss:1.3166269063949585\n",
      "训练次数：1673, loss:1.022519588470459\n",
      "训练次数：1674, loss:1.1937975883483887\n",
      "训练次数：1675, loss:1.3135921955108643\n",
      "训练次数：1676, loss:0.9374148845672607\n",
      "训练次数：1677, loss:1.2403744459152222\n",
      "训练次数：1678, loss:1.046940565109253\n",
      "训练次数：1679, loss:1.4247037172317505\n",
      "训练次数：1680, loss:1.4604814052581787\n",
      "训练次数：1681, loss:1.2814122438430786\n",
      "训练次数：1682, loss:1.015590786933899\n",
      "训练次数：1683, loss:1.2626581192016602\n",
      "训练次数：1684, loss:1.4572522640228271\n",
      "训练次数：1685, loss:1.334867000579834\n",
      "训练次数：1686, loss:1.303067922592163\n",
      "训练次数：1687, loss:1.27383291721344\n",
      "训练次数：1688, loss:1.17629075050354\n",
      "训练次数：1689, loss:1.2711541652679443\n",
      "训练次数：1690, loss:1.240105152130127\n",
      "训练次数：1691, loss:1.0289689302444458\n",
      "训练次数：1692, loss:1.1108124256134033\n",
      "训练次数：1693, loss:1.4628864526748657\n",
      "训练次数：1694, loss:1.1204068660736084\n",
      "训练次数：1695, loss:1.2139489650726318\n",
      "训练次数：1696, loss:1.016376256942749\n",
      "训练次数：1697, loss:1.5165220499038696\n",
      "训练次数：1698, loss:1.1269240379333496\n",
      "训练次数：1699, loss:1.1154112815856934\n",
      "训练次数：1700, loss:1.0809977054595947\n",
      "训练次数：1701, loss:1.3979099988937378\n",
      "训练次数：1702, loss:1.4527486562728882\n",
      "训练次数：1703, loss:1.0399518013000488\n",
      "训练次数：1704, loss:1.4658217430114746\n",
      "训练次数：1705, loss:1.6535817384719849\n",
      "训练次数：1706, loss:1.2307329177856445\n",
      "训练次数：1707, loss:1.3304418325424194\n",
      "训练次数：1708, loss:1.1764336824417114\n",
      "训练次数：1709, loss:1.235505223274231\n",
      "训练次数：1710, loss:1.271396279335022\n",
      "训练次数：1711, loss:1.2584139108657837\n",
      "训练次数：1712, loss:1.446533441543579\n",
      "训练次数：1713, loss:1.0590198040008545\n",
      "训练次数：1714, loss:1.0738966464996338\n",
      "训练次数：1715, loss:1.1488373279571533\n",
      "训练次数：1716, loss:1.2668486833572388\n",
      "训练次数：1717, loss:1.1605405807495117\n",
      "训练次数：1718, loss:1.0182502269744873\n",
      "训练次数：1719, loss:1.3063619136810303\n",
      "训练次数：1720, loss:1.1439989805221558\n",
      "训练次数：1721, loss:1.2479147911071777\n",
      "训练次数：1722, loss:1.0398136377334595\n",
      "训练次数：1723, loss:1.3488304615020752\n",
      "训练次数：1724, loss:1.3872089385986328\n",
      "训练次数：1725, loss:1.2639840841293335\n",
      "训练次数：1726, loss:1.0346673727035522\n",
      "训练次数：1727, loss:1.4701536893844604\n",
      "训练次数：1728, loss:1.4829262495040894\n",
      "训练次数：1729, loss:1.045169472694397\n",
      "训练次数：1730, loss:1.3223956823349\n",
      "训练次数：1731, loss:1.2228498458862305\n",
      "训练次数：1732, loss:1.1488364934921265\n",
      "训练次数：1733, loss:1.0075863599777222\n",
      "训练次数：1734, loss:1.1128120422363281\n",
      "训练次数：1735, loss:1.3314317464828491\n",
      "训练次数：1736, loss:1.1231136322021484\n",
      "训练次数：1737, loss:1.0751571655273438\n",
      "训练次数：1738, loss:1.3004242181777954\n",
      "训练次数：1739, loss:1.005896806716919\n",
      "训练次数：1740, loss:1.2484402656555176\n",
      "训练次数：1741, loss:1.0329389572143555\n",
      "训练次数：1742, loss:1.2484748363494873\n",
      "训练次数：1743, loss:1.1663848161697388\n",
      "训练次数：1744, loss:1.1641730070114136\n",
      "训练次数：1745, loss:1.2493442296981812\n",
      "训练次数：1746, loss:1.1450971364974976\n",
      "训练次数：1747, loss:1.2392005920410156\n",
      "训练次数：1748, loss:1.0536513328552246\n",
      "训练次数：1749, loss:1.2409272193908691\n",
      "训练次数：1750, loss:1.240079641342163\n",
      "训练次数：1751, loss:1.3698402643203735\n",
      "训练次数：1752, loss:1.0557183027267456\n",
      "训练次数：1753, loss:1.1895725727081299\n",
      "训练次数：1754, loss:1.252375602722168\n",
      "训练次数：1755, loss:0.969961941242218\n",
      "训练次数：1756, loss:1.3055076599121094\n",
      "训练次数：1757, loss:1.4898526668548584\n",
      "训练次数：1758, loss:1.084686040878296\n",
      "训练次数：1759, loss:1.1128329038619995\n",
      "训练次数：1760, loss:1.275875210762024\n",
      "训练次数：1761, loss:1.3514314889907837\n",
      "训练次数：1762, loss:1.2889599800109863\n",
      "训练次数：1763, loss:1.326796054840088\n",
      "训练次数：1764, loss:1.179362416267395\n",
      "训练次数：1765, loss:1.025310754776001\n",
      "训练次数：1766, loss:1.2169371843338013\n",
      "训练次数：1767, loss:1.2323397397994995\n",
      "训练次数：1768, loss:1.4070676565170288\n",
      "训练次数：1769, loss:1.2990871667861938\n",
      "训练次数：1770, loss:1.1211849451065063\n",
      "训练次数：1771, loss:1.081796407699585\n",
      "训练次数：1772, loss:1.2238129377365112\n",
      "训练次数：1773, loss:1.0651684999465942\n",
      "训练次数：1774, loss:1.5135743618011475\n",
      "训练次数：1775, loss:1.1335073709487915\n",
      "训练次数：1776, loss:1.112982153892517\n",
      "训练次数：1777, loss:1.151656985282898\n",
      "训练次数：1778, loss:1.2328466176986694\n",
      "训练次数：1779, loss:1.4907492399215698\n",
      "训练次数：1780, loss:1.3074796199798584\n",
      "训练次数：1781, loss:1.204030156135559\n",
      "训练次数：1782, loss:1.3593692779541016\n",
      "训练次数：1783, loss:1.2112985849380493\n",
      "训练次数：1784, loss:1.159662127494812\n",
      "训练次数：1785, loss:1.245919942855835\n",
      "训练次数：1786, loss:1.3099273443222046\n",
      "训练次数：1787, loss:1.1395186185836792\n",
      "训练次数：1788, loss:1.3633939027786255\n",
      "训练次数：1789, loss:1.3244630098342896\n",
      "训练次数：1790, loss:1.1337924003601074\n",
      "训练次数：1791, loss:1.167275071144104\n",
      "训练次数：1792, loss:0.9384657740592957\n",
      "训练次数：1793, loss:1.2286087274551392\n",
      "训练次数：1794, loss:1.339411735534668\n",
      "训练次数：1795, loss:1.1272403001785278\n",
      "训练次数：1796, loss:1.3459914922714233\n",
      "训练次数：1797, loss:1.1740113496780396\n",
      "训练次数：1798, loss:1.2866379022598267\n",
      "训练次数：1799, loss:1.0579813718795776\n",
      "训练次数：1800, loss:1.3227587938308716\n",
      "训练次数：1801, loss:1.226726770401001\n",
      "训练次数：1802, loss:1.1769680976867676\n",
      "训练次数：1803, loss:1.1298922300338745\n",
      "训练次数：1804, loss:1.0740948915481567\n",
      "训练次数：1805, loss:1.1906921863555908\n",
      "训练次数：1806, loss:1.2538249492645264\n",
      "训练次数：1807, loss:1.284907341003418\n",
      "训练次数：1808, loss:1.6610370874404907\n",
      "训练次数：1809, loss:0.9983819127082825\n",
      "训练次数：1810, loss:1.4008073806762695\n",
      "训练次数：1811, loss:1.0881757736206055\n",
      "训练次数：1812, loss:1.5116021633148193\n",
      "训练次数：1813, loss:1.4137890338897705\n",
      "训练次数：1814, loss:1.2458709478378296\n",
      "训练次数：1815, loss:1.0144097805023193\n",
      "训练次数：1816, loss:1.0666465759277344\n",
      "训练次数：1817, loss:1.2458311319351196\n",
      "训练次数：1818, loss:1.328873634338379\n",
      "训练次数：1819, loss:1.1405127048492432\n",
      "训练次数：1820, loss:0.9378353357315063\n",
      "训练次数：1821, loss:1.287001132965088\n",
      "训练次数：1822, loss:0.9968580007553101\n",
      "训练次数：1823, loss:1.4510316848754883\n",
      "训练次数：1824, loss:1.2268356084823608\n",
      "训练次数：1825, loss:1.1663917303085327\n",
      "训练次数：1826, loss:1.0416470766067505\n",
      "训练次数：1827, loss:1.496506690979004\n",
      "训练次数：1828, loss:1.1623973846435547\n",
      "训练次数：1829, loss:1.0114614963531494\n",
      "训练次数：1830, loss:1.0608487129211426\n",
      "训练次数：1831, loss:1.3718847036361694\n",
      "训练次数：1832, loss:1.0981425046920776\n",
      "训练次数：1833, loss:0.9547139406204224\n",
      "训练次数：1834, loss:1.1350905895233154\n",
      "训练次数：1835, loss:1.472405195236206\n",
      "训练次数：1836, loss:1.1133203506469727\n",
      "训练次数：1837, loss:1.020067572593689\n",
      "训练次数：1838, loss:1.163018822669983\n",
      "训练次数：1839, loss:1.0124914646148682\n",
      "训练次数：1840, loss:1.0220451354980469\n",
      "训练次数：1841, loss:1.6257420778274536\n",
      "训练次数：1842, loss:1.3330177068710327\n",
      "训练次数：1843, loss:1.2455239295959473\n",
      "训练次数：1844, loss:1.116845965385437\n",
      "训练次数：1845, loss:1.0181300640106201\n",
      "训练次数：1846, loss:1.2552653551101685\n",
      "训练次数：1847, loss:1.053153395652771\n",
      "训练次数：1848, loss:1.2326397895812988\n",
      "训练次数：1849, loss:1.085673213005066\n",
      "训练次数：1850, loss:1.3016135692596436\n",
      "训练次数：1851, loss:1.092050313949585\n",
      "训练次数：1852, loss:1.1867330074310303\n",
      "训练次数：1853, loss:1.2114349603652954\n",
      "训练次数：1854, loss:1.28008234500885\n",
      "训练次数：1855, loss:1.200527310371399\n",
      "训练次数：1856, loss:1.2827086448669434\n",
      "训练次数：1857, loss:1.0590040683746338\n",
      "训练次数：1858, loss:1.1979835033416748\n",
      "训练次数：1859, loss:1.132022500038147\n",
      "训练次数：1860, loss:1.1133915185928345\n",
      "训练次数：1861, loss:1.2565240859985352\n",
      "训练次数：1862, loss:1.1456913948059082\n",
      "训练次数：1863, loss:1.117132306098938\n",
      "训练次数：1864, loss:1.2854918241500854\n",
      "训练次数：1865, loss:1.0853766202926636\n",
      "训练次数：1866, loss:1.0069047212600708\n",
      "训练次数：1867, loss:1.2312960624694824\n",
      "训练次数：1868, loss:0.9505500197410583\n",
      "训练次数：1869, loss:0.9906790256500244\n",
      "训练次数：1870, loss:1.4836124181747437\n",
      "训练次数：1871, loss:1.2239885330200195\n",
      "训练次数：1872, loss:1.3451569080352783\n",
      "训练次数：1873, loss:1.4602795839309692\n",
      "训练次数：1874, loss:1.1914708614349365\n",
      "训练次数：1875, loss:1.183982014656067\n",
      "训练次数：1876, loss:1.0354199409484863\n",
      "训练次数：1877, loss:1.3752167224884033\n",
      "训练次数：1878, loss:1.1173877716064453\n",
      "训练次数：1879, loss:1.2051516771316528\n",
      "训练次数：1880, loss:1.4815794229507446\n",
      "训练次数：1881, loss:1.193089485168457\n",
      "训练次数：1882, loss:1.280491590499878\n",
      "训练次数：1883, loss:1.1480084657669067\n",
      "训练次数：1884, loss:1.0668034553527832\n",
      "训练次数：1885, loss:1.2508373260498047\n",
      "训练次数：1886, loss:1.0861817598342896\n",
      "训练次数：1887, loss:1.0521024465560913\n",
      "训练次数：1888, loss:1.1907199621200562\n",
      "训练次数：1889, loss:1.0354360342025757\n",
      "训练次数：1890, loss:1.0135754346847534\n",
      "训练次数：1891, loss:0.9601240158081055\n",
      "训练次数：1892, loss:1.1508522033691406\n",
      "训练次数：1893, loss:1.0755200386047363\n",
      "训练次数：1894, loss:0.9414141178131104\n",
      "训练次数：1895, loss:1.0765376091003418\n",
      "训练次数：1896, loss:1.0205397605895996\n",
      "训练次数：1897, loss:1.1553462743759155\n",
      "训练次数：1898, loss:1.1326426267623901\n",
      "训练次数：1899, loss:1.1853289604187012\n",
      "训练次数：1900, loss:1.256516456604004\n",
      "训练次数：1901, loss:1.1351110935211182\n",
      "训练次数：1902, loss:1.0410640239715576\n",
      "训练次数：1903, loss:1.0461311340332031\n",
      "训练次数：1904, loss:1.2901338338851929\n",
      "训练次数：1905, loss:1.1812032461166382\n",
      "训练次数：1906, loss:1.3229637145996094\n",
      "训练次数：1907, loss:1.2889137268066406\n",
      "训练次数：1908, loss:1.16808021068573\n",
      "训练次数：1909, loss:1.185404896736145\n",
      "训练次数：1910, loss:1.0973469018936157\n",
      "训练次数：1911, loss:1.2831050157546997\n",
      "训练次数：1912, loss:1.308642864227295\n",
      "训练次数：1913, loss:1.1633754968643188\n",
      "训练次数：1914, loss:1.208024501800537\n",
      "训练次数：1915, loss:1.2176764011383057\n",
      "训练次数：1916, loss:1.2871298789978027\n",
      "训练次数：1917, loss:1.01990807056427\n",
      "训练次数：1918, loss:1.1610064506530762\n",
      "训练次数：1919, loss:1.016701102256775\n",
      "训练次数：1920, loss:1.2329238653182983\n",
      "训练次数：1921, loss:1.187984585762024\n",
      "训练次数：1922, loss:1.3783448934555054\n",
      "训练次数：1923, loss:1.3031563758850098\n",
      "训练次数：1924, loss:1.213475227355957\n",
      "训练次数：1925, loss:1.1692514419555664\n",
      "训练次数：1926, loss:0.932731032371521\n",
      "训练次数：1927, loss:1.2655032873153687\n",
      "训练次数：1928, loss:1.1130270957946777\n",
      "训练次数：1929, loss:0.938908040523529\n",
      "训练次数：1930, loss:0.9686409831047058\n",
      "训练次数：1931, loss:1.4042469263076782\n",
      "训练次数：1932, loss:1.1676886081695557\n",
      "训练次数：1933, loss:0.9184622168540955\n",
      "训练次数：1934, loss:1.2899636030197144\n",
      "训练次数：1935, loss:0.9766039252281189\n",
      "训练次数：1936, loss:1.261540412902832\n",
      "训练次数：1937, loss:1.1471753120422363\n",
      "训练次数：1938, loss:1.1116008758544922\n",
      "训练次数：1939, loss:1.1764793395996094\n",
      "训练次数：1940, loss:0.9779343008995056\n",
      "训练次数：1941, loss:1.041700839996338\n",
      "训练次数：1942, loss:1.0605697631835938\n",
      "训练次数：1943, loss:1.2068333625793457\n",
      "训练次数：1944, loss:1.2991862297058105\n",
      "训练次数：1945, loss:1.410714864730835\n",
      "训练次数：1946, loss:1.0666825771331787\n",
      "训练次数：1947, loss:1.0552338361740112\n",
      "训练次数：1948, loss:1.0152395963668823\n",
      "训练次数：1949, loss:0.9571113586425781\n",
      "训练次数：1950, loss:0.9491317868232727\n",
      "训练次数：1951, loss:1.2531383037567139\n",
      "训练次数：1952, loss:1.4322043657302856\n",
      "训练次数：1953, loss:1.3239171504974365\n",
      "训练次数：1954, loss:1.0832496881484985\n",
      "训练次数：1955, loss:0.9671134948730469\n",
      "训练次数：1956, loss:0.7418522834777832\n",
      "训练次数：1957, loss:1.1145532131195068\n",
      "训练次数：1958, loss:1.1668577194213867\n",
      "训练次数：1959, loss:1.298198938369751\n",
      "训练次数：1960, loss:1.0420323610305786\n",
      "训练次数：1961, loss:1.0639445781707764\n",
      "训练次数：1962, loss:1.2073760032653809\n",
      "训练次数：1963, loss:1.1295557022094727\n",
      "训练次数：1964, loss:0.9002230763435364\n",
      "训练次数：1965, loss:1.0831745862960815\n",
      "训练次数：1966, loss:1.1989831924438477\n",
      "训练次数：1967, loss:1.1392042636871338\n",
      "训练次数：1968, loss:1.0722471475601196\n",
      "训练次数：1969, loss:1.2450004816055298\n",
      "训练次数：1970, loss:0.9391750693321228\n",
      "训练次数：1971, loss:1.132218360900879\n",
      "训练次数：1972, loss:0.8132361173629761\n",
      "训练次数：1973, loss:1.2912369966506958\n",
      "训练次数：1974, loss:1.0108968019485474\n",
      "训练次数：1975, loss:1.2901793718338013\n",
      "训练次数：1976, loss:0.9401660561561584\n",
      "训练次数：1977, loss:1.226334810256958\n",
      "训练次数：1978, loss:1.1529399156570435\n",
      "训练次数：1979, loss:1.1570583581924438\n",
      "训练次数：1980, loss:1.0756818056106567\n",
      "训练次数：1981, loss:1.294900894165039\n",
      "训练次数：1982, loss:1.1512393951416016\n",
      "训练次数：1983, loss:1.387802243232727\n",
      "训练次数：1984, loss:1.1233530044555664\n",
      "训练次数：1985, loss:1.3882241249084473\n",
      "训练次数：1986, loss:1.164878010749817\n",
      "训练次数：1987, loss:1.2922483682632446\n",
      "训练次数：1988, loss:1.250930666923523\n",
      "训练次数：1989, loss:1.0043312311172485\n",
      "训练次数：1990, loss:1.3241668939590454\n",
      "训练次数：1991, loss:1.1541986465454102\n",
      "训练次数：1992, loss:1.0067530870437622\n",
      "训练次数：1993, loss:1.1186964511871338\n",
      "训练次数：1994, loss:1.170527696609497\n",
      "训练次数：1995, loss:1.216848611831665\n",
      "训练次数：1996, loss:0.9550831913948059\n",
      "训练次数：1997, loss:1.1451061964035034\n",
      "训练次数：1998, loss:1.0530754327774048\n",
      "训练次数：1999, loss:1.173435926437378\n",
      "训练次数：2000, loss:1.4631752967834473\n",
      "训练次数：2001, loss:1.0926830768585205\n",
      "训练次数：2002, loss:1.0739610195159912\n",
      "训练次数：2003, loss:1.220036268234253\n",
      "训练次数：2004, loss:1.1028802394866943\n",
      "训练次数：2005, loss:0.910054087638855\n",
      "训练次数：2006, loss:1.1010156869888306\n",
      "训练次数：2007, loss:1.3945242166519165\n",
      "训练次数：2008, loss:1.0957584381103516\n",
      "训练次数：2009, loss:1.1927584409713745\n",
      "训练次数：2010, loss:1.115756630897522\n",
      "训练次数：2011, loss:1.3890995979309082\n",
      "训练次数：2012, loss:1.0270808935165405\n",
      "训练次数：2013, loss:1.1523692607879639\n",
      "训练次数：2014, loss:1.224794864654541\n",
      "训练次数：2015, loss:1.0518949031829834\n",
      "训练次数：2016, loss:0.9069383144378662\n",
      "训练次数：2017, loss:0.9060491323471069\n",
      "训练次数：2018, loss:1.0829330682754517\n",
      "训练次数：2019, loss:1.2207527160644531\n",
      "训练次数：2020, loss:1.2335067987442017\n",
      "训练次数：2021, loss:1.3260236978530884\n",
      "训练次数：2022, loss:1.0913914442062378\n",
      "训练次数：2023, loss:0.9311328530311584\n",
      "训练次数：2024, loss:0.9853907823562622\n",
      "训练次数：2025, loss:1.0569545030593872\n",
      "训练次数：2026, loss:1.0723098516464233\n",
      "训练次数：2027, loss:1.1149283647537231\n",
      "训练次数：2028, loss:1.2449591159820557\n",
      "训练次数：2029, loss:1.0898184776306152\n",
      "训练次数：2030, loss:0.8768559098243713\n",
      "训练次数：2031, loss:1.1214144229888916\n",
      "训练次数：2032, loss:0.9116631150245667\n",
      "训练次数：2033, loss:0.9450930953025818\n",
      "训练次数：2034, loss:1.1004738807678223\n",
      "训练次数：2035, loss:1.0396130084991455\n",
      "训练次数：2036, loss:1.1535385847091675\n",
      "训练次数：2037, loss:0.9843248128890991\n",
      "训练次数：2038, loss:1.0583360195159912\n",
      "训练次数：2039, loss:0.9531193375587463\n",
      "训练次数：2040, loss:1.237794280052185\n",
      "训练次数：2041, loss:1.1015352010726929\n",
      "训练次数：2042, loss:1.0771551132202148\n",
      "训练次数：2043, loss:1.3090629577636719\n",
      "训练次数：2044, loss:1.281247854232788\n",
      "训练次数：2045, loss:1.3041883707046509\n",
      "训练次数：2046, loss:1.3085978031158447\n",
      "训练次数：2047, loss:1.1031053066253662\n",
      "训练次数：2048, loss:0.9941498041152954\n",
      "训练次数：2049, loss:1.1133151054382324\n",
      "训练次数：2050, loss:0.9963889122009277\n",
      "训练次数：2051, loss:1.2664762735366821\n",
      "训练次数：2052, loss:1.3188679218292236\n",
      "训练次数：2053, loss:0.9869518280029297\n",
      "训练次数：2054, loss:0.9470663666725159\n",
      "训练次数：2055, loss:1.426564335823059\n",
      "训练次数：2056, loss:1.000948429107666\n",
      "训练次数：2057, loss:1.228386402130127\n",
      "训练次数：2058, loss:1.1601604223251343\n",
      "训练次数：2059, loss:1.2612264156341553\n",
      "训练次数：2060, loss:1.340470314025879\n",
      "训练次数：2061, loss:0.9528342485427856\n",
      "训练次数：2062, loss:1.3164582252502441\n",
      "训练次数：2063, loss:1.3341412544250488\n",
      "训练次数：2064, loss:1.0252125263214111\n",
      "训练次数：2065, loss:1.3962405920028687\n",
      "训练次数：2066, loss:0.9335194230079651\n",
      "训练次数：2067, loss:1.068232536315918\n",
      "训练次数：2068, loss:1.3082393407821655\n",
      "训练次数：2069, loss:1.1958420276641846\n",
      "训练次数：2070, loss:1.1066076755523682\n",
      "训练次数：2071, loss:1.068812370300293\n",
      "训练次数：2072, loss:1.183586597442627\n",
      "训练次数：2073, loss:1.1965692043304443\n",
      "训练次数：2074, loss:1.2634825706481934\n",
      "训练次数：2075, loss:1.073872685432434\n",
      "训练次数：2076, loss:1.1542158126831055\n",
      "训练次数：2077, loss:1.0883514881134033\n",
      "训练次数：2078, loss:1.323449969291687\n",
      "训练次数：2079, loss:1.1545653343200684\n",
      "训练次数：2080, loss:1.1069308519363403\n",
      "训练次数：2081, loss:1.198502540588379\n",
      "训练次数：2082, loss:0.9723880887031555\n",
      "训练次数：2083, loss:1.1416040658950806\n",
      "训练次数：2084, loss:1.0161503553390503\n",
      "训练次数：2085, loss:1.1796950101852417\n",
      "训练次数：2086, loss:1.2031137943267822\n",
      "训练次数：2087, loss:1.3441534042358398\n",
      "训练次数：2088, loss:1.2496944665908813\n",
      "训练次数：2089, loss:1.2870457172393799\n",
      "训练次数：2090, loss:1.352467656135559\n",
      "训练次数：2091, loss:1.2461423873901367\n",
      "训练次数：2092, loss:0.993232786655426\n",
      "训练次数：2093, loss:1.120502233505249\n",
      "训练次数：2094, loss:1.3916327953338623\n",
      "训练次数：2095, loss:1.2390328645706177\n",
      "训练次数：2096, loss:1.2017053365707397\n",
      "训练次数：2097, loss:1.230060338973999\n",
      "训练次数：2098, loss:0.8315598964691162\n",
      "训练次数：2099, loss:1.147386074066162\n",
      "训练次数：2100, loss:0.9861782789230347\n",
      "训练次数：2101, loss:1.4202100038528442\n",
      "训练次数：2102, loss:1.252316951751709\n",
      "训练次数：2103, loss:1.0897769927978516\n",
      "训练次数：2104, loss:0.9495683312416077\n",
      "训练次数：2105, loss:1.064527153968811\n",
      "训练次数：2106, loss:1.1966259479522705\n",
      "训练次数：2107, loss:1.256873607635498\n",
      "训练次数：2108, loss:1.1001254320144653\n",
      "训练次数：2109, loss:0.9847758412361145\n",
      "训练次数：2110, loss:0.9928812384605408\n",
      "训练次数：2111, loss:1.2955148220062256\n",
      "训练次数：2112, loss:1.2298200130462646\n",
      "训练次数：2113, loss:1.2387826442718506\n",
      "训练次数：2114, loss:1.082231879234314\n",
      "训练次数：2115, loss:1.0579394102096558\n",
      "训练次数：2116, loss:1.2634613513946533\n",
      "训练次数：2117, loss:0.9729437232017517\n",
      "训练次数：2118, loss:1.20649254322052\n",
      "训练次数：2119, loss:1.085463523864746\n",
      "训练次数：2120, loss:0.9061523079872131\n",
      "训练次数：2121, loss:1.2477742433547974\n",
      "训练次数：2122, loss:0.9165347814559937\n",
      "训练次数：2123, loss:1.0301883220672607\n",
      "训练次数：2124, loss:1.1775212287902832\n",
      "训练次数：2125, loss:1.0404942035675049\n",
      "训练次数：2126, loss:1.1570641994476318\n",
      "训练次数：2127, loss:1.4889042377471924\n",
      "训练次数：2128, loss:0.9903281331062317\n",
      "训练次数：2129, loss:1.0151684284210205\n",
      "训练次数：2130, loss:1.079483151435852\n",
      "训练次数：2131, loss:1.0802953243255615\n",
      "训练次数：2132, loss:1.175758719444275\n",
      "训练次数：2133, loss:1.0005475282669067\n",
      "训练次数：2134, loss:1.1283104419708252\n",
      "训练次数：2135, loss:0.9481779336929321\n",
      "训练次数：2136, loss:0.8867744207382202\n",
      "训练次数：2137, loss:1.0678730010986328\n",
      "训练次数：2138, loss:1.3069924116134644\n",
      "训练次数：2139, loss:1.1442139148712158\n",
      "训练次数：2140, loss:0.9268283843994141\n",
      "训练次数：2141, loss:1.262515902519226\n",
      "训练次数：2142, loss:1.4391270875930786\n",
      "训练次数：2143, loss:1.1078941822052002\n",
      "训练次数：2144, loss:1.2322163581848145\n",
      "训练次数：2145, loss:1.2096441984176636\n",
      "训练次数：2146, loss:1.1500763893127441\n",
      "训练次数：2147, loss:0.9886568188667297\n",
      "训练次数：2148, loss:1.5644136667251587\n",
      "训练次数：2149, loss:1.2087677717208862\n",
      "训练次数：2150, loss:1.2065616846084595\n",
      "训练次数：2151, loss:1.1837337017059326\n",
      "训练次数：2152, loss:0.8677301406860352\n",
      "训练次数：2153, loss:1.2627612352371216\n",
      "训练次数：2154, loss:1.2505831718444824\n",
      "训练次数：2155, loss:1.1860604286193848\n",
      "训练次数：2156, loss:1.0321166515350342\n",
      "训练次数：2157, loss:1.4760290384292603\n",
      "训练次数：2158, loss:1.0982154607772827\n",
      "训练次数：2159, loss:1.2113951444625854\n",
      "训练次数：2160, loss:1.4847736358642578\n",
      "训练次数：2161, loss:1.2608920335769653\n",
      "训练次数：2162, loss:1.0584105253219604\n",
      "训练次数：2163, loss:1.3880783319473267\n",
      "训练次数：2164, loss:1.2483540773391724\n",
      "训练次数：2165, loss:1.2126082181930542\n",
      "训练次数：2166, loss:1.0356731414794922\n",
      "训练次数：2167, loss:0.7409048080444336\n",
      "训练次数：2168, loss:1.4552466869354248\n",
      "训练次数：2169, loss:1.006901741027832\n",
      "训练次数：2170, loss:0.9152495861053467\n",
      "训练次数：2171, loss:0.9576658010482788\n",
      "训练次数：2172, loss:1.0670415163040161\n",
      "训练次数：2173, loss:1.1169285774230957\n",
      "训练次数：2174, loss:1.189475178718567\n",
      "训练次数：2175, loss:1.107297658920288\n",
      "训练次数：2176, loss:1.2646485567092896\n",
      "训练次数：2177, loss:1.052679419517517\n",
      "训练次数：2178, loss:0.9817076325416565\n",
      "训练次数：2179, loss:1.0896546840667725\n",
      "训练次数：2180, loss:0.9937628507614136\n",
      "训练次数：2181, loss:1.0120741128921509\n",
      "训练次数：2182, loss:0.9603367447853088\n",
      "训练次数：2183, loss:1.246911883354187\n",
      "训练次数：2184, loss:1.3795158863067627\n",
      "训练次数：2185, loss:1.2088643312454224\n",
      "训练次数：2186, loss:1.3799517154693604\n",
      "训练次数：2187, loss:1.067219614982605\n",
      "训练次数：2188, loss:0.9797865152359009\n",
      "训练次数：2189, loss:1.0941158533096313\n",
      "训练次数：2190, loss:0.9657851457595825\n",
      "训练次数：2191, loss:1.0607680082321167\n",
      "训练次数：2192, loss:1.249176025390625\n",
      "训练次数：2193, loss:1.1738837957382202\n",
      "训练次数：2194, loss:0.9880341291427612\n",
      "训练次数：2195, loss:1.2447152137756348\n",
      "训练次数：2196, loss:1.5494248867034912\n",
      "训练次数：2197, loss:1.2876322269439697\n",
      "训练次数：2198, loss:0.8821040987968445\n",
      "训练次数：2199, loss:1.1538619995117188\n",
      "训练次数：2200, loss:0.9547200798988342\n",
      "训练次数：2201, loss:1.0566754341125488\n",
      "训练次数：2202, loss:1.2545729875564575\n",
      "训练次数：2203, loss:1.2253259420394897\n",
      "训练次数：2204, loss:1.344262719154358\n",
      "训练次数：2205, loss:1.2538701295852661\n",
      "训练次数：2206, loss:1.1616039276123047\n",
      "训练次数：2207, loss:1.1330389976501465\n",
      "训练次数：2208, loss:0.9180814623832703\n",
      "训练次数：2209, loss:1.0163079500198364\n",
      "训练次数：2210, loss:1.2011820077896118\n",
      "训练次数：2211, loss:1.0858962535858154\n",
      "训练次数：2212, loss:0.9969967603683472\n",
      "训练次数：2213, loss:1.248665452003479\n",
      "训练次数：2214, loss:1.5632977485656738\n",
      "训练次数：2215, loss:1.09395170211792\n",
      "训练次数：2216, loss:0.9753210544586182\n",
      "训练次数：2217, loss:0.9050334692001343\n",
      "训练次数：2218, loss:1.270383358001709\n",
      "训练次数：2219, loss:1.1923953294754028\n",
      "训练次数：2220, loss:1.2010687589645386\n",
      "训练次数：2221, loss:1.0963866710662842\n",
      "训练次数：2222, loss:1.0680612325668335\n",
      "训练次数：2223, loss:1.2709418535232544\n",
      "训练次数：2224, loss:1.068670630455017\n",
      "训练次数：2225, loss:1.073939561843872\n",
      "训练次数：2226, loss:1.010720133781433\n",
      "训练次数：2227, loss:0.9650706648826599\n",
      "训练次数：2228, loss:1.3010162115097046\n",
      "训练次数：2229, loss:1.2244340181350708\n",
      "训练次数：2230, loss:1.4838109016418457\n",
      "训练次数：2231, loss:1.102471113204956\n",
      "训练次数：2232, loss:1.2376295328140259\n",
      "训练次数：2233, loss:1.095221757888794\n",
      "训练次数：2234, loss:1.0799171924591064\n",
      "训练次数：2235, loss:1.1827802658081055\n",
      "训练次数：2236, loss:1.2050845623016357\n",
      "训练次数：2237, loss:1.0925489664077759\n",
      "训练次数：2238, loss:0.897497832775116\n",
      "训练次数：2239, loss:1.2570202350616455\n",
      "训练次数：2240, loss:1.3360750675201416\n",
      "训练次数：2241, loss:1.3907660245895386\n",
      "训练次数：2242, loss:1.0112642049789429\n",
      "训练次数：2243, loss:1.0309737920761108\n",
      "训练次数：2244, loss:1.1817559003829956\n",
      "训练次数：2245, loss:1.2761598825454712\n",
      "训练次数：2246, loss:0.8530041575431824\n",
      "训练次数：2247, loss:0.8589907288551331\n",
      "训练次数：2248, loss:1.2676607370376587\n",
      "训练次数：2249, loss:1.1672757863998413\n",
      "训练次数：2250, loss:1.1801109313964844\n",
      "训练次数：2251, loss:0.9494044184684753\n",
      "训练次数：2252, loss:1.1198958158493042\n",
      "训练次数：2253, loss:1.1087244749069214\n",
      "训练次数：2254, loss:1.1132874488830566\n",
      "训练次数：2255, loss:1.199244499206543\n",
      "训练次数：2256, loss:1.1878283023834229\n",
      "训练次数：2257, loss:1.2311162948608398\n",
      "训练次数：2258, loss:1.198697566986084\n",
      "训练次数：2259, loss:0.9793872833251953\n",
      "训练次数：2260, loss:1.193747878074646\n",
      "训练次数：2261, loss:1.1304900646209717\n",
      "训练次数：2262, loss:1.1756978034973145\n",
      "训练次数：2263, loss:0.9907816648483276\n",
      "训练次数：2264, loss:1.1157498359680176\n",
      "训练次数：2265, loss:1.1386363506317139\n",
      "训练次数：2266, loss:1.2438924312591553\n",
      "训练次数：2267, loss:1.2060340642929077\n",
      "训练次数：2268, loss:0.9081266522407532\n",
      "训练次数：2269, loss:1.0900847911834717\n",
      "训练次数：2270, loss:1.029461145401001\n",
      "训练次数：2271, loss:1.3505243062973022\n",
      "训练次数：2272, loss:1.0748852491378784\n",
      "训练次数：2273, loss:1.0953701734542847\n",
      "训练次数：2274, loss:0.7462590336799622\n",
      "训练次数：2275, loss:1.2895431518554688\n",
      "训练次数：2276, loss:0.9422634840011597\n",
      "训练次数：2277, loss:1.0245171785354614\n",
      "训练次数：2278, loss:1.0166616439819336\n",
      "训练次数：2279, loss:1.186319351196289\n",
      "训练次数：2280, loss:1.1567460298538208\n",
      "训练次数：2281, loss:0.9840086102485657\n",
      "训练次数：2282, loss:1.1425920724868774\n",
      "训练次数：2283, loss:1.0114127397537231\n",
      "训练次数：2284, loss:0.959682285785675\n",
      "训练次数：2285, loss:0.9011207818984985\n",
      "训练次数：2286, loss:1.0304054021835327\n",
      "训练次数：2287, loss:0.9819710850715637\n",
      "训练次数：2288, loss:1.4224735498428345\n",
      "训练次数：2289, loss:1.1029900312423706\n",
      "训练次数：2290, loss:1.1109257936477661\n",
      "训练次数：2291, loss:1.5342378616333008\n",
      "训练次数：2292, loss:1.4715551137924194\n",
      "训练次数：2293, loss:0.9448258280754089\n",
      "训练次数：2294, loss:1.1875300407409668\n",
      "训练次数：2295, loss:1.014355182647705\n",
      "训练次数：2296, loss:0.9706276059150696\n",
      "训练次数：2297, loss:1.0236802101135254\n",
      "训练次数：2298, loss:1.1608573198318481\n",
      "训练次数：2299, loss:1.1249254941940308\n",
      "训练次数：2300, loss:1.457312822341919\n",
      "训练次数：2301, loss:0.8800722360610962\n",
      "训练次数：2302, loss:1.007417917251587\n",
      "训练次数：2303, loss:0.9825034141540527\n",
      "训练次数：2304, loss:1.028205156326294\n",
      "训练次数：2305, loss:1.1714472770690918\n",
      "训练次数：2306, loss:1.110576868057251\n",
      "训练次数：2307, loss:1.290285587310791\n",
      "训练次数：2308, loss:0.6953792572021484\n",
      "训练次数：2309, loss:1.0432459115982056\n",
      "训练次数：2310, loss:1.2589868307113647\n",
      "训练次数：2311, loss:0.914938747882843\n",
      "训练次数：2312, loss:1.2708009481430054\n",
      "训练次数：2313, loss:1.3386735916137695\n",
      "训练次数：2314, loss:1.0447216033935547\n",
      "训练次数：2315, loss:1.5163383483886719\n",
      "训练次数：2316, loss:1.226201057434082\n",
      "训练次数：2317, loss:1.1082985401153564\n",
      "训练次数：2318, loss:1.2964322566986084\n",
      "训练次数：2319, loss:0.9305486083030701\n",
      "训练次数：2320, loss:1.2140074968338013\n",
      "训练次数：2321, loss:1.1714390516281128\n",
      "训练次数：2322, loss:0.8996777534484863\n",
      "训练次数：2323, loss:0.9625479578971863\n",
      "训练次数：2324, loss:1.1033457517623901\n",
      "训练次数：2325, loss:0.9770278334617615\n",
      "训练次数：2326, loss:1.2904831171035767\n",
      "训练次数：2327, loss:1.117105484008789\n",
      "训练次数：2328, loss:1.1007384061813354\n",
      "训练次数：2329, loss:1.1988571882247925\n",
      "训练次数：2330, loss:1.2392383813858032\n",
      "训练次数：2331, loss:1.0181626081466675\n",
      "训练次数：2332, loss:0.9929948449134827\n",
      "训练次数：2333, loss:1.0193393230438232\n",
      "训练次数：2334, loss:0.9556027054786682\n",
      "训练次数：2335, loss:1.3789490461349487\n",
      "训练次数：2336, loss:1.2449541091918945\n",
      "训练次数：2337, loss:1.1561428308486938\n",
      "训练次数：2338, loss:0.9943217635154724\n",
      "训练次数：2339, loss:1.18813157081604\n",
      "训练次数：2340, loss:1.061893343925476\n",
      "训练次数：2341, loss:1.2835363149642944\n",
      "训练次数：2342, loss:1.0654683113098145\n",
      "训练次数：2343, loss:0.9344717264175415\n",
      "训练次数：2344, loss:0.9170525670051575\n",
      "训练次数：2345, loss:1.1816620826721191\n",
      "训练次数：2346, loss:1.7301465272903442\n",
      "----------第3轮训练开始----------\n",
      "训练次数：2347, loss:1.0897289514541626\n",
      "训练次数：2348, loss:0.9409054517745972\n",
      "训练次数：2349, loss:1.273617148399353\n",
      "训练次数：2350, loss:0.9337962865829468\n",
      "训练次数：2351, loss:0.9982879757881165\n",
      "训练次数：2352, loss:1.1246346235275269\n",
      "训练次数：2353, loss:1.153765320777893\n",
      "训练次数：2354, loss:1.1558749675750732\n",
      "训练次数：2355, loss:1.0093823671340942\n",
      "训练次数：2356, loss:1.4182076454162598\n",
      "训练次数：2357, loss:1.2702419757843018\n",
      "训练次数：2358, loss:1.0910933017730713\n",
      "训练次数：2359, loss:1.2573306560516357\n",
      "训练次数：2360, loss:1.3148268461227417\n",
      "训练次数：2361, loss:0.9757980108261108\n",
      "训练次数：2362, loss:1.0145914554595947\n",
      "训练次数：2363, loss:1.0940229892730713\n",
      "训练次数：2364, loss:1.134307622909546\n",
      "训练次数：2365, loss:0.9728876352310181\n",
      "训练次数：2366, loss:1.0374336242675781\n",
      "训练次数：2367, loss:1.3307558298110962\n",
      "训练次数：2368, loss:1.1774917840957642\n",
      "训练次数：2369, loss:1.0894711017608643\n",
      "训练次数：2370, loss:1.2844464778900146\n",
      "训练次数：2371, loss:1.0630483627319336\n",
      "训练次数：2372, loss:1.1265881061553955\n",
      "训练次数：2373, loss:1.0649101734161377\n",
      "训练次数：2374, loss:1.0486161708831787\n",
      "训练次数：2375, loss:0.9648313522338867\n",
      "训练次数：2376, loss:0.8792953491210938\n",
      "训练次数：2377, loss:1.3374067544937134\n",
      "训练次数：2378, loss:1.1649121046066284\n",
      "训练次数：2379, loss:0.9848365187644958\n",
      "训练次数：2380, loss:0.9619009494781494\n",
      "训练次数：2381, loss:0.9934912919998169\n",
      "训练次数：2382, loss:0.9585275053977966\n",
      "训练次数：2383, loss:0.8295141458511353\n",
      "训练次数：2384, loss:1.0729337930679321\n",
      "训练次数：2385, loss:0.8124619126319885\n",
      "训练次数：2386, loss:1.1375484466552734\n",
      "训练次数：2387, loss:0.7771320939064026\n",
      "训练次数：2388, loss:0.8299090266227722\n",
      "训练次数：2389, loss:1.02164888381958\n",
      "训练次数：2390, loss:1.3132120370864868\n",
      "训练次数：2391, loss:0.8671281933784485\n",
      "训练次数：2392, loss:1.0146960020065308\n",
      "训练次数：2393, loss:1.3024003505706787\n",
      "训练次数：2394, loss:1.1651968955993652\n",
      "训练次数：2395, loss:1.0230841636657715\n",
      "训练次数：2396, loss:1.1485893726348877\n",
      "训练次数：2397, loss:1.1754162311553955\n",
      "训练次数：2398, loss:1.0999770164489746\n",
      "训练次数：2399, loss:1.237940788269043\n",
      "训练次数：2400, loss:1.1107548475265503\n",
      "训练次数：2401, loss:1.0754764080047607\n",
      "训练次数：2402, loss:0.9569045901298523\n",
      "训练次数：2403, loss:1.1613320112228394\n",
      "训练次数：2404, loss:0.7289181351661682\n",
      "训练次数：2405, loss:0.7297800779342651\n",
      "训练次数：2406, loss:0.9619047045707703\n",
      "训练次数：2407, loss:1.1859867572784424\n",
      "训练次数：2408, loss:1.1086996793746948\n",
      "训练次数：2409, loss:1.0820410251617432\n",
      "训练次数：2410, loss:1.1878645420074463\n",
      "训练次数：2411, loss:1.1280603408813477\n",
      "训练次数：2412, loss:1.1235274076461792\n",
      "训练次数：2413, loss:0.9655967950820923\n",
      "训练次数：2414, loss:1.1410971879959106\n",
      "训练次数：2415, loss:0.889755129814148\n",
      "训练次数：2416, loss:1.0429251194000244\n",
      "训练次数：2417, loss:1.313913106918335\n",
      "训练次数：2418, loss:0.8191633224487305\n",
      "训练次数：2419, loss:0.9860492944717407\n",
      "训练次数：2420, loss:1.005494236946106\n",
      "训练次数：2421, loss:1.0978869199752808\n",
      "训练次数：2422, loss:1.263196587562561\n",
      "训练次数：2423, loss:1.212728500366211\n",
      "训练次数：2424, loss:1.3454632759094238\n",
      "训练次数：2425, loss:1.0332310199737549\n",
      "训练次数：2426, loss:1.362866997718811\n",
      "训练次数：2427, loss:1.339900016784668\n",
      "训练次数：2428, loss:1.140986442565918\n",
      "训练次数：2429, loss:1.120044231414795\n",
      "训练次数：2430, loss:0.9048101902008057\n",
      "训练次数：2431, loss:1.4343901872634888\n",
      "训练次数：2432, loss:1.1301811933517456\n",
      "训练次数：2433, loss:1.1621774435043335\n",
      "训练次数：2434, loss:0.9452019929885864\n",
      "训练次数：2435, loss:0.9526111483573914\n",
      "训练次数：2436, loss:1.0055526494979858\n",
      "训练次数：2437, loss:1.2778230905532837\n",
      "训练次数：2438, loss:1.2034858465194702\n",
      "训练次数：2439, loss:0.9068909883499146\n",
      "训练次数：2440, loss:1.0451078414916992\n",
      "训练次数：2441, loss:1.2079821825027466\n",
      "训练次数：2442, loss:1.0423587560653687\n",
      "训练次数：2443, loss:0.9197909832000732\n",
      "训练次数：2444, loss:1.2740991115570068\n",
      "训练次数：2445, loss:1.0826632976531982\n",
      "训练次数：2446, loss:0.9323793649673462\n",
      "训练次数：2447, loss:0.9896137714385986\n",
      "训练次数：2448, loss:0.8707362413406372\n",
      "训练次数：2449, loss:0.9549500346183777\n",
      "训练次数：2450, loss:1.113663911819458\n",
      "训练次数：2451, loss:1.1676156520843506\n",
      "训练次数：2452, loss:0.8686488270759583\n",
      "训练次数：2453, loss:1.0791826248168945\n",
      "训练次数：2454, loss:1.3440719842910767\n",
      "训练次数：2455, loss:1.1064376831054688\n",
      "训练次数：2456, loss:1.0958762168884277\n",
      "训练次数：2457, loss:1.175485610961914\n",
      "训练次数：2458, loss:1.019057035446167\n",
      "训练次数：2459, loss:1.132935643196106\n",
      "训练次数：2460, loss:1.0193095207214355\n",
      "训练次数：2461, loss:1.3821097612380981\n",
      "训练次数：2462, loss:1.346246600151062\n",
      "训练次数：2463, loss:1.0754541158676147\n",
      "训练次数：2464, loss:1.078026294708252\n",
      "训练次数：2465, loss:1.1576476097106934\n",
      "训练次数：2466, loss:1.117635726928711\n",
      "训练次数：2467, loss:1.3625942468643188\n",
      "训练次数：2468, loss:1.3269261121749878\n",
      "训练次数：2469, loss:1.0800594091415405\n",
      "训练次数：2470, loss:1.2951068878173828\n",
      "训练次数：2471, loss:1.2827939987182617\n",
      "训练次数：2472, loss:0.9911454319953918\n",
      "训练次数：2473, loss:0.941734790802002\n",
      "训练次数：2474, loss:1.159391164779663\n",
      "训练次数：2475, loss:1.395290493965149\n",
      "训练次数：2476, loss:0.9199979901313782\n",
      "训练次数：2477, loss:1.1086584329605103\n",
      "训练次数：2478, loss:0.9468781352043152\n",
      "训练次数：2479, loss:1.4252309799194336\n",
      "训练次数：2480, loss:1.1189403533935547\n",
      "训练次数：2481, loss:1.0940998792648315\n",
      "训练次数：2482, loss:0.9333099722862244\n",
      "训练次数：2483, loss:1.1866137981414795\n",
      "训练次数：2484, loss:1.1413086652755737\n",
      "训练次数：2485, loss:0.8530311584472656\n",
      "训练次数：2486, loss:1.4588041305541992\n",
      "训练次数：2487, loss:1.3398531675338745\n",
      "训练次数：2488, loss:1.124619960784912\n",
      "训练次数：2489, loss:1.1696782112121582\n",
      "训练次数：2490, loss:0.9570300579071045\n",
      "训练次数：2491, loss:1.1164376735687256\n",
      "训练次数：2492, loss:1.1320821046829224\n",
      "训练次数：2493, loss:1.0514369010925293\n",
      "训练次数：2494, loss:1.2451729774475098\n",
      "训练次数：2495, loss:1.1657334566116333\n",
      "训练次数：2496, loss:1.0390803813934326\n",
      "训练次数：2497, loss:1.114382028579712\n",
      "训练次数：2498, loss:1.1979230642318726\n",
      "训练次数：2499, loss:1.0383394956588745\n",
      "训练次数：2500, loss:1.03428053855896\n",
      "训练次数：2501, loss:1.327826976776123\n",
      "训练次数：2502, loss:1.1800603866577148\n",
      "训练次数：2503, loss:1.060877799987793\n",
      "训练次数：2504, loss:1.0581642389297485\n",
      "训练次数：2505, loss:1.1909050941467285\n",
      "训练次数：2506, loss:1.281312108039856\n",
      "训练次数：2507, loss:1.1961491107940674\n",
      "训练次数：2508, loss:0.7629895210266113\n",
      "训练次数：2509, loss:1.270833969116211\n",
      "训练次数：2510, loss:1.3258126974105835\n",
      "训练次数：2511, loss:1.0460678339004517\n",
      "训练次数：2512, loss:1.1906335353851318\n",
      "训练次数：2513, loss:1.0227267742156982\n",
      "训练次数：2514, loss:1.0834789276123047\n",
      "训练次数：2515, loss:0.957313060760498\n",
      "训练次数：2516, loss:1.0864211320877075\n",
      "训练次数：2517, loss:1.269729495048523\n",
      "训练次数：2518, loss:1.064209222793579\n",
      "训练次数：2519, loss:0.916183352470398\n",
      "训练次数：2520, loss:1.2352561950683594\n",
      "训练次数：2521, loss:0.901133120059967\n",
      "训练次数：2522, loss:1.1269489526748657\n",
      "训练次数：2523, loss:0.8184760212898254\n",
      "训练次数：2524, loss:1.16775643825531\n",
      "训练次数：2525, loss:1.0440917015075684\n",
      "训练次数：2526, loss:1.071108102798462\n",
      "训练次数：2527, loss:1.04413902759552\n",
      "训练次数：2528, loss:1.013790488243103\n",
      "训练次数：2529, loss:1.05034601688385\n",
      "训练次数：2530, loss:1.2028491497039795\n",
      "训练次数：2531, loss:1.1837221384048462\n",
      "训练次数：2532, loss:1.108493685722351\n",
      "训练次数：2533, loss:1.1555322408676147\n",
      "训练次数：2534, loss:1.102460503578186\n",
      "训练次数：2535, loss:1.0780854225158691\n",
      "训练次数：2536, loss:1.1393139362335205\n",
      "训练次数：2537, loss:0.8524182438850403\n",
      "训练次数：2538, loss:1.1685032844543457\n",
      "训练次数：2539, loss:1.3052812814712524\n",
      "训练次数：2540, loss:0.9904838800430298\n",
      "训练次数：2541, loss:0.9560304284095764\n",
      "训练次数：2542, loss:1.2902064323425293\n",
      "训练次数：2543, loss:1.2893459796905518\n",
      "训练次数：2544, loss:0.999860942363739\n",
      "训练次数：2545, loss:1.0397807359695435\n",
      "训练次数：2546, loss:1.0403110980987549\n",
      "训练次数：2547, loss:0.7635735869407654\n",
      "训练次数：2548, loss:1.3384580612182617\n",
      "训练次数：2549, loss:1.1831097602844238\n",
      "训练次数：2550, loss:1.2721154689788818\n",
      "训练次数：2551, loss:1.102439045906067\n",
      "训练次数：2552, loss:1.1382191181182861\n",
      "训练次数：2553, loss:1.0846079587936401\n",
      "训练次数：2554, loss:1.2024050951004028\n",
      "训练次数：2555, loss:1.022712230682373\n",
      "训练次数：2556, loss:1.356697678565979\n",
      "训练次数：2557, loss:1.1882847547531128\n",
      "训练次数：2558, loss:1.2407623529434204\n",
      "训练次数：2559, loss:1.060506820678711\n",
      "训练次数：2560, loss:1.135787844657898\n",
      "训练次数：2561, loss:1.3857883214950562\n",
      "训练次数：2562, loss:1.1367225646972656\n",
      "训练次数：2563, loss:1.1393991708755493\n",
      "训练次数：2564, loss:1.2647488117218018\n",
      "训练次数：2565, loss:1.0485492944717407\n",
      "训练次数：2566, loss:1.0701261758804321\n",
      "训练次数：2567, loss:1.1132893562316895\n",
      "训练次数：2568, loss:1.25285804271698\n",
      "训练次数：2569, loss:1.1853997707366943\n",
      "训练次数：2570, loss:1.1761727333068848\n",
      "训练次数：2571, loss:1.2657946348190308\n",
      "训练次数：2572, loss:0.9350206255912781\n",
      "训练次数：2573, loss:1.1173205375671387\n",
      "训练次数：2574, loss:0.9675359725952148\n",
      "训练次数：2575, loss:1.1008691787719727\n",
      "训练次数：2576, loss:1.2257436513900757\n",
      "训练次数：2577, loss:1.0691328048706055\n",
      "训练次数：2578, loss:1.0945367813110352\n",
      "训练次数：2579, loss:1.246657371520996\n",
      "训练次数：2580, loss:1.2901839017868042\n",
      "训练次数：2581, loss:0.8585352897644043\n",
      "训练次数：2582, loss:1.2240625619888306\n",
      "训练次数：2583, loss:1.0395960807800293\n",
      "训练次数：2584, loss:0.9845029711723328\n",
      "训练次数：2585, loss:1.0371609926223755\n",
      "训练次数：2586, loss:1.0136139392852783\n",
      "训练次数：2587, loss:0.9997991323471069\n",
      "训练次数：2588, loss:1.2795385122299194\n",
      "训练次数：2589, loss:1.1440504789352417\n",
      "训练次数：2590, loss:1.6595458984375\n",
      "训练次数：2591, loss:0.8454924821853638\n",
      "训练次数：2592, loss:1.4837777614593506\n",
      "训练次数：2593, loss:1.0022680759429932\n",
      "训练次数：2594, loss:1.291593074798584\n",
      "训练次数：2595, loss:1.1771750450134277\n",
      "训练次数：2596, loss:1.091731071472168\n",
      "训练次数：2597, loss:0.9157810807228088\n",
      "训练次数：2598, loss:1.0724910497665405\n",
      "训练次数：2599, loss:1.1932694911956787\n",
      "训练次数：2600, loss:1.093216061592102\n",
      "训练次数：2601, loss:1.0732319355010986\n",
      "训练次数：2602, loss:0.837945818901062\n",
      "训练次数：2603, loss:1.1187257766723633\n",
      "训练次数：2604, loss:0.8763554692268372\n",
      "训练次数：2605, loss:1.4134502410888672\n",
      "训练次数：2606, loss:1.0340111255645752\n",
      "训练次数：2607, loss:1.3653167486190796\n",
      "训练次数：2608, loss:1.211506724357605\n",
      "训练次数：2609, loss:1.376826286315918\n",
      "训练次数：2610, loss:1.0923529863357544\n",
      "训练次数：2611, loss:1.1524699926376343\n",
      "训练次数：2612, loss:1.035908818244934\n",
      "训练次数：2613, loss:1.2376947402954102\n",
      "训练次数：2614, loss:1.0244065523147583\n",
      "训练次数：2615, loss:0.9524314999580383\n",
      "训练次数：2616, loss:1.1260770559310913\n",
      "训练次数：2617, loss:1.269141435623169\n",
      "训练次数：2618, loss:1.1372507810592651\n",
      "训练次数：2619, loss:0.9814872741699219\n",
      "训练次数：2620, loss:1.0791008472442627\n",
      "训练次数：2621, loss:1.0621999502182007\n",
      "训练次数：2622, loss:1.0115994215011597\n",
      "训练次数：2623, loss:1.4445312023162842\n",
      "训练次数：2624, loss:1.1998867988586426\n",
      "训练次数：2625, loss:1.311856985092163\n",
      "训练次数：2626, loss:1.077493667602539\n",
      "训练次数：2627, loss:0.923783540725708\n",
      "训练次数：2628, loss:1.2561651468276978\n",
      "训练次数：2629, loss:0.9422933459281921\n",
      "训练次数：2630, loss:1.2461446523666382\n",
      "训练次数：2631, loss:0.9833971858024597\n",
      "训练次数：2632, loss:1.1765773296356201\n",
      "训练次数：2633, loss:1.1046897172927856\n",
      "训练次数：2634, loss:1.04511559009552\n",
      "训练次数：2635, loss:1.0499460697174072\n",
      "训练次数：2636, loss:1.0980932712554932\n",
      "训练次数：2637, loss:1.1338906288146973\n",
      "训练次数：2638, loss:1.3346415758132935\n",
      "训练次数：2639, loss:1.0001015663146973\n",
      "训练次数：2640, loss:1.0831546783447266\n",
      "训练次数：2641, loss:1.114712119102478\n",
      "训练次数：2642, loss:0.9374314546585083\n",
      "训练次数：2643, loss:1.2311385869979858\n",
      "训练次数：2644, loss:1.1284677982330322\n",
      "训练次数：2645, loss:1.0676944255828857\n",
      "训练次数：2646, loss:1.2308963537216187\n",
      "训练次数：2647, loss:0.999455988407135\n",
      "训练次数：2648, loss:0.9765942096710205\n",
      "训练次数：2649, loss:1.0308563709259033\n",
      "训练次数：2650, loss:0.7757766246795654\n",
      "训练次数：2651, loss:0.8517795205116272\n",
      "训练次数：2652, loss:1.3544780015945435\n",
      "训练次数：2653, loss:1.167004942893982\n",
      "训练次数：2654, loss:1.3141241073608398\n",
      "训练次数：2655, loss:1.5087890625\n",
      "训练次数：2656, loss:1.1518217325210571\n",
      "训练次数：2657, loss:1.1090388298034668\n",
      "训练次数：2658, loss:0.9769622087478638\n",
      "训练次数：2659, loss:1.1855406761169434\n",
      "训练次数：2660, loss:1.0092079639434814\n",
      "训练次数：2661, loss:1.1975765228271484\n",
      "训练次数：2662, loss:1.4000356197357178\n",
      "训练次数：2663, loss:1.104797601699829\n",
      "训练次数：2664, loss:1.2435352802276611\n",
      "训练次数：2665, loss:0.908575177192688\n",
      "训练次数：2666, loss:0.9830577969551086\n",
      "训练次数：2667, loss:1.359817385673523\n",
      "训练次数：2668, loss:1.1004557609558105\n",
      "训练次数：2669, loss:1.1490381956100464\n",
      "训练次数：2670, loss:1.1910760402679443\n",
      "训练次数：2671, loss:1.0206987857818604\n",
      "训练次数：2672, loss:0.9539470672607422\n",
      "训练次数：2673, loss:1.0492838621139526\n",
      "训练次数：2674, loss:1.0134024620056152\n",
      "训练次数：2675, loss:1.109345555305481\n",
      "训练次数：2676, loss:1.0158517360687256\n",
      "训练次数：2677, loss:1.0758031606674194\n",
      "训练次数：2678, loss:1.0679283142089844\n",
      "训练次数：2679, loss:1.171547293663025\n",
      "训练次数：2680, loss:1.011080026626587\n",
      "训练次数：2681, loss:1.1604914665222168\n",
      "训练次数：2682, loss:1.2367500066757202\n",
      "训练次数：2683, loss:1.193116545677185\n",
      "训练次数：2684, loss:1.0536296367645264\n",
      "训练次数：2685, loss:1.0176475048065186\n",
      "训练次数：2686, loss:1.280613660812378\n",
      "训练次数：2687, loss:1.0910160541534424\n",
      "训练次数：2688, loss:1.3553755283355713\n",
      "训练次数：2689, loss:1.2914307117462158\n",
      "训练次数：2690, loss:1.2024255990982056\n",
      "训练次数：2691, loss:1.1098079681396484\n",
      "训练次数：2692, loss:1.0237449407577515\n",
      "训练次数：2693, loss:1.1740167140960693\n",
      "训练次数：2694, loss:1.1984838247299194\n",
      "训练次数：2695, loss:0.9551576375961304\n",
      "训练次数：2696, loss:1.177731990814209\n",
      "训练次数：2697, loss:1.2326010465621948\n",
      "训练次数：2698, loss:1.193407654762268\n",
      "训练次数：2699, loss:1.0559862852096558\n",
      "训练次数：2700, loss:1.1655765771865845\n",
      "训练次数：2701, loss:0.876520574092865\n",
      "训练次数：2702, loss:1.2344720363616943\n",
      "训练次数：2703, loss:1.0884132385253906\n",
      "训练次数：2704, loss:1.1480789184570312\n",
      "训练次数：2705, loss:1.3731640577316284\n",
      "训练次数：2706, loss:1.022993803024292\n",
      "训练次数：2707, loss:1.2127091884613037\n",
      "训练次数：2708, loss:0.9019463062286377\n",
      "训练次数：2709, loss:1.130570411682129\n",
      "训练次数：2710, loss:0.9998753070831299\n",
      "训练次数：2711, loss:0.9342924356460571\n",
      "训练次数：2712, loss:0.8846856951713562\n",
      "训练次数：2713, loss:1.2705570459365845\n",
      "训练次数：2714, loss:1.1165145635604858\n",
      "训练次数：2715, loss:0.8740928769111633\n",
      "训练次数：2716, loss:1.2171711921691895\n",
      "训练次数：2717, loss:0.8909224271774292\n",
      "训练次数：2718, loss:1.0728168487548828\n",
      "训练次数：2719, loss:1.2220187187194824\n",
      "训练次数：2720, loss:1.0706467628479004\n",
      "训练次数：2721, loss:1.168623447418213\n",
      "训练次数：2722, loss:0.9167323708534241\n",
      "训练次数：2723, loss:0.9834897518157959\n",
      "训练次数：2724, loss:0.9817078709602356\n",
      "训练次数：2725, loss:1.0499017238616943\n",
      "训练次数：2726, loss:1.2332613468170166\n",
      "训练次数：2727, loss:1.2963263988494873\n",
      "训练次数：2728, loss:1.0123885869979858\n",
      "训练次数：2729, loss:0.97802734375\n",
      "训练次数：2730, loss:1.0664429664611816\n",
      "训练次数：2731, loss:0.9269967079162598\n",
      "训练次数：2732, loss:0.9426133036613464\n",
      "训练次数：2733, loss:1.08938467502594\n",
      "训练次数：2734, loss:1.403004765510559\n",
      "训练次数：2735, loss:1.2492214441299438\n",
      "训练次数：2736, loss:1.1041722297668457\n",
      "训练次数：2737, loss:0.8893296718597412\n",
      "训练次数：2738, loss:0.7107495665550232\n",
      "训练次数：2739, loss:0.9827557802200317\n",
      "训练次数：2740, loss:1.1214078664779663\n",
      "训练次数：2741, loss:1.1847989559173584\n",
      "训练次数：2742, loss:1.1440472602844238\n",
      "训练次数：2743, loss:1.0851064920425415\n",
      "训练次数：2744, loss:1.0774729251861572\n",
      "训练次数：2745, loss:1.0703401565551758\n",
      "训练次数：2746, loss:0.9371057152748108\n",
      "训练次数：2747, loss:1.084652304649353\n",
      "训练次数：2748, loss:1.3819689750671387\n",
      "训练次数：2749, loss:1.0567941665649414\n",
      "训练次数：2750, loss:0.953117311000824\n",
      "训练次数：2751, loss:1.1818355321884155\n",
      "训练次数：2752, loss:1.0181962251663208\n",
      "训练次数：2753, loss:1.1809368133544922\n",
      "训练次数：2754, loss:0.7010996341705322\n",
      "训练次数：2755, loss:1.081463098526001\n",
      "训练次数：2756, loss:0.8154521584510803\n",
      "训练次数：2757, loss:1.2573904991149902\n",
      "训练次数：2758, loss:1.0337275266647339\n",
      "训练次数：2759, loss:1.1538809537887573\n",
      "训练次数：2760, loss:0.9347791075706482\n",
      "训练次数：2761, loss:1.2175461053848267\n",
      "训练次数：2762, loss:0.9622855186462402\n",
      "训练次数：2763, loss:1.1991863250732422\n",
      "训练次数：2764, loss:1.0432997941970825\n",
      "训练次数：2765, loss:1.3306382894515991\n",
      "训练次数：2766, loss:0.9469969868659973\n",
      "训练次数：2767, loss:1.2518190145492554\n",
      "训练次数：2768, loss:1.0340924263000488\n",
      "训练次数：2769, loss:1.3074134588241577\n",
      "训练次数：2770, loss:1.2687292098999023\n",
      "训练次数：2771, loss:0.9288100600242615\n",
      "训练次数：2772, loss:1.2815614938735962\n",
      "训练次数：2773, loss:1.1656758785247803\n",
      "训练次数：2774, loss:0.8132491111755371\n",
      "训练次数：2775, loss:1.0394121408462524\n",
      "训练次数：2776, loss:1.061453104019165\n",
      "训练次数：2777, loss:1.0973801612854004\n",
      "训练次数：2778, loss:0.9125277400016785\n",
      "训练次数：2779, loss:0.8833449482917786\n",
      "训练次数：2780, loss:0.9165343046188354\n",
      "训练次数：2781, loss:1.0720055103302002\n",
      "训练次数：2782, loss:1.399275541305542\n",
      "训练次数：2783, loss:0.9133004546165466\n",
      "训练次数：2784, loss:0.870871365070343\n",
      "训练次数：2785, loss:1.0124915838241577\n",
      "训练次数：2786, loss:1.000780701637268\n",
      "训练次数：2787, loss:0.8190637230873108\n",
      "训练次数：2788, loss:1.000644564628601\n",
      "训练次数：2789, loss:1.3391581773757935\n",
      "训练次数：2790, loss:1.056707739830017\n",
      "训练次数：2791, loss:1.005091667175293\n",
      "训练次数：2792, loss:1.249291181564331\n",
      "训练次数：2793, loss:1.2001442909240723\n",
      "训练次数：2794, loss:0.991117000579834\n",
      "训练次数：2795, loss:1.0476114749908447\n",
      "训练次数：2796, loss:1.1357942819595337\n",
      "训练次数：2797, loss:0.9679090976715088\n",
      "训练次数：2798, loss:0.9006482362747192\n",
      "训练次数：2799, loss:0.8159473538398743\n",
      "训练次数：2800, loss:1.0089045763015747\n",
      "训练次数：2801, loss:1.2697501182556152\n",
      "训练次数：2802, loss:1.1261461973190308\n",
      "训练次数：2803, loss:1.5082075595855713\n",
      "训练次数：2804, loss:1.0035396814346313\n",
      "训练次数：2805, loss:0.8008012175559998\n",
      "训练次数：2806, loss:1.041347861289978\n",
      "训练次数：2807, loss:0.9432256817817688\n",
      "训练次数：2808, loss:0.9807026982307434\n",
      "训练次数：2809, loss:1.1592234373092651\n",
      "训练次数：2810, loss:1.0865646600723267\n",
      "训练次数：2811, loss:0.9763368368148804\n",
      "训练次数：2812, loss:0.8907551765441895\n",
      "训练次数：2813, loss:1.108372688293457\n",
      "训练次数：2814, loss:0.9124627709388733\n",
      "训练次数：2815, loss:0.8300929069519043\n",
      "训练次数：2816, loss:1.164170503616333\n",
      "训练次数：2817, loss:0.9318388104438782\n",
      "训练次数：2818, loss:1.202213168144226\n",
      "训练次数：2819, loss:0.8843566179275513\n",
      "训练次数：2820, loss:1.0606036186218262\n",
      "训练次数：2821, loss:1.0390878915786743\n",
      "训练次数：2822, loss:1.1728057861328125\n",
      "训练次数：2823, loss:1.1226098537445068\n",
      "训练次数：2824, loss:0.9579896330833435\n",
      "训练次数：2825, loss:1.2782113552093506\n",
      "训练次数：2826, loss:1.335341215133667\n",
      "训练次数：2827, loss:1.139615774154663\n",
      "训练次数：2828, loss:1.3891849517822266\n",
      "训练次数：2829, loss:1.06913423538208\n",
      "训练次数：2830, loss:0.896644115447998\n",
      "训练次数：2831, loss:1.1209185123443604\n",
      "训练次数：2832, loss:0.879721999168396\n",
      "训练次数：2833, loss:1.10541832447052\n",
      "训练次数：2834, loss:1.1873016357421875\n",
      "训练次数：2835, loss:0.9203726649284363\n",
      "训练次数：2836, loss:0.8775642514228821\n",
      "训练次数：2837, loss:1.348531723022461\n",
      "训练次数：2838, loss:0.9111653566360474\n",
      "训练次数：2839, loss:1.2186667919158936\n",
      "训练次数：2840, loss:1.175123691558838\n",
      "训练次数：2841, loss:1.2740956544876099\n",
      "训练次数：2842, loss:1.0898001194000244\n",
      "训练次数：2843, loss:0.848305344581604\n",
      "训练次数：2844, loss:1.1947669982910156\n",
      "训练次数：2845, loss:1.3150800466537476\n",
      "训练次数：2846, loss:1.0158677101135254\n",
      "训练次数：2847, loss:1.384936809539795\n",
      "训练次数：2848, loss:0.7788552641868591\n",
      "训练次数：2849, loss:1.0465739965438843\n",
      "训练次数：2850, loss:1.247886061668396\n",
      "训练次数：2851, loss:1.1063575744628906\n",
      "训练次数：2852, loss:1.1475391387939453\n",
      "训练次数：2853, loss:1.1142479181289673\n",
      "训练次数：2854, loss:1.2603882551193237\n",
      "训练次数：2855, loss:1.077211618423462\n",
      "训练次数：2856, loss:1.2597177028656006\n",
      "训练次数：2857, loss:1.0000851154327393\n",
      "训练次数：2858, loss:1.0849202871322632\n",
      "训练次数：2859, loss:0.9872031211853027\n",
      "训练次数：2860, loss:1.2184910774230957\n",
      "训练次数：2861, loss:0.982517421245575\n",
      "训练次数：2862, loss:1.1072726249694824\n",
      "训练次数：2863, loss:1.2054553031921387\n",
      "训练次数：2864, loss:0.9617376327514648\n",
      "训练次数：2865, loss:0.935033917427063\n",
      "训练次数：2866, loss:0.8730801939964294\n",
      "训练次数：2867, loss:1.0895435810089111\n",
      "训练次数：2868, loss:1.1753766536712646\n",
      "训练次数：2869, loss:1.1927998065948486\n",
      "训练次数：2870, loss:1.3601830005645752\n",
      "训练次数：2871, loss:1.2624306678771973\n",
      "训练次数：2872, loss:1.3097537755966187\n",
      "训练次数：2873, loss:1.270817518234253\n",
      "训练次数：2874, loss:1.0201587677001953\n",
      "训练次数：2875, loss:1.0877517461776733\n",
      "训练次数：2876, loss:1.1562998294830322\n",
      "训练次数：2877, loss:1.3161780834197998\n",
      "训练次数：2878, loss:0.9945982694625854\n",
      "训练次数：2879, loss:0.9825786352157593\n",
      "训练次数：2880, loss:0.8287969827651978\n",
      "训练次数：2881, loss:1.1299917697906494\n",
      "训练次数：2882, loss:1.0611474514007568\n",
      "训练次数：2883, loss:1.310271978378296\n",
      "训练次数：2884, loss:1.0646694898605347\n",
      "训练次数：2885, loss:1.4089685678482056\n",
      "训练次数：2886, loss:0.9935380220413208\n",
      "训练次数：2887, loss:0.9346574544906616\n",
      "训练次数：2888, loss:1.0710586309432983\n",
      "训练次数：2889, loss:1.292758584022522\n",
      "训练次数：2890, loss:1.2313507795333862\n",
      "训练次数：2891, loss:0.8736788034439087\n",
      "训练次数：2892, loss:0.9458612203598022\n",
      "训练次数：2893, loss:1.251755952835083\n",
      "训练次数：2894, loss:1.3076977729797363\n",
      "训练次数：2895, loss:1.0801961421966553\n",
      "训练次数：2896, loss:1.0021731853485107\n",
      "训练次数：2897, loss:0.9485414028167725\n",
      "训练次数：2898, loss:1.1565808057785034\n",
      "训练次数：2899, loss:1.080965280532837\n",
      "训练次数：2900, loss:1.1389598846435547\n",
      "训练次数：2901, loss:1.0105019807815552\n",
      "训练次数：2902, loss:0.9168190956115723\n",
      "训练次数：2903, loss:1.1493942737579346\n",
      "训练次数：2904, loss:0.8833950757980347\n",
      "训练次数：2905, loss:0.9691138863563538\n",
      "训练次数：2906, loss:1.1659915447235107\n",
      "训练次数：2907, loss:0.951775848865509\n",
      "训练次数：2908, loss:1.2038954496383667\n",
      "训练次数：2909, loss:1.262131929397583\n",
      "训练次数：2910, loss:0.9744501709938049\n",
      "训练次数：2911, loss:0.9090250134468079\n",
      "训练次数：2912, loss:1.0533510446548462\n",
      "训练次数：2913, loss:1.0047852993011475\n",
      "训练次数：2914, loss:1.028394103050232\n",
      "训练次数：2915, loss:0.9353870153427124\n",
      "训练次数：2916, loss:1.0606290102005005\n",
      "训练次数：2917, loss:0.8355688452720642\n",
      "训练次数：2918, loss:0.8043715953826904\n",
      "训练次数：2919, loss:1.0549430847167969\n",
      "训练次数：2920, loss:1.2110867500305176\n",
      "训练次数：2921, loss:1.0252739191055298\n",
      "训练次数：2922, loss:0.8800487518310547\n",
      "训练次数：2923, loss:1.0774701833724976\n",
      "训练次数：2924, loss:1.2914921045303345\n",
      "训练次数：2925, loss:0.9459671974182129\n",
      "训练次数：2926, loss:1.06241774559021\n",
      "训练次数：2927, loss:1.102360486984253\n",
      "训练次数：2928, loss:1.153593897819519\n",
      "训练次数：2929, loss:0.9107763171195984\n",
      "训练次数：2930, loss:1.4055739641189575\n",
      "训练次数：2931, loss:1.0264337062835693\n",
      "训练次数：2932, loss:1.1547819375991821\n",
      "训练次数：2933, loss:1.2035402059555054\n",
      "训练次数：2934, loss:0.823318600654602\n",
      "训练次数：2935, loss:1.2569992542266846\n",
      "训练次数：2936, loss:1.1418559551239014\n",
      "训练次数：2937, loss:1.1541286706924438\n",
      "训练次数：2938, loss:0.964853048324585\n",
      "训练次数：2939, loss:1.4658273458480835\n",
      "训练次数：2940, loss:1.0742521286010742\n",
      "训练次数：2941, loss:1.0970464944839478\n",
      "训练次数：2942, loss:1.4690532684326172\n",
      "训练次数：2943, loss:1.1108417510986328\n",
      "训练次数：2944, loss:1.0566556453704834\n",
      "训练次数：2945, loss:1.1471341848373413\n",
      "训练次数：2946, loss:1.1885933876037598\n",
      "训练次数：2947, loss:1.1118930578231812\n",
      "训练次数：2948, loss:1.0850046873092651\n",
      "训练次数：2949, loss:0.7561237215995789\n",
      "训练次数：2950, loss:1.2608201503753662\n",
      "训练次数：2951, loss:1.013179898262024\n",
      "训练次数：2952, loss:0.9349063038825989\n",
      "训练次数：2953, loss:0.8906779885292053\n",
      "训练次数：2954, loss:0.9241148233413696\n",
      "训练次数：2955, loss:0.9576423168182373\n",
      "训练次数：2956, loss:1.1432461738586426\n",
      "训练次数：2957, loss:1.0355921983718872\n",
      "训练次数：2958, loss:1.0308135747909546\n",
      "训练次数：2959, loss:0.9550535678863525\n",
      "训练次数：2960, loss:0.9036296606063843\n",
      "训练次数：2961, loss:1.1636936664581299\n",
      "训练次数：2962, loss:0.9678069949150085\n",
      "训练次数：2963, loss:1.0363667011260986\n",
      "训练次数：2964, loss:0.9929089546203613\n",
      "训练次数：2965, loss:1.1505550146102905\n",
      "训练次数：2966, loss:1.316616415977478\n",
      "训练次数：2967, loss:1.121543049812317\n",
      "训练次数：2968, loss:1.261056900024414\n",
      "训练次数：2969, loss:1.0228646993637085\n",
      "训练次数：2970, loss:0.9876580834388733\n",
      "训练次数：2971, loss:1.155569314956665\n",
      "训练次数：2972, loss:0.8915566205978394\n",
      "训练次数：2973, loss:0.9713789224624634\n",
      "训练次数：2974, loss:1.166486382484436\n",
      "训练次数：2975, loss:1.1030101776123047\n",
      "训练次数：2976, loss:0.8091974258422852\n",
      "训练次数：2977, loss:1.1618579626083374\n",
      "训练次数：2978, loss:1.352929711341858\n",
      "训练次数：2979, loss:1.1376615762710571\n",
      "训练次数：2980, loss:0.9596474766731262\n",
      "训练次数：2981, loss:1.1500086784362793\n",
      "训练次数：2982, loss:0.8247584700584412\n",
      "训练次数：2983, loss:1.0172696113586426\n",
      "训练次数：2984, loss:1.2613877058029175\n",
      "训练次数：2985, loss:1.0203864574432373\n",
      "训练次数：2986, loss:1.3447517156600952\n",
      "训练次数：2987, loss:1.3025572299957275\n",
      "训练次数：2988, loss:0.9564333558082581\n",
      "训练次数：2989, loss:1.1002494096755981\n",
      "训练次数：2990, loss:0.8072564005851746\n",
      "训练次数：2991, loss:0.9803856015205383\n",
      "训练次数：2992, loss:1.3480727672576904\n",
      "训练次数：2993, loss:1.1310993432998657\n",
      "训练次数：2994, loss:0.9514061212539673\n",
      "训练次数：2995, loss:1.0888898372650146\n",
      "训练次数：2996, loss:1.4405083656311035\n",
      "训练次数：2997, loss:0.8803881406784058\n",
      "训练次数：2998, loss:0.9214155077934265\n",
      "训练次数：2999, loss:0.7676268815994263\n",
      "训练次数：3000, loss:1.0971152782440186\n",
      "训练次数：3001, loss:1.2189580202102661\n",
      "训练次数：3002, loss:1.1405389308929443\n",
      "训练次数：3003, loss:1.0094845294952393\n",
      "训练次数：3004, loss:1.1269967555999756\n",
      "训练次数：3005, loss:1.1712453365325928\n",
      "训练次数：3006, loss:0.9001550674438477\n",
      "训练次数：3007, loss:0.9086534976959229\n",
      "训练次数：3008, loss:1.0199016332626343\n",
      "训练次数：3009, loss:0.9250528812408447\n",
      "训练次数：3010, loss:1.0542975664138794\n",
      "训练次数：3011, loss:1.1783887147903442\n",
      "训练次数：3012, loss:1.3754404783248901\n",
      "训练次数：3013, loss:1.1027686595916748\n",
      "训练次数：3014, loss:1.2731274366378784\n",
      "训练次数：3015, loss:1.037839651107788\n",
      "训练次数：3016, loss:0.908271849155426\n",
      "训练次数：3017, loss:0.9908871054649353\n",
      "训练次数：3018, loss:1.242352843284607\n",
      "训练次数：3019, loss:1.0623037815093994\n",
      "训练次数：3020, loss:0.9037017822265625\n",
      "训练次数：3021, loss:1.264776349067688\n",
      "训练次数：3022, loss:1.1996128559112549\n",
      "训练次数：3023, loss:1.3316210508346558\n",
      "训练次数：3024, loss:1.0232521295547485\n",
      "训练次数：3025, loss:0.9454267621040344\n",
      "训练次数：3026, loss:1.1878148317337036\n",
      "训练次数：3027, loss:1.2241578102111816\n",
      "训练次数：3028, loss:0.8257544040679932\n",
      "训练次数：3029, loss:0.8571586012840271\n",
      "训练次数：3030, loss:1.2547943592071533\n",
      "训练次数：3031, loss:1.245432734489441\n",
      "训练次数：3032, loss:1.1378672122955322\n",
      "训练次数：3033, loss:1.0863982439041138\n",
      "训练次数：3034, loss:1.073222041130066\n",
      "训练次数：3035, loss:0.9110357165336609\n",
      "训练次数：3036, loss:1.0936839580535889\n",
      "训练次数：3037, loss:1.1301701068878174\n",
      "训练次数：3038, loss:1.1171919107437134\n",
      "训练次数：3039, loss:1.0980768203735352\n",
      "训练次数：3040, loss:1.06220543384552\n",
      "训练次数：3041, loss:1.0825377702713013\n",
      "训练次数：3042, loss:1.1034197807312012\n",
      "训练次数：3043, loss:1.047533631324768\n",
      "训练次数：3044, loss:1.0386210680007935\n",
      "训练次数：3045, loss:0.842276930809021\n",
      "训练次数：3046, loss:1.1256849765777588\n",
      "训练次数：3047, loss:1.1579210758209229\n",
      "训练次数：3048, loss:1.1123756170272827\n",
      "训练次数：3049, loss:1.1884255409240723\n",
      "训练次数：3050, loss:0.823053777217865\n",
      "训练次数：3051, loss:1.059457778930664\n",
      "训练次数：3052, loss:1.1267707347869873\n",
      "训练次数：3053, loss:1.2634665966033936\n",
      "训练次数：3054, loss:1.0367244482040405\n",
      "训练次数：3055, loss:1.163149356842041\n",
      "训练次数：3056, loss:0.7821712493896484\n",
      "训练次数：3057, loss:1.197981357574463\n",
      "训练次数：3058, loss:0.9716954827308655\n",
      "训练次数：3059, loss:0.9375807642936707\n",
      "训练次数：3060, loss:1.014272928237915\n",
      "训练次数：3061, loss:0.9689035415649414\n",
      "训练次数：3062, loss:1.077143907546997\n",
      "训练次数：3063, loss:0.9376580119132996\n",
      "训练次数：3064, loss:1.053959846496582\n",
      "训练次数：3065, loss:0.933734655380249\n",
      "训练次数：3066, loss:0.8436471223831177\n",
      "训练次数：3067, loss:0.9400110840797424\n",
      "训练次数：3068, loss:1.1748228073120117\n",
      "训练次数：3069, loss:0.9199254512786865\n",
      "训练次数：3070, loss:1.703688621520996\n",
      "训练次数：3071, loss:1.0136303901672363\n",
      "训练次数：3072, loss:0.8826950192451477\n",
      "训练次数：3073, loss:1.4731348752975464\n",
      "训练次数：3074, loss:1.253717303276062\n",
      "训练次数：3075, loss:1.0703099966049194\n",
      "训练次数：3076, loss:1.0544612407684326\n",
      "训练次数：3077, loss:1.0324434041976929\n",
      "训练次数：3078, loss:1.0076326131820679\n",
      "训练次数：3079, loss:0.8645644187927246\n",
      "训练次数：3080, loss:1.0564433336257935\n",
      "训练次数：3081, loss:1.0141217708587646\n",
      "训练次数：3082, loss:1.237424373626709\n",
      "训练次数：3083, loss:0.8415942788124084\n",
      "训练次数：3084, loss:0.9440523982048035\n",
      "训练次数：3085, loss:0.8589319586753845\n",
      "训练次数：3086, loss:0.9740273356437683\n",
      "训练次数：3087, loss:1.065578579902649\n",
      "训练次数：3088, loss:1.0269869565963745\n",
      "训练次数：3089, loss:1.1324509382247925\n",
      "训练次数：3090, loss:0.5898705720901489\n",
      "训练次数：3091, loss:1.0127613544464111\n",
      "训练次数：3092, loss:1.1175401210784912\n",
      "训练次数：3093, loss:0.8849554657936096\n",
      "训练次数：3094, loss:1.3188235759735107\n",
      "训练次数：3095, loss:1.0712244510650635\n",
      "训练次数：3096, loss:1.1334965229034424\n",
      "训练次数：3097, loss:1.3860036134719849\n",
      "训练次数：3098, loss:1.1475491523742676\n",
      "训练次数：3099, loss:1.1685338020324707\n",
      "训练次数：3100, loss:1.2783151865005493\n",
      "训练次数：3101, loss:0.8346328735351562\n",
      "训练次数：3102, loss:0.9856539368629456\n",
      "训练次数：3103, loss:1.1431993246078491\n",
      "训练次数：3104, loss:0.8768362402915955\n",
      "训练次数：3105, loss:1.1574125289916992\n",
      "训练次数：3106, loss:0.9623172283172607\n",
      "训练次数：3107, loss:0.9557269215583801\n",
      "训练次数：3108, loss:1.1709154844284058\n",
      "训练次数：3109, loss:0.9959959983825684\n",
      "训练次数：3110, loss:0.9523831009864807\n",
      "训练次数：3111, loss:1.0300058126449585\n",
      "训练次数：3112, loss:1.0766303539276123\n",
      "训练次数：3113, loss:0.9456503987312317\n",
      "训练次数：3114, loss:1.041979193687439\n",
      "训练次数：3115, loss:0.8360833525657654\n",
      "训练次数：3116, loss:0.8889217376708984\n",
      "训练次数：3117, loss:1.2826077938079834\n",
      "训练次数：3118, loss:1.1792703866958618\n",
      "训练次数：3119, loss:1.084046721458435\n",
      "训练次数：3120, loss:0.8093620538711548\n",
      "训练次数：3121, loss:1.1546800136566162\n",
      "训练次数：3122, loss:1.0451103448867798\n",
      "训练次数：3123, loss:1.162253975868225\n",
      "训练次数：3124, loss:1.0829188823699951\n",
      "训练次数：3125, loss:0.9138891696929932\n",
      "训练次数：3126, loss:0.7901376485824585\n",
      "训练次数：3127, loss:1.1133174896240234\n",
      "训练次数：3128, loss:1.433719515800476\n",
      "----------第4轮训练开始----------\n",
      "训练次数：3129, loss:1.0029046535491943\n",
      "训练次数：3130, loss:0.9975449442863464\n",
      "训练次数：3131, loss:1.421006679534912\n",
      "训练次数：3132, loss:0.8618311285972595\n",
      "训练次数：3133, loss:1.0152251720428467\n",
      "训练次数：3134, loss:1.3198224306106567\n",
      "训练次数：3135, loss:1.0976308584213257\n",
      "训练次数：3136, loss:1.1449065208435059\n",
      "训练次数：3137, loss:1.1278893947601318\n",
      "训练次数：3138, loss:1.3872672319412231\n",
      "训练次数：3139, loss:1.2600281238555908\n",
      "训练次数：3140, loss:1.286693811416626\n",
      "训练次数：3141, loss:1.6408747434616089\n",
      "训练次数：3142, loss:1.1960560083389282\n",
      "训练次数：3143, loss:1.019621729850769\n",
      "训练次数：3144, loss:1.0349903106689453\n",
      "训练次数：3145, loss:1.0799123048782349\n",
      "训练次数：3146, loss:1.3432815074920654\n",
      "训练次数：3147, loss:1.0967636108398438\n",
      "训练次数：3148, loss:1.2604329586029053\n",
      "训练次数：3149, loss:1.2691266536712646\n",
      "训练次数：3150, loss:1.4789949655532837\n",
      "训练次数：3151, loss:1.1554300785064697\n",
      "训练次数：3152, loss:1.2776072025299072\n",
      "训练次数：3153, loss:1.331184983253479\n",
      "训练次数：3154, loss:1.2286323308944702\n",
      "训练次数：3155, loss:1.1651962995529175\n",
      "训练次数：3156, loss:1.0695911645889282\n",
      "训练次数：3157, loss:1.198747992515564\n",
      "训练次数：3158, loss:0.9826472401618958\n",
      "训练次数：3159, loss:1.3688786029815674\n",
      "训练次数：3160, loss:1.4440020322799683\n",
      "训练次数：3161, loss:1.0361608266830444\n",
      "训练次数：3162, loss:1.141720175743103\n",
      "训练次数：3163, loss:1.064791202545166\n",
      "训练次数：3164, loss:0.9931578636169434\n",
      "训练次数：3165, loss:0.9074013233184814\n",
      "训练次数：3166, loss:1.185699224472046\n",
      "训练次数：3167, loss:0.8937892913818359\n",
      "训练次数：3168, loss:1.1214752197265625\n",
      "训练次数：3169, loss:0.8139870166778564\n",
      "训练次数：3170, loss:0.8243370652198792\n",
      "训练次数：3171, loss:1.0629009008407593\n",
      "训练次数：3172, loss:1.0999561548233032\n",
      "训练次数：3173, loss:0.8158283233642578\n",
      "训练次数：3174, loss:0.988009512424469\n",
      "训练次数：3175, loss:1.3412885665893555\n",
      "训练次数：3176, loss:1.1550146341323853\n",
      "训练次数：3177, loss:0.9870949983596802\n",
      "训练次数：3178, loss:1.208687663078308\n",
      "训练次数：3179, loss:1.265555500984192\n",
      "训练次数：3180, loss:1.2144036293029785\n",
      "训练次数：3181, loss:1.2786363363265991\n",
      "训练次数：3182, loss:1.1559844017028809\n",
      "训练次数：3183, loss:1.1950860023498535\n",
      "训练次数：3184, loss:1.0461435317993164\n",
      "训练次数：3185, loss:1.1370248794555664\n",
      "训练次数：3186, loss:0.8429142236709595\n",
      "训练次数：3187, loss:0.7950652241706848\n",
      "训练次数：3188, loss:1.0617753267288208\n",
      "训练次数：3189, loss:1.2069344520568848\n",
      "训练次数：3190, loss:1.0122404098510742\n",
      "训练次数：3191, loss:1.2012429237365723\n",
      "训练次数：3192, loss:1.10861074924469\n",
      "训练次数：3193, loss:1.208719253540039\n",
      "训练次数：3194, loss:1.1817530393600464\n",
      "训练次数：3195, loss:1.02782142162323\n",
      "训练次数：3196, loss:1.1058375835418701\n",
      "训练次数：3197, loss:0.7928480505943298\n",
      "训练次数：3198, loss:1.0880335569381714\n",
      "训练次数：3199, loss:1.257855772972107\n",
      "训练次数：3200, loss:0.7065736055374146\n",
      "训练次数：3201, loss:1.1159404516220093\n",
      "训练次数：3202, loss:1.1144779920578003\n",
      "训练次数：3203, loss:1.1898928880691528\n",
      "训练次数：3204, loss:1.3453153371810913\n",
      "训练次数：3205, loss:1.1849982738494873\n",
      "训练次数：3206, loss:1.3510693311691284\n",
      "训练次数：3207, loss:1.150572657585144\n",
      "训练次数：3208, loss:1.6355153322219849\n",
      "训练次数：3209, loss:1.4027104377746582\n",
      "训练次数：3210, loss:1.0696874856948853\n",
      "训练次数：3211, loss:1.356891393661499\n",
      "训练次数：3212, loss:0.9603652954101562\n",
      "训练次数：3213, loss:1.2264317274093628\n",
      "训练次数：3214, loss:1.049135684967041\n",
      "训练次数：3215, loss:1.2202465534210205\n",
      "训练次数：3216, loss:0.9443578124046326\n",
      "训练次数：3217, loss:0.989745557308197\n",
      "训练次数：3218, loss:1.103512167930603\n",
      "训练次数：3219, loss:1.0724289417266846\n",
      "训练次数：3220, loss:1.3800647258758545\n",
      "训练次数：3221, loss:0.926019012928009\n",
      "训练次数：3222, loss:1.0945985317230225\n",
      "训练次数：3223, loss:1.3805011510849\n",
      "训练次数：3224, loss:1.140466332435608\n",
      "训练次数：3225, loss:0.8334039449691772\n",
      "训练次数：3226, loss:1.193762183189392\n",
      "训练次数：3227, loss:1.1151583194732666\n",
      "训练次数：3228, loss:1.044992446899414\n",
      "训练次数：3229, loss:1.0510597229003906\n",
      "训练次数：3230, loss:0.9566343426704407\n",
      "训练次数：3231, loss:0.8786165714263916\n",
      "训练次数：3232, loss:1.1926432847976685\n",
      "训练次数：3233, loss:1.043497920036316\n",
      "训练次数：3234, loss:0.8661915063858032\n",
      "训练次数：3235, loss:1.021255373954773\n",
      "训练次数：3236, loss:1.3401298522949219\n",
      "训练次数：3237, loss:1.0716227293014526\n",
      "训练次数：3238, loss:0.9168235063552856\n",
      "训练次数：3239, loss:1.0558984279632568\n",
      "训练次数：3240, loss:0.9699879288673401\n",
      "训练次数：3241, loss:1.1894489526748657\n",
      "训练次数：3242, loss:1.0269505977630615\n",
      "训练次数：3243, loss:1.1601406335830688\n",
      "训练次数：3244, loss:1.2846386432647705\n",
      "训练次数：3245, loss:1.2168567180633545\n",
      "训练次数：3246, loss:1.0986359119415283\n",
      "训练次数：3247, loss:1.1142299175262451\n",
      "训练次数：3248, loss:1.2826249599456787\n",
      "训练次数：3249, loss:1.4662790298461914\n",
      "训练次数：3250, loss:1.3620389699935913\n",
      "训练次数：3251, loss:1.1053094863891602\n",
      "训练次数：3252, loss:1.2325187921524048\n",
      "训练次数：3253, loss:1.2126479148864746\n",
      "训练次数：3254, loss:0.9341186881065369\n",
      "训练次数：3255, loss:0.9592788219451904\n",
      "训练次数：3256, loss:1.2172635793685913\n",
      "训练次数：3257, loss:1.12062668800354\n",
      "训练次数：3258, loss:0.9124305844306946\n",
      "训练次数：3259, loss:1.0579913854599\n",
      "训练次数：3260, loss:0.975677490234375\n",
      "训练次数：3261, loss:1.4518177509307861\n",
      "训练次数：3262, loss:1.0379220247268677\n",
      "训练次数：3263, loss:0.9881058931350708\n",
      "训练次数：3264, loss:0.9433916807174683\n",
      "训练次数：3265, loss:1.1089529991149902\n",
      "训练次数：3266, loss:1.1616709232330322\n",
      "训练次数：3267, loss:0.8185600638389587\n",
      "训练次数：3268, loss:1.47683584690094\n",
      "训练次数：3269, loss:1.3700453042984009\n",
      "训练次数：3270, loss:1.0186152458190918\n",
      "训练次数：3271, loss:1.0582056045532227\n",
      "训练次数：3272, loss:0.9910776615142822\n",
      "训练次数：3273, loss:1.1991610527038574\n",
      "训练次数：3274, loss:1.095215916633606\n",
      "训练次数：3275, loss:1.0984086990356445\n",
      "训练次数：3276, loss:1.166968822479248\n",
      "训练次数：3277, loss:0.9784069657325745\n",
      "训练次数：3278, loss:0.9310131072998047\n",
      "训练次数：3279, loss:1.0388554334640503\n",
      "训练次数：3280, loss:1.1240540742874146\n",
      "训练次数：3281, loss:0.9822107553482056\n",
      "训练次数：3282, loss:1.1041525602340698\n",
      "训练次数：3283, loss:1.2450326681137085\n",
      "训练次数：3284, loss:1.150665521621704\n",
      "训练次数：3285, loss:1.224233865737915\n",
      "训练次数：3286, loss:1.0257954597473145\n",
      "训练次数：3287, loss:1.1566150188446045\n",
      "训练次数：3288, loss:1.1983836889266968\n",
      "训练次数：3289, loss:1.0625054836273193\n",
      "训练次数：3290, loss:0.9613459706306458\n",
      "训练次数：3291, loss:1.465269684791565\n",
      "训练次数：3292, loss:1.3815977573394775\n",
      "训练次数：3293, loss:0.9347563982009888\n",
      "训练次数：3294, loss:1.1261351108551025\n",
      "训练次数：3295, loss:1.0378937721252441\n",
      "训练次数：3296, loss:0.995757520198822\n",
      "训练次数：3297, loss:0.9606139063835144\n",
      "训练次数：3298, loss:1.0613006353378296\n",
      "训练次数：3299, loss:1.4418962001800537\n",
      "训练次数：3300, loss:1.1683897972106934\n",
      "训练次数：3301, loss:0.9495182037353516\n",
      "训练次数：3302, loss:1.3465094566345215\n",
      "训练次数：3303, loss:1.0094530582427979\n",
      "训练次数：3304, loss:1.1071594953536987\n",
      "训练次数：3305, loss:0.8871608972549438\n",
      "训练次数：3306, loss:1.1063990592956543\n",
      "训练次数：3307, loss:0.8927934169769287\n",
      "训练次数：3308, loss:1.1235508918762207\n",
      "训练次数：3309, loss:0.8522557616233826\n",
      "训练次数：3310, loss:0.9495223164558411\n",
      "训练次数：3311, loss:0.9935451149940491\n",
      "训练次数：3312, loss:1.152986764907837\n",
      "训练次数：3313, loss:1.4292880296707153\n",
      "训练次数：3314, loss:1.0953973531723022\n",
      "训练次数：3315, loss:1.5347431898117065\n",
      "训练次数：3316, loss:1.1596277952194214\n",
      "训练次数：3317, loss:1.0273938179016113\n",
      "训练次数：3318, loss:1.1549440622329712\n",
      "训练次数：3319, loss:0.8635488152503967\n",
      "训练次数：3320, loss:1.0058326721191406\n",
      "训练次数：3321, loss:1.2927018404006958\n",
      "训练次数：3322, loss:1.0624161958694458\n",
      "训练次数：3323, loss:0.9797115325927734\n",
      "训练次数：3324, loss:1.327277660369873\n",
      "训练次数：3325, loss:1.4832357168197632\n",
      "训练次数：3326, loss:0.9433763027191162\n",
      "训练次数：3327, loss:1.2084535360336304\n",
      "训练次数：3328, loss:1.152482509613037\n",
      "训练次数：3329, loss:0.8190913200378418\n",
      "训练次数：3330, loss:1.452061414718628\n",
      "训练次数：3331, loss:1.367710828781128\n",
      "训练次数：3332, loss:1.3415123224258423\n",
      "训练次数：3333, loss:1.1518816947937012\n",
      "训练次数：3334, loss:1.2792954444885254\n",
      "训练次数：3335, loss:0.9980823397636414\n",
      "训练次数：3336, loss:1.2267067432403564\n",
      "训练次数：3337, loss:0.9648507237434387\n",
      "训练次数：3338, loss:1.4722797870635986\n",
      "训练次数：3339, loss:1.0852553844451904\n",
      "训练次数：3340, loss:1.102832317352295\n",
      "训练次数：3341, loss:1.0420401096343994\n",
      "训练次数：3342, loss:1.091165542602539\n",
      "训练次数：3343, loss:1.4155993461608887\n",
      "训练次数：3344, loss:1.2211874723434448\n",
      "训练次数：3345, loss:0.9993432760238647\n",
      "训练次数：3346, loss:1.3649694919586182\n",
      "训练次数：3347, loss:1.1614142656326294\n",
      "训练次数：3348, loss:1.0464353561401367\n",
      "训练次数：3349, loss:1.21945059299469\n",
      "训练次数：3350, loss:1.2922842502593994\n",
      "训练次数：3351, loss:1.073982834815979\n",
      "训练次数：3352, loss:1.2320127487182617\n",
      "训练次数：3353, loss:1.2387439012527466\n",
      "训练次数：3354, loss:0.9306343793869019\n",
      "训练次数：3355, loss:1.136181354522705\n",
      "训练次数：3356, loss:1.0534353256225586\n",
      "训练次数：3357, loss:1.0986205339431763\n",
      "训练次数：3358, loss:1.0643094778060913\n",
      "训练次数：3359, loss:1.0313482284545898\n",
      "训练次数：3360, loss:1.1466926336288452\n",
      "训练次数：3361, loss:1.1291192770004272\n",
      "训练次数：3362, loss:1.3091999292373657\n",
      "训练次数：3363, loss:0.8319076299667358\n",
      "训练次数：3364, loss:1.2282384634017944\n",
      "训练次数：3365, loss:1.2668155431747437\n",
      "训练次数：3366, loss:1.118284821510315\n",
      "训练次数：3367, loss:1.1111913919448853\n",
      "训练次数：3368, loss:1.1535098552703857\n",
      "训练次数：3369, loss:1.036550521850586\n",
      "训练次数：3370, loss:1.1455003023147583\n",
      "训练次数：3371, loss:1.2172729969024658\n",
      "训练次数：3372, loss:1.5393046140670776\n",
      "训练次数：3373, loss:1.00423002243042\n",
      "训练次数：3374, loss:1.4437744617462158\n",
      "训练次数：3375, loss:1.1133480072021484\n",
      "训练次数：3376, loss:1.2631303071975708\n",
      "训练次数：3377, loss:1.1239126920700073\n",
      "训练次数：3378, loss:1.1632229089736938\n",
      "训练次数：3379, loss:0.8569547533988953\n",
      "训练次数：3380, loss:1.1301307678222656\n",
      "训练次数：3381, loss:1.0540070533752441\n",
      "训练次数：3382, loss:1.328279972076416\n",
      "训练次数：3383, loss:1.144687294960022\n",
      "训练次数：3384, loss:0.9409920573234558\n",
      "训练次数：3385, loss:1.2577331066131592\n",
      "训练次数：3386, loss:0.8393968343734741\n",
      "训练次数：3387, loss:1.512376070022583\n",
      "训练次数：3388, loss:1.1448006629943848\n",
      "训练次数：3389, loss:1.3183796405792236\n",
      "训练次数：3390, loss:1.2322959899902344\n",
      "训练次数：3391, loss:1.2054860591888428\n",
      "训练次数：3392, loss:0.9631056189537048\n",
      "训练次数：3393, loss:1.1673181056976318\n",
      "训练次数：3394, loss:1.1204389333724976\n",
      "训练次数：3395, loss:1.2908735275268555\n",
      "训练次数：3396, loss:0.9516198635101318\n",
      "训练次数：3397, loss:1.0391428470611572\n",
      "训练次数：3398, loss:0.9943796396255493\n",
      "训练次数：3399, loss:1.2842378616333008\n",
      "训练次数：3400, loss:1.0444834232330322\n",
      "训练次数：3401, loss:1.2633228302001953\n",
      "训练次数：3402, loss:1.080518364906311\n",
      "训练次数：3403, loss:0.9449294209480286\n",
      "训练次数：3404, loss:1.0463577508926392\n",
      "训练次数：3405, loss:1.457375168800354\n",
      "训练次数：3406, loss:1.2564027309417725\n",
      "训练次数：3407, loss:1.20223069190979\n",
      "训练次数：3408, loss:1.0781506299972534\n",
      "训练次数：3409, loss:0.9480271339416504\n",
      "训练次数：3410, loss:1.421726942062378\n",
      "训练次数：3411, loss:0.8794977068901062\n",
      "训练次数：3412, loss:1.2434052228927612\n",
      "训练次数：3413, loss:1.2469191551208496\n",
      "训练次数：3414, loss:1.2699987888336182\n",
      "训练次数：3415, loss:1.17178213596344\n",
      "训练次数：3416, loss:1.0768991708755493\n",
      "训练次数：3417, loss:1.0821876525878906\n",
      "训练次数：3418, loss:1.1312947273254395\n",
      "训练次数：3419, loss:1.159196376800537\n",
      "训练次数：3420, loss:1.236782193183899\n",
      "训练次数：3421, loss:0.9358583092689514\n",
      "训练次数：3422, loss:1.1900008916854858\n",
      "训练次数：3423, loss:1.1625125408172607\n",
      "训练次数：3424, loss:0.9578152298927307\n",
      "训练次数：3425, loss:1.2640173435211182\n",
      "训练次数：3426, loss:1.1115580797195435\n",
      "训练次数：3427, loss:1.1203047037124634\n",
      "训练次数：3428, loss:1.2064533233642578\n",
      "训练次数：3429, loss:1.1125833988189697\n",
      "训练次数：3430, loss:1.0026782751083374\n",
      "训练次数：3431, loss:0.9711434245109558\n",
      "训练次数：3432, loss:0.7597718834877014\n",
      "训练次数：3433, loss:0.9053393006324768\n",
      "训练次数：3434, loss:1.4154671430587769\n",
      "训练次数：3435, loss:1.073392629623413\n",
      "训练次数：3436, loss:1.286483645439148\n",
      "训练次数：3437, loss:1.4052810668945312\n",
      "训练次数：3438, loss:1.3701539039611816\n",
      "训练次数：3439, loss:1.0186898708343506\n",
      "训练次数：3440, loss:1.1569852828979492\n",
      "训练次数：3441, loss:1.0689853429794312\n",
      "训练次数：3442, loss:1.259653925895691\n",
      "训练次数：3443, loss:1.092255711555481\n",
      "训练次数：3444, loss:1.1993043422698975\n",
      "训练次数：3445, loss:1.0066590309143066\n",
      "训练次数：3446, loss:1.2130017280578613\n",
      "训练次数：3447, loss:0.9632889032363892\n",
      "训练次数：3448, loss:1.0888044834136963\n",
      "训练次数：3449, loss:1.1519237756729126\n",
      "训练次数：3450, loss:1.0521376132965088\n",
      "训练次数：3451, loss:1.0295350551605225\n",
      "训练次数：3452, loss:1.1927640438079834\n",
      "训练次数：3453, loss:1.0736603736877441\n",
      "训练次数：3454, loss:0.9264524579048157\n",
      "训练次数：3455, loss:0.9613263010978699\n",
      "训练次数：3456, loss:1.115532636642456\n",
      "训练次数：3457, loss:1.0146697759628296\n",
      "训练次数：3458, loss:0.9223132133483887\n",
      "训练次数：3459, loss:1.113426685333252\n",
      "训练次数：3460, loss:0.963166356086731\n",
      "训练次数：3461, loss:1.0199629068374634\n",
      "训练次数：3462, loss:0.8723999857902527\n",
      "训练次数：3463, loss:1.1776920557022095\n",
      "训练次数：3464, loss:1.0921902656555176\n",
      "训练次数：3465, loss:1.0653406381607056\n",
      "训练次数：3466, loss:0.931613028049469\n",
      "训练次数：3467, loss:0.9782848954200745\n",
      "训练次数：3468, loss:1.240891695022583\n",
      "训练次数：3469, loss:1.085666537284851\n",
      "训练次数：3470, loss:1.4012947082519531\n",
      "训练次数：3471, loss:1.2016773223876953\n",
      "训练次数：3472, loss:1.0921965837478638\n",
      "训练次数：3473, loss:1.3255771398544312\n",
      "训练次数：3474, loss:1.0101077556610107\n",
      "训练次数：3475, loss:1.2379953861236572\n",
      "训练次数：3476, loss:1.2707940340042114\n",
      "训练次数：3477, loss:1.0004857778549194\n",
      "训练次数：3478, loss:1.3039098978042603\n",
      "训练次数：3479, loss:1.0989195108413696\n",
      "训练次数：3480, loss:1.0461809635162354\n",
      "训练次数：3481, loss:0.9063024520874023\n",
      "训练次数：3482, loss:1.23884117603302\n",
      "训练次数：3483, loss:0.9063947796821594\n",
      "训练次数：3484, loss:1.3005839586257935\n",
      "训练次数：3485, loss:1.058879017829895\n",
      "训练次数：3486, loss:1.3348556756973267\n",
      "训练次数：3487, loss:1.4509284496307373\n",
      "训练次数：3488, loss:1.0923058986663818\n",
      "训练次数：3489, loss:1.1525369882583618\n",
      "训练次数：3490, loss:0.9210735559463501\n",
      "训练次数：3491, loss:1.1602429151535034\n",
      "训练次数：3492, loss:1.011845350265503\n",
      "训练次数：3493, loss:1.0408306121826172\n",
      "训练次数：3494, loss:0.8257381916046143\n",
      "训练次数：3495, loss:1.2260465621948242\n",
      "训练次数：3496, loss:1.1657764911651611\n",
      "训练次数：3497, loss:0.8375284075737\n",
      "训练次数：3498, loss:1.2073853015899658\n",
      "训练次数：3499, loss:1.0775808095932007\n",
      "训练次数：3500, loss:1.2430830001831055\n",
      "训练次数：3501, loss:1.1153810024261475\n",
      "训练次数：3502, loss:1.263171911239624\n",
      "训练次数：3503, loss:1.195067048072815\n",
      "训练次数：3504, loss:0.9600116014480591\n",
      "训练次数：3505, loss:0.8995310664176941\n",
      "训练次数：3506, loss:0.9696964025497437\n",
      "训练次数：3507, loss:1.0608714818954468\n",
      "训练次数：3508, loss:1.2616543769836426\n",
      "训练次数：3509, loss:1.2058148384094238\n",
      "训练次数：3510, loss:1.1544206142425537\n",
      "训练次数：3511, loss:1.0331032276153564\n",
      "训练次数：3512, loss:1.1451663970947266\n",
      "训练次数：3513, loss:1.0552566051483154\n",
      "训练次数：3514, loss:0.985993504524231\n",
      "训练次数：3515, loss:1.2139627933502197\n",
      "训练次数：3516, loss:1.4849447011947632\n",
      "训练次数：3517, loss:1.2223092317581177\n",
      "训练次数：3518, loss:0.9846882224082947\n",
      "训练次数：3519, loss:0.9801561832427979\n",
      "训练次数：3520, loss:0.9252141118049622\n",
      "训练次数：3521, loss:0.9114236831665039\n",
      "训练次数：3522, loss:1.1332390308380127\n",
      "训练次数：3523, loss:1.2904951572418213\n",
      "训练次数：3524, loss:1.2531952857971191\n",
      "训练次数：3525, loss:1.2360283136367798\n",
      "训练次数：3526, loss:0.9756368398666382\n",
      "训练次数：3527, loss:1.04373300075531\n",
      "训练次数：3528, loss:1.1117640733718872\n",
      "训练次数：3529, loss:1.2313494682312012\n",
      "训练次数：3530, loss:1.369596004486084\n",
      "训练次数：3531, loss:1.145925521850586\n",
      "训练次数：3532, loss:1.146520733833313\n",
      "训练次数：3533, loss:1.0775413513183594\n",
      "训练次数：3534, loss:0.9071474671363831\n",
      "训练次数：3535, loss:1.040942907333374\n",
      "训练次数：3536, loss:0.7121666073799133\n",
      "训练次数：3537, loss:1.1337450742721558\n",
      "训练次数：3538, loss:0.6866483092308044\n",
      "训练次数：3539, loss:1.08993399143219\n",
      "训练次数：3540, loss:1.2017203569412231\n",
      "训练次数：3541, loss:1.2993232011795044\n",
      "训练次数：3542, loss:0.9091445207595825\n",
      "训练次数：3543, loss:1.174932599067688\n",
      "训练次数：3544, loss:1.141913890838623\n",
      "训练次数：3545, loss:1.3916856050491333\n",
      "训练次数：3546, loss:1.2272454500198364\n",
      "训练次数：3547, loss:1.3105323314666748\n",
      "训练次数：3548, loss:1.0556609630584717\n",
      "训练次数：3549, loss:1.249576210975647\n",
      "训练次数：3550, loss:1.0951563119888306\n",
      "训练次数：3551, loss:1.3385804891586304\n",
      "训练次数：3552, loss:1.2534185647964478\n",
      "训练次数：3553, loss:1.176304578781128\n",
      "训练次数：3554, loss:1.361359715461731\n",
      "训练次数：3555, loss:1.142095923423767\n",
      "训练次数：3556, loss:0.9083356857299805\n",
      "训练次数：3557, loss:1.1448214054107666\n",
      "训练次数：3558, loss:1.204263687133789\n",
      "训练次数：3559, loss:1.0912431478500366\n",
      "训练次数：3560, loss:0.9285921454429626\n",
      "训练次数：3561, loss:0.9716338515281677\n",
      "训练次数：3562, loss:1.009561538696289\n",
      "训练次数：3563, loss:1.254287600517273\n",
      "训练次数：3564, loss:1.3145668506622314\n",
      "训练次数：3565, loss:1.1149955987930298\n",
      "训练次数：3566, loss:1.0285660028457642\n",
      "训练次数：3567, loss:1.3331743478775024\n",
      "训练次数：3568, loss:1.0106921195983887\n",
      "训练次数：3569, loss:1.2527416944503784\n",
      "训练次数：3570, loss:1.0867512226104736\n",
      "训练次数：3571, loss:1.3775081634521484\n",
      "训练次数：3572, loss:1.1237101554870605\n",
      "训练次数：3573, loss:1.2115700244903564\n",
      "训练次数：3574, loss:1.1279114484786987\n",
      "训练次数：3575, loss:1.3426889181137085\n",
      "训练次数：3576, loss:1.0367889404296875\n",
      "训练次数：3577, loss:1.1590627431869507\n",
      "训练次数：3578, loss:1.513959288597107\n",
      "训练次数：3579, loss:1.010553002357483\n",
      "训练次数：3580, loss:0.9286030530929565\n",
      "训练次数：3581, loss:1.0498589277267456\n",
      "训练次数：3582, loss:1.4278892278671265\n",
      "训练次数：3583, loss:1.293562412261963\n",
      "训练次数：3584, loss:1.2707971334457397\n",
      "训练次数：3585, loss:1.388096570968628\n",
      "训练次数：3586, loss:1.0409328937530518\n",
      "训练次数：3587, loss:0.9090740084648132\n",
      "训练次数：3588, loss:1.1710400581359863\n",
      "训练次数：3589, loss:1.038086175918579\n",
      "训练次数：3590, loss:1.1125890016555786\n",
      "训练次数：3591, loss:1.2954227924346924\n",
      "训练次数：3592, loss:1.3101062774658203\n",
      "训练次数：3593, loss:1.0949996709823608\n",
      "训练次数：3594, loss:0.9842670559883118\n",
      "训练次数：3595, loss:1.2894909381866455\n",
      "训练次数：3596, loss:1.0327874422073364\n",
      "训练次数：3597, loss:0.8838965892791748\n",
      "训练次数：3598, loss:1.599450707435608\n",
      "训练次数：3599, loss:0.9790346026420593\n",
      "训练次数：3600, loss:1.1122835874557495\n",
      "训练次数：3601, loss:0.907855749130249\n",
      "训练次数：3602, loss:0.9706891775131226\n",
      "训练次数：3603, loss:1.162917971611023\n",
      "训练次数：3604, loss:1.1800647974014282\n",
      "训练次数：3605, loss:1.0966864824295044\n",
      "训练次数：3606, loss:0.8858429193496704\n",
      "训练次数：3607, loss:1.4256654977798462\n",
      "训练次数：3608, loss:1.5421100854873657\n",
      "训练次数：3609, loss:1.280168890953064\n",
      "训练次数：3610, loss:1.3630765676498413\n",
      "训练次数：3611, loss:1.0529992580413818\n",
      "训练次数：3612, loss:0.9799948334693909\n",
      "训练次数：3613, loss:1.3378249406814575\n",
      "训练次数：3614, loss:1.2330262660980225\n",
      "训练次数：3615, loss:0.9470952749252319\n",
      "训练次数：3616, loss:1.33766770362854\n",
      "训练次数：3617, loss:1.2245832681655884\n",
      "训练次数：3618, loss:0.8557608723640442\n",
      "训练次数：3619, loss:1.4041846990585327\n",
      "训练次数：3620, loss:1.1075010299682617\n",
      "训练次数：3621, loss:1.1965034008026123\n",
      "训练次数：3622, loss:1.303708553314209\n",
      "训练次数：3623, loss:1.2192792892456055\n",
      "训练次数：3624, loss:1.4266068935394287\n",
      "训练次数：3625, loss:0.9433823823928833\n",
      "训练次数：3626, loss:1.3008774518966675\n",
      "训练次数：3627, loss:1.1153194904327393\n",
      "训练次数：3628, loss:1.0805648565292358\n",
      "训练次数：3629, loss:1.4077352285385132\n",
      "训练次数：3630, loss:1.054120421409607\n",
      "训练次数：3631, loss:1.0500630140304565\n",
      "训练次数：3632, loss:1.5061867237091064\n",
      "训练次数：3633, loss:1.1134744882583618\n",
      "训练次数：3634, loss:1.4601417779922485\n",
      "训练次数：3635, loss:1.0181357860565186\n",
      "训练次数：3636, loss:1.1697791814804077\n",
      "训练次数：3637, loss:1.320760726928711\n",
      "训练次数：3638, loss:1.2775449752807617\n",
      "训练次数：3639, loss:1.123537540435791\n",
      "训练次数：3640, loss:1.0082100629806519\n",
      "训练次数：3641, loss:0.9794564843177795\n",
      "训练次数：3642, loss:1.3204182386398315\n",
      "训练次数：3643, loss:1.1590956449508667\n",
      "训练次数：3644, loss:1.2779345512390137\n",
      "训练次数：3645, loss:1.311489224433899\n",
      "训练次数：3646, loss:1.04486083984375\n",
      "训练次数：3647, loss:1.138146162033081\n",
      "训练次数：3648, loss:1.0429134368896484\n",
      "训练次数：3649, loss:1.212599277496338\n",
      "训练次数：3650, loss:1.2261686325073242\n",
      "训练次数：3651, loss:1.4040272235870361\n",
      "训练次数：3652, loss:1.4806828498840332\n",
      "训练次数：3653, loss:1.5194975137710571\n",
      "训练次数：3654, loss:1.1507941484451294\n",
      "训练次数：3655, loss:1.213154911994934\n",
      "训练次数：3656, loss:1.2598609924316406\n",
      "训练次数：3657, loss:1.1581023931503296\n",
      "训练次数：3658, loss:1.3926713466644287\n",
      "训练次数：3659, loss:1.4019585847854614\n",
      "训练次数：3660, loss:1.082309603691101\n",
      "训练次数：3661, loss:1.2971934080123901\n",
      "训练次数：3662, loss:0.8896945714950562\n",
      "训练次数：3663, loss:1.125083327293396\n",
      "训练次数：3664, loss:1.1240665912628174\n",
      "训练次数：3665, loss:1.3452035188674927\n",
      "训练次数：3666, loss:1.0524561405181885\n",
      "训练次数：3667, loss:1.094896674156189\n",
      "训练次数：3668, loss:1.0408480167388916\n",
      "训练次数：3669, loss:1.0066540241241455\n",
      "训练次数：3670, loss:1.242825984954834\n",
      "训练次数：3671, loss:1.2615032196044922\n",
      "训练次数：3672, loss:1.1960891485214233\n",
      "训练次数：3673, loss:0.9807546138763428\n",
      "训练次数：3674, loss:1.009634256362915\n",
      "训练次数：3675, loss:1.3276638984680176\n",
      "训练次数：3676, loss:1.167381763458252\n",
      "训练次数：3677, loss:1.4140734672546387\n",
      "训练次数：3678, loss:1.283028244972229\n",
      "训练次数：3679, loss:1.0197521448135376\n",
      "训练次数：3680, loss:1.2043215036392212\n",
      "训练次数：3681, loss:0.8684105277061462\n",
      "训练次数：3682, loss:1.2500423192977905\n",
      "训练次数：3683, loss:1.15814208984375\n",
      "训练次数：3684, loss:0.9655055403709412\n",
      "训练次数：3685, loss:1.3975080251693726\n",
      "训练次数：3686, loss:0.9581044316291809\n",
      "训练次数：3687, loss:1.1445063352584839\n",
      "训练次数：3688, loss:1.119152307510376\n",
      "训练次数：3689, loss:1.0213700532913208\n",
      "训练次数：3690, loss:1.156457781791687\n",
      "训练次数：3691, loss:1.3527299165725708\n",
      "训练次数：3692, loss:0.9860718846321106\n",
      "训练次数：3693, loss:1.00809645652771\n",
      "训练次数：3694, loss:1.0993868112564087\n",
      "训练次数：3695, loss:1.152840495109558\n",
      "训练次数：3696, loss:1.2309324741363525\n",
      "训练次数：3697, loss:0.978731632232666\n",
      "训练次数：3698, loss:1.1850786209106445\n",
      "训练次数：3699, loss:0.900261402130127\n",
      "训练次数：3700, loss:0.8681690692901611\n",
      "训练次数：3701, loss:1.0603076219558716\n",
      "训练次数：3702, loss:1.3477332592010498\n",
      "训练次数：3703, loss:0.9987705945968628\n",
      "训练次数：3704, loss:1.3153941631317139\n",
      "训练次数：3705, loss:1.2795838117599487\n",
      "训练次数：3706, loss:1.5075678825378418\n",
      "训练次数：3707, loss:0.8919283151626587\n",
      "训练次数：3708, loss:1.101914405822754\n",
      "训练次数：3709, loss:1.2396405935287476\n",
      "训练次数：3710, loss:1.2787219285964966\n",
      "训练次数：3711, loss:1.0548080205917358\n",
      "训练次数：3712, loss:1.398092269897461\n",
      "训练次数：3713, loss:1.2354604005813599\n",
      "训练次数：3714, loss:1.31606125831604\n",
      "训练次数：3715, loss:1.134147047996521\n",
      "训练次数：3716, loss:0.9419574737548828\n",
      "训练次数：3717, loss:1.1715052127838135\n",
      "训练次数：3718, loss:1.2323932647705078\n",
      "训练次数：3719, loss:1.0853767395019531\n",
      "训练次数：3720, loss:1.1141618490219116\n",
      "训练次数：3721, loss:1.6018069982528687\n",
      "训练次数：3722, loss:1.1980234384536743\n",
      "训练次数：3723, loss:1.0187164545059204\n",
      "训练次数：3724, loss:1.4817872047424316\n",
      "训练次数：3725, loss:1.4213389158248901\n",
      "训练次数：3726, loss:1.1747113466262817\n",
      "训练次数：3727, loss:1.1978683471679688\n",
      "训练次数：3728, loss:1.4217761754989624\n",
      "训练次数：3729, loss:1.1320774555206299\n",
      "训练次数：3730, loss:1.314555048942566\n",
      "训练次数：3731, loss:1.0710774660110474\n",
      "训练次数：3732, loss:1.4270851612091064\n",
      "训练次数：3733, loss:0.9995046854019165\n",
      "训练次数：3734, loss:1.0842503309249878\n",
      "训练次数：3735, loss:1.16007399559021\n",
      "训练次数：3736, loss:1.1263891458511353\n",
      "训练次数：3737, loss:1.0842918157577515\n",
      "训练次数：3738, loss:1.2760823965072632\n",
      "训练次数：3739, loss:1.2666932344436646\n",
      "训练次数：3740, loss:1.2441359758377075\n",
      "训练次数：3741, loss:1.0812188386917114\n",
      "训练次数：3742, loss:0.8765339255332947\n",
      "训练次数：3743, loss:1.3433455228805542\n",
      "训练次数：3744, loss:0.9975391030311584\n",
      "训练次数：3745, loss:1.2300972938537598\n",
      "训练次数：3746, loss:1.1217246055603027\n",
      "训练次数：3747, loss:1.259748935699463\n",
      "训练次数：3748, loss:1.5180703401565552\n",
      "训练次数：3749, loss:1.1312004327774048\n",
      "训练次数：3750, loss:1.3603794574737549\n",
      "训练次数：3751, loss:1.0971367359161377\n",
      "训练次数：3752, loss:1.0096784830093384\n",
      "训练次数：3753, loss:1.1569989919662476\n",
      "训练次数：3754, loss:0.8366355299949646\n",
      "训练次数：3755, loss:1.0885721445083618\n",
      "训练次数：3756, loss:1.148939847946167\n",
      "训练次数：3757, loss:1.4411306381225586\n",
      "训练次数：3758, loss:0.8001958727836609\n",
      "训练次数：3759, loss:1.1575762033462524\n",
      "训练次数：3760, loss:1.5879552364349365\n",
      "训练次数：3761, loss:1.2509286403656006\n",
      "训练次数：3762, loss:1.068226933479309\n",
      "训练次数：3763, loss:1.257087230682373\n",
      "训练次数：3764, loss:0.8815761208534241\n",
      "训练次数：3765, loss:1.135459542274475\n",
      "训练次数：3766, loss:1.2960516214370728\n",
      "训练次数：3767, loss:1.212664008140564\n",
      "训练次数：3768, loss:1.3646440505981445\n",
      "训练次数：3769, loss:1.1161338090896606\n",
      "训练次数：3770, loss:1.2191323041915894\n",
      "训练次数：3771, loss:1.0203889608383179\n",
      "训练次数：3772, loss:0.9527278542518616\n",
      "训练次数：3773, loss:0.9104301929473877\n",
      "训练次数：3774, loss:1.540364146232605\n",
      "训练次数：3775, loss:1.159859299659729\n",
      "训练次数：3776, loss:1.0794036388397217\n",
      "训练次数：3777, loss:1.1349962949752808\n",
      "训练次数：3778, loss:1.4970643520355225\n",
      "训练次数：3779, loss:1.2216689586639404\n",
      "训练次数：3780, loss:1.1250314712524414\n",
      "训练次数：3781, loss:0.8266364336013794\n",
      "训练次数：3782, loss:1.2248586416244507\n",
      "训练次数：3783, loss:0.9999879002571106\n",
      "训练次数：3784, loss:1.0294040441513062\n",
      "训练次数：3785, loss:1.1454517841339111\n",
      "训练次数：3786, loss:1.1514657735824585\n",
      "训练次数：3787, loss:1.162432074546814\n",
      "训练次数：3788, loss:0.9706116914749146\n",
      "训练次数：3789, loss:0.9341529607772827\n",
      "训练次数：3790, loss:0.9828265905380249\n",
      "训练次数：3791, loss:0.9421371221542358\n",
      "训练次数：3792, loss:1.0962058305740356\n",
      "训练次数：3793, loss:1.4911929368972778\n",
      "训练次数：3794, loss:1.310882329940796\n",
      "训练次数：3795, loss:1.2163012027740479\n",
      "训练次数：3796, loss:1.1996135711669922\n",
      "训练次数：3797, loss:1.0813416242599487\n",
      "训练次数：3798, loss:1.0750126838684082\n",
      "训练次数：3799, loss:1.1131194829940796\n",
      "训练次数：3800, loss:1.1681795120239258\n",
      "训练次数：3801, loss:1.095144510269165\n",
      "训练次数：3802, loss:1.0573657751083374\n",
      "训练次数：3803, loss:1.389567255973816\n",
      "训练次数：3804, loss:1.1312131881713867\n",
      "训练次数：3805, loss:1.5330476760864258\n",
      "训练次数：3806, loss:1.0685148239135742\n",
      "训练次数：3807, loss:0.8751737475395203\n",
      "训练次数：3808, loss:1.3178222179412842\n",
      "训练次数：3809, loss:1.2364776134490967\n",
      "训练次数：3810, loss:0.7243240475654602\n",
      "训练次数：3811, loss:1.0184181928634644\n",
      "训练次数：3812, loss:1.1211413145065308\n",
      "训练次数：3813, loss:1.028703212738037\n",
      "训练次数：3814, loss:1.3832073211669922\n",
      "训练次数：3815, loss:1.1123713254928589\n",
      "训练次数：3816, loss:1.0586539506912231\n",
      "训练次数：3817, loss:0.9532355070114136\n",
      "训练次数：3818, loss:1.189318060874939\n",
      "训练次数：3819, loss:1.2700077295303345\n",
      "训练次数：3820, loss:1.0364224910736084\n",
      "训练次数：3821, loss:1.1288026571273804\n",
      "训练次数：3822, loss:1.2285066843032837\n",
      "训练次数：3823, loss:1.007399320602417\n",
      "训练次数：3824, loss:1.1226226091384888\n",
      "训练次数：3825, loss:1.1476316452026367\n",
      "训练次数：3826, loss:1.1561225652694702\n",
      "训练次数：3827, loss:0.8284184336662292\n",
      "训练次数：3828, loss:1.2693846225738525\n",
      "训练次数：3829, loss:1.1603634357452393\n",
      "训练次数：3830, loss:1.194197654724121\n",
      "训练次数：3831, loss:1.0707982778549194\n",
      "训练次数：3832, loss:0.9551044702529907\n",
      "训练次数：3833, loss:1.2810002565383911\n",
      "训练次数：3834, loss:1.0844656229019165\n",
      "训练次数：3835, loss:1.3551613092422485\n",
      "训练次数：3836, loss:1.005122423171997\n",
      "训练次数：3837, loss:1.4053491353988647\n",
      "训练次数：3838, loss:1.2833033800125122\n",
      "训练次数：3839, loss:1.095289945602417\n",
      "训练次数：3840, loss:0.9632291197776794\n",
      "训练次数：3841, loss:1.0960748195648193\n",
      "训练次数：3842, loss:1.0769264698028564\n",
      "训练次数：3843, loss:1.1058114767074585\n",
      "训练次数：3844, loss:1.1038129329681396\n",
      "训练次数：3845, loss:1.349417805671692\n",
      "训练次数：3846, loss:1.2405354976654053\n",
      "训练次数：3847, loss:1.119025468826294\n",
      "训练次数：3848, loss:0.9083624482154846\n",
      "训练次数：3849, loss:1.2601863145828247\n",
      "训练次数：3850, loss:1.015705943107605\n",
      "训练次数：3851, loss:1.0111526250839233\n",
      "训练次数：3852, loss:1.6145741939544678\n",
      "训练次数：3853, loss:1.2009609937667847\n",
      "训练次数：3854, loss:0.9986690878868103\n",
      "训练次数：3855, loss:1.665884017944336\n",
      "训练次数：3856, loss:1.7832993268966675\n",
      "训练次数：3857, loss:1.0631715059280396\n",
      "训练次数：3858, loss:1.5910086631774902\n",
      "训练次数：3859, loss:1.159873366355896\n",
      "训练次数：3860, loss:0.9783959984779358\n",
      "训练次数：3861, loss:1.1104599237442017\n",
      "训练次数：3862, loss:1.1998316049575806\n",
      "训练次数：3863, loss:1.7077304124832153\n",
      "训练次数：3864, loss:1.563791036605835\n",
      "训练次数：3865, loss:1.2514158487319946\n",
      "训练次数：3866, loss:1.4288265705108643\n",
      "训练次数：3867, loss:1.2488077878952026\n",
      "训练次数：3868, loss:1.1210591793060303\n",
      "训练次数：3869, loss:1.4360779523849487\n",
      "训练次数：3870, loss:1.1385647058486938\n",
      "训练次数：3871, loss:1.6353486776351929\n",
      "训练次数：3872, loss:0.8341832756996155\n",
      "训练次数：3873, loss:1.2204471826553345\n",
      "训练次数：3874, loss:1.2161048650741577\n",
      "训练次数：3875, loss:1.273194432258606\n",
      "训练次数：3876, loss:1.3466503620147705\n",
      "训练次数：3877, loss:1.316759467124939\n",
      "训练次数：3878, loss:1.3808214664459229\n",
      "训练次数：3879, loss:1.506581425666809\n",
      "训练次数：3880, loss:1.353699803352356\n",
      "训练次数：3881, loss:1.509458065032959\n",
      "训练次数：3882, loss:1.2759546041488647\n",
      "训练次数：3883, loss:1.086395502090454\n",
      "训练次数：3884, loss:1.3452959060668945\n",
      "训练次数：3885, loss:1.2978143692016602\n",
      "训练次数：3886, loss:1.0202572345733643\n",
      "训练次数：3887, loss:1.3785139322280884\n",
      "训练次数：3888, loss:1.095664143562317\n",
      "训练次数：3889, loss:1.3075056076049805\n",
      "训练次数：3890, loss:1.5447436571121216\n",
      "训练次数：3891, loss:1.2442951202392578\n",
      "训练次数：3892, loss:1.4297635555267334\n",
      "训练次数：3893, loss:1.4826605319976807\n",
      "训练次数：3894, loss:1.4819633960723877\n",
      "训练次数：3895, loss:1.0720319747924805\n",
      "训练次数：3896, loss:1.498499870300293\n",
      "训练次数：3897, loss:1.3325560092926025\n",
      "训练次数：3898, loss:1.4354695081710815\n",
      "训练次数：3899, loss:1.5540035963058472\n",
      "训练次数：3900, loss:1.2735240459442139\n",
      "训练次数：3901, loss:1.245240330696106\n",
      "训练次数：3902, loss:1.3269380331039429\n",
      "训练次数：3903, loss:1.4481698274612427\n",
      "训练次数：3904, loss:1.145070195198059\n",
      "训练次数：3905, loss:1.5324339866638184\n",
      "训练次数：3906, loss:1.0982505083084106\n",
      "训练次数：3907, loss:1.222762942314148\n",
      "训练次数：3908, loss:0.9767305850982666\n",
      "训练次数：3909, loss:1.371627926826477\n",
      "训练次数：3910, loss:1.6210544109344482\n",
      "----------第5轮训练开始----------\n",
      "训练次数：3911, loss:1.3286852836608887\n",
      "训练次数：3912, loss:0.983910858631134\n",
      "训练次数：3913, loss:1.3348430395126343\n",
      "训练次数：3914, loss:0.9465578198432922\n",
      "训练次数：3915, loss:1.388838291168213\n",
      "训练次数：3916, loss:1.1968873739242554\n",
      "训练次数：3917, loss:1.2991294860839844\n",
      "训练次数：3918, loss:1.3068395853042603\n",
      "训练次数：3919, loss:1.2053581476211548\n",
      "训练次数：3920, loss:1.3462581634521484\n",
      "训练次数：3921, loss:1.376396656036377\n",
      "训练次数：3922, loss:1.0010095834732056\n",
      "训练次数：3923, loss:1.5521984100341797\n",
      "训练次数：3924, loss:1.3766160011291504\n",
      "训练次数：3925, loss:1.1821242570877075\n",
      "训练次数：3926, loss:1.0783478021621704\n",
      "训练次数：3927, loss:1.1594326496124268\n",
      "训练次数：3928, loss:1.1423488855361938\n",
      "训练次数：3929, loss:0.9725058674812317\n",
      "训练次数：3930, loss:1.1787837743759155\n",
      "训练次数：3931, loss:1.4353687763214111\n",
      "训练次数：3932, loss:1.284788727760315\n",
      "训练次数：3933, loss:1.5247958898544312\n",
      "训练次数：3934, loss:1.4236481189727783\n",
      "训练次数：3935, loss:1.2083443403244019\n",
      "训练次数：3936, loss:1.5817089080810547\n",
      "训练次数：3937, loss:1.517667293548584\n",
      "训练次数：3938, loss:0.9749395251274109\n",
      "训练次数：3939, loss:0.9453781247138977\n",
      "训练次数：3940, loss:1.1316990852355957\n",
      "训练次数：3941, loss:1.4849531650543213\n",
      "训练次数：3942, loss:1.6980407238006592\n",
      "训练次数：3943, loss:1.1124640703201294\n",
      "训练次数：3944, loss:1.4642294645309448\n",
      "训练次数：3945, loss:1.1534031629562378\n",
      "训练次数：3946, loss:1.0667515993118286\n",
      "训练次数：3947, loss:1.1487627029418945\n",
      "训练次数：3948, loss:1.135485291481018\n",
      "训练次数：3949, loss:1.2472728490829468\n",
      "训练次数：3950, loss:1.3286744356155396\n",
      "训练次数：3951, loss:0.947230339050293\n",
      "训练次数：3952, loss:0.9729002714157104\n",
      "训练次数：3953, loss:1.3102731704711914\n",
      "训练次数：3954, loss:1.3186167478561401\n",
      "训练次数：3955, loss:1.226810336112976\n",
      "训练次数：3956, loss:1.1720037460327148\n",
      "训练次数：3957, loss:1.5851128101348877\n",
      "训练次数：3958, loss:1.380432367324829\n",
      "训练次数：3959, loss:1.348142385482788\n",
      "训练次数：3960, loss:1.2517211437225342\n",
      "训练次数：3961, loss:1.4969377517700195\n",
      "训练次数：3962, loss:1.4425914287567139\n",
      "训练次数：3963, loss:1.702049970626831\n",
      "训练次数：3964, loss:1.2651336193084717\n",
      "训练次数：3965, loss:1.2847847938537598\n",
      "训练次数：3966, loss:1.1230493783950806\n",
      "训练次数：3967, loss:1.6101646423339844\n",
      "训练次数：3968, loss:1.0591541528701782\n",
      "训练次数：3969, loss:1.001905918121338\n",
      "训练次数：3970, loss:1.3402769565582275\n",
      "训练次数：3971, loss:1.0932199954986572\n",
      "训练次数：3972, loss:1.3641436100006104\n",
      "训练次数：3973, loss:1.254381775856018\n",
      "训练次数：3974, loss:1.2178033590316772\n",
      "训练次数：3975, loss:1.227813720703125\n",
      "训练次数：3976, loss:1.4235973358154297\n",
      "训练次数：3977, loss:1.0427758693695068\n",
      "训练次数：3978, loss:1.1771371364593506\n",
      "训练次数：3979, loss:0.8917407989501953\n",
      "训练次数：3980, loss:1.1083167791366577\n",
      "训练次数：3981, loss:1.299832820892334\n",
      "训练次数：3982, loss:0.824731171131134\n",
      "训练次数：3983, loss:1.0816395282745361\n",
      "训练次数：3984, loss:1.2282462120056152\n",
      "训练次数：3985, loss:1.1963006258010864\n",
      "训练次数：3986, loss:1.315506935119629\n",
      "训练次数：3987, loss:1.2393707036972046\n",
      "训练次数：3988, loss:1.3975560665130615\n",
      "训练次数：3989, loss:1.1866626739501953\n",
      "训练次数：3990, loss:1.500704288482666\n",
      "训练次数：3991, loss:1.3891139030456543\n",
      "训练次数：3992, loss:1.2869313955307007\n",
      "训练次数：3993, loss:1.6775736808776855\n",
      "训练次数：3994, loss:0.8891431093215942\n",
      "训练次数：3995, loss:1.2555829286575317\n",
      "训练次数：3996, loss:1.1814674139022827\n",
      "训练次数：3997, loss:1.5276482105255127\n",
      "训练次数：3998, loss:1.0093193054199219\n",
      "训练次数：3999, loss:1.0285348892211914\n",
      "训练次数：4000, loss:1.0842325687408447\n",
      "训练次数：4001, loss:1.1401159763336182\n",
      "训练次数：4002, loss:1.5896204710006714\n",
      "训练次数：4003, loss:0.960176408290863\n",
      "训练次数：4004, loss:1.3178493976593018\n",
      "训练次数：4005, loss:1.3535478115081787\n",
      "训练次数：4006, loss:1.2997130155563354\n",
      "训练次数：4007, loss:1.0433199405670166\n",
      "训练次数：4008, loss:1.2782866954803467\n",
      "训练次数：4009, loss:1.3166184425354004\n",
      "训练次数：4010, loss:1.3191823959350586\n",
      "训练次数：4011, loss:1.0851346254348755\n",
      "训练次数：4012, loss:1.0883198976516724\n",
      "训练次数：4013, loss:0.9433411359786987\n",
      "训练次数：4014, loss:1.2214555740356445\n",
      "训练次数：4015, loss:0.9554893970489502\n",
      "训练次数：4016, loss:0.837960958480835\n",
      "训练次数：4017, loss:1.123282551765442\n",
      "训练次数：4018, loss:1.3915109634399414\n",
      "训练次数：4019, loss:1.0692726373672485\n",
      "训练次数：4020, loss:1.1268478631973267\n",
      "训练次数：4021, loss:1.1367791891098022\n",
      "训练次数：4022, loss:1.1767950057983398\n",
      "训练次数：4023, loss:1.1379722356796265\n",
      "训练次数：4024, loss:1.1377229690551758\n",
      "训练次数：4025, loss:1.2798848152160645\n",
      "训练次数：4026, loss:1.5241315364837646\n",
      "训练次数：4027, loss:1.2404292821884155\n",
      "训练次数：4028, loss:1.0409401655197144\n",
      "训练次数：4029, loss:1.298694133758545\n",
      "训练次数：4030, loss:1.3512568473815918\n",
      "训练次数：4031, loss:1.4625835418701172\n",
      "训练次数：4032, loss:1.5000227689743042\n",
      "训练次数：4033, loss:1.308667540550232\n",
      "训练次数：4034, loss:1.4411962032318115\n",
      "训练次数：4035, loss:1.4594539403915405\n",
      "训练次数：4036, loss:1.0593619346618652\n",
      "训练次数：4037, loss:1.1189481019973755\n",
      "训练次数：4038, loss:1.3563770055770874\n",
      "训练次数：4039, loss:1.410117268562317\n",
      "训练次数：4040, loss:1.5154920816421509\n",
      "训练次数：4041, loss:1.2103943824768066\n",
      "训练次数：4042, loss:1.3877263069152832\n",
      "训练次数：4043, loss:1.6097332239151\n",
      "训练次数：4044, loss:1.772952675819397\n",
      "训练次数：4045, loss:1.2283985614776611\n",
      "训练次数：4046, loss:1.260959506034851\n",
      "训练次数：4047, loss:1.4962960481643677\n",
      "训练次数：4048, loss:1.3220677375793457\n",
      "训练次数：4049, loss:1.0786879062652588\n",
      "训练次数：4050, loss:1.654780387878418\n",
      "训练次数：4051, loss:1.5000797510147095\n",
      "训练次数：4052, loss:1.1886391639709473\n",
      "训练次数：4053, loss:1.086355447769165\n",
      "训练次数：4054, loss:1.198479413986206\n",
      "训练次数：4055, loss:1.1789120435714722\n",
      "训练次数：4056, loss:1.2254407405853271\n",
      "训练次数：4057, loss:1.3108773231506348\n",
      "训练次数：4058, loss:1.6376756429672241\n",
      "训练次数：4059, loss:1.3642568588256836\n",
      "训练次数：4060, loss:1.246596097946167\n",
      "训练次数：4061, loss:1.3274205923080444\n",
      "训练次数：4062, loss:1.2365871667861938\n",
      "训练次数：4063, loss:1.1755115985870361\n",
      "训练次数：4064, loss:1.2248733043670654\n",
      "训练次数：4065, loss:1.3522465229034424\n",
      "训练次数：4066, loss:1.320827603340149\n",
      "训练次数：4067, loss:1.2690309286117554\n",
      "训练次数：4068, loss:1.1794236898422241\n",
      "训练次数：4069, loss:1.2323330640792847\n",
      "训练次数：4070, loss:1.447314977645874\n",
      "训练次数：4071, loss:1.3368375301361084\n",
      "训练次数：4072, loss:1.0212548971176147\n",
      "训练次数：4073, loss:1.42070472240448\n",
      "训练次数：4074, loss:1.590890884399414\n",
      "训练次数：4075, loss:0.9340197443962097\n",
      "训练次数：4076, loss:1.444786787033081\n",
      "训练次数：4077, loss:1.318857192993164\n",
      "训练次数：4078, loss:1.2437834739685059\n",
      "训练次数：4079, loss:0.9685385823249817\n",
      "训练次数：4080, loss:1.2463210821151733\n",
      "训练次数：4081, loss:1.4373955726623535\n",
      "训练次数：4082, loss:0.9124543070793152\n",
      "训练次数：4083, loss:1.2070984840393066\n",
      "训练次数：4084, loss:1.390210509300232\n",
      "训练次数：4085, loss:1.1712318658828735\n",
      "训练次数：4086, loss:1.1326980590820312\n",
      "训练次数：4087, loss:0.9652661085128784\n",
      "训练次数：4088, loss:1.3103090524673462\n",
      "训练次数：4089, loss:1.0407217741012573\n",
      "训练次数：4090, loss:1.227230191230774\n",
      "训练次数：4091, loss:1.0111925601959229\n",
      "训练次数：4092, loss:1.033007025718689\n",
      "训练次数：4093, loss:1.2216265201568604\n",
      "训练次数：4094, loss:1.2870303392410278\n",
      "训练次数：4095, loss:1.1215863227844238\n",
      "训练次数：4096, loss:1.3771913051605225\n",
      "训练次数：4097, loss:1.3873451948165894\n",
      "训练次数：4098, loss:1.2005895376205444\n",
      "训练次数：4099, loss:1.1048470735549927\n",
      "训练次数：4100, loss:1.2466412782669067\n",
      "训练次数：4101, loss:1.060394287109375\n",
      "训练次数：4102, loss:1.1878187656402588\n",
      "训练次数：4103, loss:1.525241494178772\n",
      "训练次数：4104, loss:1.18327796459198\n",
      "训练次数：4105, loss:1.2480576038360596\n",
      "训练次数：4106, loss:1.4822019338607788\n",
      "训练次数：4107, loss:1.2906744480133057\n",
      "训练次数：4108, loss:1.1463485956192017\n",
      "训练次数：4109, loss:1.0746829509735107\n",
      "训练次数：4110, loss:1.315274715423584\n",
      "训练次数：4111, loss:0.9338192939758301\n",
      "训练次数：4112, loss:1.5260820388793945\n",
      "训练次数：4113, loss:1.4143519401550293\n",
      "训练次数：4114, loss:1.421910285949707\n",
      "训练次数：4115, loss:1.1993260383605957\n",
      "训练次数：4116, loss:1.3984397649765015\n",
      "训练次数：4117, loss:1.1043903827667236\n",
      "训练次数：4118, loss:1.3435229063034058\n",
      "训练次数：4119, loss:1.0766979455947876\n",
      "训练次数：4120, loss:1.4341195821762085\n",
      "训练次数：4121, loss:0.9978197813034058\n",
      "训练次数：4122, loss:1.3467803001403809\n",
      "训练次数：4123, loss:0.9982001781463623\n",
      "训练次数：4124, loss:1.1255366802215576\n",
      "训练次数：4125, loss:1.3858956098556519\n",
      "训练次数：4126, loss:1.1087323427200317\n",
      "训练次数：4127, loss:1.2240424156188965\n",
      "训练次数：4128, loss:1.2062256336212158\n",
      "训练次数：4129, loss:1.1871302127838135\n",
      "训练次数：4130, loss:1.1145766973495483\n",
      "训练次数：4131, loss:1.144171118736267\n",
      "训练次数：4132, loss:1.37466299533844\n",
      "训练次数：4133, loss:1.1294788122177124\n",
      "训练次数：4134, loss:1.3169223070144653\n",
      "训练次数：4135, loss:1.2498974800109863\n",
      "训练次数：4136, loss:1.1035274267196655\n",
      "训练次数：4137, loss:1.065335988998413\n",
      "训练次数：4138, loss:1.0571799278259277\n",
      "训练次数：4139, loss:1.224013328552246\n",
      "训练次数：4140, loss:1.1241523027420044\n",
      "训练次数：4141, loss:1.0109467506408691\n",
      "训练次数：4142, loss:1.2402950525283813\n",
      "训练次数：4143, loss:1.367301106452942\n",
      "训练次数：4144, loss:1.5043038129806519\n",
      "训练次数：4145, loss:0.8419051170349121\n",
      "训练次数：4146, loss:0.9768961668014526\n",
      "训练次数：4147, loss:1.0270696878433228\n",
      "训练次数：4148, loss:1.17095947265625\n",
      "训练次数：4149, loss:1.0558472871780396\n",
      "训练次数：4150, loss:1.0697391033172607\n",
      "训练次数：4151, loss:1.5511094331741333\n",
      "训练次数：4152, loss:1.0992414951324463\n",
      "训练次数：4153, loss:1.509067177772522\n",
      "训练次数：4154, loss:1.7491079568862915\n",
      "训练次数：4155, loss:1.3992326259613037\n",
      "训练次数：4156, loss:1.3995345830917358\n",
      "训练次数：4157, loss:1.1760222911834717\n",
      "训练次数：4158, loss:1.6326688528060913\n",
      "训练次数：4159, loss:1.2912224531173706\n",
      "训练次数：4160, loss:1.0902314186096191\n",
      "训练次数：4161, loss:1.0590927600860596\n",
      "训练次数：4162, loss:1.3016749620437622\n",
      "训练次数：4163, loss:1.1838574409484863\n",
      "训练次数：4164, loss:1.413482666015625\n",
      "训练次数：4165, loss:1.3707929849624634\n",
      "训练次数：4166, loss:0.9074379205703735\n",
      "训练次数：4167, loss:1.3203132152557373\n",
      "训练次数：4168, loss:1.233772873878479\n",
      "训练次数：4169, loss:1.567428469657898\n",
      "训练次数：4170, loss:1.2204242944717407\n",
      "训练次数：4171, loss:1.2458027601242065\n",
      "训练次数：4172, loss:1.1921144723892212\n",
      "训练次数：4173, loss:1.732900857925415\n",
      "训练次数：4174, loss:1.2483022212982178\n",
      "训练次数：4175, loss:1.209049105644226\n",
      "训练次数：4176, loss:1.1399940252304077\n",
      "训练次数：4177, loss:1.4452431201934814\n",
      "训练次数：4178, loss:1.3465553522109985\n",
      "训练次数：4179, loss:1.052889347076416\n",
      "训练次数：4180, loss:1.1836836338043213\n",
      "训练次数：4181, loss:1.4062259197235107\n",
      "训练次数：4182, loss:1.08995521068573\n",
      "训练次数：4183, loss:1.2363927364349365\n",
      "训练次数：4184, loss:1.2581934928894043\n",
      "训练次数：4185, loss:1.0617897510528564\n",
      "训练次数：4186, loss:1.6177854537963867\n",
      "训练次数：4187, loss:1.7269008159637451\n",
      "训练次数：4188, loss:1.3433564901351929\n",
      "训练次数：4189, loss:1.6780589818954468\n",
      "训练次数：4190, loss:1.140805959701538\n",
      "训练次数：4191, loss:1.719699501991272\n",
      "训练次数：4192, loss:1.5868799686431885\n",
      "训练次数：4193, loss:1.4120813608169556\n",
      "训练次数：4194, loss:1.7546347379684448\n",
      "训练次数：4195, loss:1.3466440439224243\n",
      "训练次数：4196, loss:1.4753761291503906\n",
      "训练次数：4197, loss:1.349806785583496\n",
      "训练次数：4198, loss:1.1946521997451782\n",
      "训练次数：4199, loss:1.4280221462249756\n",
      "训练次数：4200, loss:1.3772814273834229\n",
      "训练次数：4201, loss:1.20558500289917\n",
      "训练次数：4202, loss:1.8971798419952393\n",
      "训练次数：4203, loss:1.2281436920166016\n",
      "训练次数：4204, loss:1.4434826374053955\n",
      "训练次数：4205, loss:1.6726070642471313\n",
      "训练次数：4206, loss:1.297776699066162\n",
      "训练次数：4207, loss:1.3886528015136719\n",
      "训练次数：4208, loss:1.2441083192825317\n",
      "训练次数：4209, loss:1.2728191614151\n",
      "训练次数：4210, loss:1.4003632068634033\n",
      "训练次数：4211, loss:1.329168677330017\n",
      "训练次数：4212, loss:1.5257989168167114\n",
      "训练次数：4213, loss:1.38632071018219\n",
      "训练次数：4214, loss:1.1063692569732666\n",
      "训练次数：4215, loss:1.460244059562683\n",
      "训练次数：4216, loss:1.4749436378479004\n",
      "训练次数：4217, loss:1.474726676940918\n",
      "训练次数：4218, loss:1.4766055345535278\n",
      "训练次数：4219, loss:1.6212865114212036\n",
      "训练次数：4220, loss:1.4343796968460083\n",
      "训练次数：4221, loss:1.7269209623336792\n",
      "训练次数：4222, loss:1.449154019355774\n",
      "训练次数：4223, loss:1.4767130613327026\n",
      "训练次数：4224, loss:1.6328920125961304\n",
      "训练次数：4225, loss:1.2725964784622192\n",
      "训练次数：4226, loss:1.645293116569519\n",
      "训练次数：4227, loss:1.3844414949417114\n",
      "训练次数：4228, loss:1.5052818059921265\n",
      "训练次数：4229, loss:1.2996033430099487\n",
      "训练次数：4230, loss:1.394248127937317\n",
      "训练次数：4231, loss:1.7700996398925781\n",
      "训练次数：4232, loss:1.324878215789795\n",
      "训练次数：4233, loss:1.4989687204360962\n",
      "训练次数：4234, loss:1.4184141159057617\n",
      "训练次数：4235, loss:1.2191294431686401\n",
      "训练次数：4236, loss:1.2490344047546387\n",
      "训练次数：4237, loss:1.3368721008300781\n",
      "训练次数：4238, loss:1.4447693824768066\n",
      "训练次数：4239, loss:1.1704612970352173\n",
      "训练次数：4240, loss:1.2958300113677979\n",
      "训练次数：4241, loss:1.388640284538269\n",
      "训练次数：4242, loss:1.1182924509048462\n",
      "训练次数：4243, loss:1.4730952978134155\n",
      "训练次数：4244, loss:1.3162487745285034\n",
      "训练次数：4245, loss:1.4544681310653687\n",
      "训练次数：4246, loss:1.9519860744476318\n",
      "训练次数：4247, loss:1.5720274448394775\n",
      "训练次数：4248, loss:1.6677422523498535\n",
      "训练次数：4249, loss:1.3592743873596191\n",
      "训练次数：4250, loss:1.3837687969207764\n",
      "训练次数：4251, loss:1.5053750276565552\n",
      "训练次数：4252, loss:1.7308731079101562\n",
      "训练次数：4253, loss:1.3326112031936646\n",
      "训练次数：4254, loss:1.347097396850586\n",
      "训练次数：4255, loss:1.2691495418548584\n",
      "训练次数：4256, loss:1.17850661277771\n",
      "训练次数：4257, loss:1.2598124742507935\n",
      "训练次数：4258, loss:1.5383127927780151\n",
      "训练次数：4259, loss:1.1031008958816528\n",
      "训练次数：4260, loss:1.5674500465393066\n",
      "训练次数：4261, loss:1.264100193977356\n",
      "训练次数：4262, loss:1.3995661735534668\n",
      "训练次数：4263, loss:1.3889271020889282\n",
      "训练次数：4264, loss:1.2645621299743652\n",
      "训练次数：4265, loss:1.0829460620880127\n",
      "训练次数：4266, loss:1.7676348686218262\n",
      "训练次数：4267, loss:1.4999315738677979\n",
      "训练次数：4268, loss:1.4595530033111572\n",
      "训练次数：4269, loss:1.5701929330825806\n",
      "训练次数：4270, loss:1.290798306465149\n",
      "训练次数：4271, loss:1.5308443307876587\n",
      "训练次数：4272, loss:0.9205435514450073\n",
      "训练次数：4273, loss:1.4461402893066406\n",
      "训练次数：4274, loss:1.22574782371521\n",
      "训练次数：4275, loss:1.2372162342071533\n",
      "训练次数：4276, loss:1.2296870946884155\n",
      "训练次数：4277, loss:1.4633926153182983\n",
      "训练次数：4278, loss:1.4475367069244385\n",
      "训练次数：4279, loss:0.9788662195205688\n",
      "训练次数：4280, loss:1.5482181310653687\n",
      "训练次数：4281, loss:1.6058285236358643\n",
      "训练次数：4282, loss:1.6759189367294312\n",
      "训练次数：4283, loss:1.2571969032287598\n",
      "训练次数：4284, loss:1.6608952283859253\n",
      "训练次数：4285, loss:1.5482277870178223\n",
      "训练次数：4286, loss:1.1344643831253052\n",
      "训练次数：4287, loss:1.0193239450454712\n",
      "训练次数：4288, loss:1.405348777770996\n",
      "训练次数：4289, loss:1.5279027223587036\n",
      "训练次数：4290, loss:1.5094809532165527\n",
      "训练次数：4291, loss:1.4174963235855103\n",
      "训练次数：4292, loss:1.5122137069702148\n",
      "训练次数：4293, loss:1.3400558233261108\n",
      "训练次数：4294, loss:1.2571154832839966\n",
      "训练次数：4295, loss:1.4330096244812012\n",
      "训练次数：4296, loss:1.6124260425567627\n",
      "训练次数：4297, loss:1.5687997341156006\n",
      "训练次数：4298, loss:1.7662250995635986\n",
      "训练次数：4299, loss:1.4962186813354492\n",
      "训练次数：4300, loss:1.2854454517364502\n",
      "训练次数：4301, loss:1.1692668199539185\n",
      "训练次数：4302, loss:0.8611534833908081\n",
      "训练次数：4303, loss:1.1684199571609497\n",
      "训练次数：4304, loss:1.3884453773498535\n",
      "训练次数：4305, loss:1.5601794719696045\n",
      "训练次数：4306, loss:1.367440938949585\n",
      "训练次数：4307, loss:1.416210412979126\n",
      "训练次数：4308, loss:1.1974190473556519\n",
      "训练次数：4309, loss:1.3303242921829224\n",
      "训练次数：4310, loss:1.0956523418426514\n",
      "训练次数：4311, loss:1.4170418977737427\n",
      "训练次数：4312, loss:1.8367886543273926\n",
      "训练次数：4313, loss:1.1793577671051025\n",
      "训练次数：4314, loss:1.048542857170105\n",
      "训练次数：4315, loss:1.1555495262145996\n",
      "训练次数：4316, loss:1.4778813123703003\n",
      "训练次数：4317, loss:1.2233291864395142\n",
      "训练次数：4318, loss:1.1608214378356934\n",
      "训练次数：4319, loss:1.3150055408477783\n",
      "训练次数：4320, loss:1.3248789310455322\n",
      "训练次数：4321, loss:1.4683136940002441\n",
      "训练次数：4322, loss:1.4582833051681519\n",
      "训练次数：4323, loss:1.3554613590240479\n",
      "训练次数：4324, loss:1.2309119701385498\n",
      "训练次数：4325, loss:1.6036009788513184\n",
      "训练次数：4326, loss:1.2147539854049683\n",
      "训练次数：4327, loss:1.3649146556854248\n",
      "训练次数：4328, loss:1.5161948204040527\n",
      "训练次数：4329, loss:1.621289610862732\n",
      "训练次数：4330, loss:1.3851877450942993\n",
      "训练次数：4331, loss:1.443056583404541\n",
      "训练次数：4332, loss:1.673705816268921\n",
      "训练次数：4333, loss:1.4735714197158813\n",
      "训练次数：4334, loss:1.4560232162475586\n",
      "训练次数：4335, loss:1.4476290941238403\n",
      "训练次数：4336, loss:1.4821523427963257\n",
      "训练次数：4337, loss:1.4917192459106445\n",
      "训练次数：4338, loss:1.156048059463501\n",
      "训练次数：4339, loss:1.3747963905334473\n",
      "训练次数：4340, loss:1.1399240493774414\n",
      "训练次数：4341, loss:1.3830631971359253\n",
      "训练次数：4342, loss:0.9324350357055664\n",
      "训练次数：4343, loss:1.2739372253417969\n",
      "训练次数：4344, loss:1.1215572357177734\n",
      "训练次数：4345, loss:1.335877537727356\n",
      "训练次数：4346, loss:1.3010926246643066\n",
      "训练次数：4347, loss:1.076829433441162\n",
      "训练次数：4348, loss:1.1088545322418213\n",
      "训练次数：4349, loss:1.3095279932022095\n",
      "训练次数：4350, loss:1.0619093179702759\n",
      "训练次数：4351, loss:1.1449705362319946\n",
      "训练次数：4352, loss:0.9421675801277161\n",
      "训练次数：4353, loss:1.4934896230697632\n",
      "训练次数：4354, loss:1.2351936101913452\n",
      "训练次数：4355, loss:1.1139535903930664\n",
      "训练次数：4356, loss:1.3092759847640991\n",
      "训练次数：4357, loss:1.2688820362091064\n",
      "训练次数：4358, loss:1.3406819105148315\n",
      "训练次数：4359, loss:1.2339662313461304\n",
      "训练次数：4360, loss:1.6780576705932617\n",
      "训练次数：4361, loss:1.0687001943588257\n",
      "训练次数：4362, loss:1.1091742515563965\n",
      "训练次数：4363, loss:1.1308625936508179\n",
      "训练次数：4364, loss:1.166937232017517\n",
      "训练次数：4365, loss:1.4282211065292358\n",
      "训练次数：4366, loss:1.2214986085891724\n",
      "训练次数：4367, loss:1.4993647336959839\n",
      "训练次数：4368, loss:1.2105824947357178\n",
      "训练次数：4369, loss:0.9549741148948669\n",
      "训练次数：4370, loss:1.089192271232605\n",
      "训练次数：4371, loss:1.0866249799728394\n",
      "训练次数：4372, loss:1.043472170829773\n",
      "训练次数：4373, loss:1.3794960975646973\n",
      "训练次数：4374, loss:1.2165989875793457\n",
      "训练次数：4375, loss:1.0797357559204102\n",
      "训练次数：4376, loss:1.20334792137146\n",
      "训练次数：4377, loss:1.3663475513458252\n",
      "训练次数：4378, loss:1.0183123350143433\n",
      "训练次数：4379, loss:0.9388089179992676\n",
      "训练次数：4380, loss:1.4235435724258423\n",
      "训练次数：4381, loss:0.9663206934928894\n",
      "训练次数：4382, loss:1.8099409341812134\n",
      "训练次数：4383, loss:2.7204430103302\n",
      "训练次数：4384, loss:1.1195285320281982\n",
      "训练次数：4385, loss:1.221139669418335\n",
      "训练次数：4386, loss:1.6554397344589233\n",
      "训练次数：4387, loss:1.6718415021896362\n",
      "训练次数：4388, loss:3.757185697555542\n",
      "训练次数：4389, loss:6.082186222076416\n",
      "训练次数：4390, loss:2.6598243713378906\n",
      "训练次数：4391, loss:9.970015525817871\n",
      "训练次数：4392, loss:37.0721435546875\n",
      "训练次数：4393, loss:273.515625\n",
      "训练次数：4394, loss:61.51654052734375\n",
      "训练次数：4395, loss:3669.6904296875\n",
      "训练次数：4396, loss:26157.009765625\n",
      "训练次数：4397, loss:13308.427734375\n",
      "训练次数：4398, loss:18490.109375\n",
      "训练次数：4399, loss:170360.3125\n",
      "训练次数：4400, loss:89908.4375\n",
      "训练次数：4401, loss:21918.423828125\n",
      "训练次数：4402, loss:82131.0\n",
      "训练次数：4403, loss:100616.640625\n",
      "训练次数：4404, loss:100962.453125\n",
      "训练次数：4405, loss:60522.42578125\n",
      "训练次数：4406, loss:151655.515625\n",
      "训练次数：4407, loss:224861.03125\n",
      "训练次数：4408, loss:127305.2578125\n",
      "训练次数：4409, loss:465751.4375\n",
      "训练次数：4410, loss:98254.1953125\n",
      "训练次数：4411, loss:84487.109375\n",
      "训练次数：4412, loss:154380.390625\n",
      "训练次数：4413, loss:93495.1171875\n",
      "训练次数：4414, loss:97388.8515625\n",
      "训练次数：4415, loss:84476.7734375\n",
      "训练次数：4416, loss:55049.9453125\n",
      "训练次数：4417, loss:71960.2265625\n",
      "训练次数：4418, loss:44401.50390625\n",
      "训练次数：4419, loss:24908.478515625\n",
      "训练次数：4420, loss:20297.7578125\n",
      "训练次数：4421, loss:138817.484375\n",
      "训练次数：4422, loss:83999.9765625\n",
      "训练次数：4423, loss:34344.203125\n",
      "训练次数：4424, loss:43517.02734375\n",
      "训练次数：4425, loss:48329.8984375\n",
      "训练次数：4426, loss:30219.830078125\n",
      "训练次数：4427, loss:98434.0859375\n",
      "训练次数：4428, loss:87384.09375\n",
      "训练次数：4429, loss:68430.8515625\n",
      "训练次数：4430, loss:31262.826171875\n",
      "训练次数：4431, loss:74812.34375\n",
      "训练次数：4432, loss:42149.20703125\n",
      "训练次数：4433, loss:46386.44921875\n",
      "训练次数：4434, loss:13073.3408203125\n",
      "训练次数：4435, loss:42429.88671875\n",
      "训练次数：4436, loss:20304.212890625\n",
      "训练次数：4437, loss:62595.58203125\n",
      "训练次数：4438, loss:51012.296875\n",
      "训练次数：4439, loss:15445.3251953125\n",
      "训练次数：4440, loss:48159.6328125\n",
      "训练次数：4441, loss:26784.44921875\n",
      "训练次数：4442, loss:19377.185546875\n",
      "训练次数：4443, loss:36782.0546875\n",
      "训练次数：4444, loss:21587.240234375\n",
      "训练次数：4445, loss:22569.12109375\n",
      "训练次数：4446, loss:21793.111328125\n",
      "训练次数：4447, loss:9307.57421875\n",
      "训练次数：4448, loss:10886.98046875\n",
      "训练次数：4449, loss:8988.2939453125\n",
      "训练次数：4450, loss:19931.794921875\n",
      "训练次数：4451, loss:5765.56201171875\n",
      "训练次数：4452, loss:17167.716796875\n",
      "训练次数：4453, loss:10832.615234375\n",
      "训练次数：4454, loss:14257.6962890625\n",
      "训练次数：4455, loss:15840.0361328125\n",
      "训练次数：4456, loss:11026.6474609375\n",
      "训练次数：4457, loss:8768.84765625\n",
      "训练次数：4458, loss:11126.1669921875\n",
      "训练次数：4459, loss:6927.44091796875\n",
      "训练次数：4460, loss:5195.5888671875\n",
      "训练次数：4461, loss:9275.5283203125\n",
      "训练次数：4462, loss:10118.6572265625\n",
      "训练次数：4463, loss:5482.37060546875\n",
      "训练次数：4464, loss:2643.476318359375\n",
      "训练次数：4465, loss:2830.26220703125\n",
      "训练次数：4466, loss:4058.347412109375\n",
      "训练次数：4467, loss:5819.07373046875\n",
      "训练次数：4468, loss:4569.21728515625\n",
      "训练次数：4469, loss:5197.66552734375\n",
      "训练次数：4470, loss:3973.79833984375\n",
      "训练次数：4471, loss:4476.83984375\n",
      "训练次数：4472, loss:3746.872802734375\n",
      "训练次数：4473, loss:2148.643310546875\n",
      "训练次数：4474, loss:1125.7342529296875\n",
      "训练次数：4475, loss:1926.5205078125\n",
      "训练次数：4476, loss:2371.201904296875\n",
      "训练次数：4477, loss:2295.995361328125\n",
      "训练次数：4478, loss:1472.8192138671875\n",
      "训练次数：4479, loss:1160.3883056640625\n",
      "训练次数：4480, loss:511.4823303222656\n",
      "训练次数：4481, loss:1591.113037109375\n",
      "训练次数：4482, loss:1136.39599609375\n",
      "训练次数：4483, loss:798.0410766601562\n",
      "训练次数：4484, loss:1007.6951904296875\n",
      "训练次数：4485, loss:1736.4757080078125\n",
      "训练次数：4486, loss:690.8815307617188\n",
      "训练次数：4487, loss:1290.544189453125\n",
      "训练次数：4488, loss:1040.51708984375\n",
      "训练次数：4489, loss:1276.3687744140625\n",
      "训练次数：4490, loss:910.896240234375\n",
      "训练次数：4491, loss:817.4041748046875\n",
      "训练次数：4492, loss:515.0826416015625\n",
      "训练次数：4493, loss:730.9299926757812\n",
      "训练次数：4494, loss:861.8511962890625\n",
      "训练次数：4495, loss:968.5386352539062\n",
      "训练次数：4496, loss:1044.1334228515625\n",
      "训练次数：4497, loss:669.7249755859375\n",
      "训练次数：4498, loss:696.9674072265625\n",
      "训练次数：4499, loss:590.8475341796875\n",
      "训练次数：4500, loss:343.04705810546875\n",
      "训练次数：4501, loss:588.4808349609375\n",
      "训练次数：4502, loss:518.7193603515625\n",
      "训练次数：4503, loss:524.7986450195312\n",
      "训练次数：4504, loss:390.9136962890625\n",
      "训练次数：4505, loss:342.88067626953125\n",
      "训练次数：4506, loss:400.18310546875\n",
      "训练次数：4507, loss:271.50140380859375\n",
      "训练次数：4508, loss:274.92840576171875\n",
      "训练次数：4509, loss:210.10186767578125\n",
      "训练次数：4510, loss:300.5540771484375\n",
      "训练次数：4511, loss:347.1207580566406\n",
      "训练次数：4512, loss:252.74282836914062\n",
      "训练次数：4513, loss:185.93565368652344\n",
      "训练次数：4514, loss:162.283935546875\n",
      "训练次数：4515, loss:234.0386962890625\n",
      "训练次数：4516, loss:212.45260620117188\n",
      "训练次数：4517, loss:214.21339416503906\n",
      "训练次数：4518, loss:160.75341796875\n",
      "训练次数：4519, loss:203.68092346191406\n",
      "训练次数：4520, loss:149.81982421875\n",
      "训练次数：4521, loss:157.09226989746094\n",
      "训练次数：4522, loss:174.38819885253906\n",
      "训练次数：4523, loss:158.8928985595703\n",
      "训练次数：4524, loss:159.73780822753906\n",
      "训练次数：4525, loss:164.7685546875\n",
      "训练次数：4526, loss:188.7845458984375\n",
      "训练次数：4527, loss:128.98873901367188\n",
      "训练次数：4528, loss:135.66249084472656\n",
      "训练次数：4529, loss:110.98922729492188\n",
      "训练次数：4530, loss:176.99871826171875\n",
      "训练次数：4531, loss:165.2374267578125\n",
      "训练次数：4532, loss:144.87786865234375\n",
      "训练次数：4533, loss:112.40763092041016\n",
      "训练次数：4534, loss:137.72398376464844\n",
      "训练次数：4535, loss:134.3448944091797\n",
      "训练次数：4536, loss:106.38731384277344\n",
      "训练次数：4537, loss:115.66791534423828\n",
      "训练次数：4538, loss:134.71922302246094\n",
      "训练次数：4539, loss:140.17259216308594\n",
      "训练次数：4540, loss:117.55525207519531\n",
      "训练次数：4541, loss:108.42084503173828\n",
      "训练次数：4542, loss:131.52755737304688\n",
      "训练次数：4543, loss:92.8301010131836\n",
      "训练次数：4544, loss:86.29988861083984\n",
      "训练次数：4545, loss:111.21078491210938\n",
      "训练次数：4546, loss:93.83241271972656\n",
      "训练次数：4547, loss:79.59976196289062\n",
      "训练次数：4548, loss:79.17694091796875\n",
      "训练次数：4549, loss:103.10771179199219\n",
      "训练次数：4550, loss:101.21858978271484\n",
      "训练次数：4551, loss:113.10270690917969\n",
      "训练次数：4552, loss:99.04981231689453\n",
      "训练次数：4553, loss:105.74091339111328\n",
      "训练次数：4554, loss:94.82782745361328\n",
      "训练次数：4555, loss:98.00250244140625\n",
      "训练次数：4556, loss:94.00431823730469\n",
      "训练次数：4557, loss:94.5711669921875\n",
      "训练次数：4558, loss:71.14459228515625\n",
      "训练次数：4559, loss:161.64291381835938\n",
      "训练次数：4560, loss:65.71956634521484\n",
      "训练次数：4561, loss:102.24625396728516\n",
      "训练次数：4562, loss:67.9365234375\n",
      "训练次数：4563, loss:71.28983306884766\n",
      "训练次数：4564, loss:81.1624755859375\n",
      "训练次数：4565, loss:111.30357360839844\n",
      "训练次数：4566, loss:87.13705444335938\n",
      "训练次数：4567, loss:93.12834167480469\n",
      "训练次数：4568, loss:88.70345306396484\n",
      "训练次数：4569, loss:85.72626495361328\n",
      "训练次数：4570, loss:84.50508880615234\n",
      "训练次数：4571, loss:65.150634765625\n",
      "训练次数：4572, loss:77.80876159667969\n",
      "训练次数：4573, loss:53.14564514160156\n",
      "训练次数：4574, loss:61.36699295043945\n",
      "训练次数：4575, loss:74.1336669921875\n",
      "训练次数：4576, loss:49.54804992675781\n",
      "训练次数：4577, loss:53.310298919677734\n",
      "训练次数：4578, loss:66.4508056640625\n",
      "训练次数：4579, loss:55.60913848876953\n",
      "训练次数：4580, loss:64.19221496582031\n",
      "训练次数：4581, loss:59.72713851928711\n",
      "训练次数：4582, loss:69.23381042480469\n",
      "训练次数：4583, loss:55.67798614501953\n",
      "训练次数：4584, loss:52.28094482421875\n",
      "训练次数：4585, loss:65.09828186035156\n",
      "训练次数：4586, loss:55.788455963134766\n",
      "训练次数：4587, loss:107.77706909179688\n",
      "训练次数：4588, loss:46.17790603637695\n",
      "训练次数：4589, loss:72.24671936035156\n",
      "训练次数：4590, loss:56.592037200927734\n",
      "训练次数：4591, loss:70.81776428222656\n",
      "训练次数：4592, loss:43.59516906738281\n",
      "训练次数：4593, loss:50.74492645263672\n",
      "训练次数：4594, loss:54.10597610473633\n",
      "训练次数：4595, loss:74.14667510986328\n",
      "训练次数：4596, loss:57.883544921875\n",
      "训练次数：4597, loss:44.296817779541016\n",
      "训练次数：4598, loss:51.86882400512695\n",
      "训练次数：4599, loss:45.85978317260742\n",
      "训练次数：4600, loss:67.6495361328125\n",
      "训练次数：4601, loss:48.36167526245117\n",
      "训练次数：4602, loss:52.9522705078125\n",
      "训练次数：4603, loss:63.05139923095703\n",
      "训练次数：4604, loss:53.17018508911133\n",
      "训练次数：4605, loss:67.82024383544922\n",
      "训练次数：4606, loss:40.144798278808594\n",
      "训练次数：4607, loss:58.50816345214844\n",
      "训练次数：4608, loss:43.82462692260742\n",
      "训练次数：4609, loss:35.814170837402344\n",
      "训练次数：4610, loss:44.073944091796875\n",
      "训练次数：4611, loss:56.83134841918945\n",
      "训练次数：4612, loss:62.7258415222168\n",
      "训练次数：4613, loss:36.269004821777344\n",
      "训练次数：4614, loss:41.93216323852539\n",
      "训练次数：4615, loss:55.726715087890625\n",
      "训练次数：4616, loss:42.51515197753906\n",
      "训练次数：4617, loss:47.59050750732422\n",
      "训练次数：4618, loss:38.36134719848633\n",
      "训练次数：4619, loss:44.150115966796875\n",
      "训练次数：4620, loss:39.36015319824219\n",
      "训练次数：4621, loss:49.40788269042969\n",
      "训练次数：4622, loss:59.5400390625\n",
      "训练次数：4623, loss:47.22400665283203\n",
      "训练次数：4624, loss:36.966835021972656\n",
      "训练次数：4625, loss:37.524635314941406\n",
      "训练次数：4626, loss:48.2165641784668\n",
      "训练次数：4627, loss:51.72810745239258\n",
      "训练次数：4628, loss:58.93608093261719\n",
      "训练次数：4629, loss:41.60826110839844\n",
      "训练次数：4630, loss:43.08058166503906\n",
      "训练次数：4631, loss:35.68149185180664\n",
      "训练次数：4632, loss:40.40785217285156\n",
      "训练次数：4633, loss:39.9533576965332\n",
      "训练次数：4634, loss:34.58192443847656\n",
      "训练次数：4635, loss:42.6477165222168\n",
      "训练次数：4636, loss:41.228355407714844\n",
      "训练次数：4637, loss:39.3012580871582\n",
      "训练次数：4638, loss:70.5271987915039\n",
      "训练次数：4639, loss:36.01382064819336\n",
      "训练次数：4640, loss:69.1619873046875\n",
      "训练次数：4641, loss:43.40380096435547\n",
      "训练次数：4642, loss:70.46253204345703\n",
      "训练次数：4643, loss:47.82787322998047\n",
      "训练次数：4644, loss:63.05331039428711\n",
      "训练次数：4645, loss:57.20928955078125\n",
      "训练次数：4646, loss:51.10430145263672\n",
      "训练次数：4647, loss:53.972679138183594\n",
      "训练次数：4648, loss:37.41177749633789\n",
      "训练次数：4649, loss:39.53500747680664\n",
      "训练次数：4650, loss:45.35011291503906\n",
      "训练次数：4651, loss:43.73426055908203\n",
      "训练次数：4652, loss:31.514421463012695\n",
      "训练次数：4653, loss:43.5743293762207\n",
      "训练次数：4654, loss:43.17845916748047\n",
      "训练次数：4655, loss:34.4281005859375\n",
      "训练次数：4656, loss:40.63401412963867\n",
      "训练次数：4657, loss:35.88197326660156\n",
      "训练次数：4658, loss:38.35186767578125\n",
      "训练次数：4659, loss:45.33576965332031\n",
      "训练次数：4660, loss:33.389686584472656\n",
      "训练次数：4661, loss:46.88593673706055\n",
      "训练次数：4662, loss:41.77412414550781\n",
      "训练次数：4663, loss:25.829940795898438\n",
      "训练次数：4664, loss:29.968542098999023\n",
      "训练次数：4665, loss:39.809085845947266\n",
      "训练次数：4666, loss:35.22391128540039\n",
      "训练次数：4667, loss:41.293304443359375\n",
      "训练次数：4668, loss:29.94892692565918\n",
      "训练次数：4669, loss:32.50331115722656\n",
      "训练次数：4670, loss:34.68584442138672\n",
      "训练次数：4671, loss:43.45082092285156\n",
      "训练次数：4672, loss:34.90855407714844\n",
      "训练次数：4673, loss:27.431974411010742\n",
      "训练次数：4674, loss:38.41194534301758\n",
      "训练次数：4675, loss:54.47184371948242\n",
      "训练次数：4676, loss:43.907772064208984\n",
      "训练次数：4677, loss:40.86911392211914\n",
      "训练次数：4678, loss:32.82167053222656\n",
      "训练次数：4679, loss:22.83682632446289\n",
      "训练次数：4680, loss:32.25965881347656\n",
      "训练次数：4681, loss:33.98686218261719\n",
      "训练次数：4682, loss:28.257675170898438\n",
      "训练次数：4683, loss:32.3381462097168\n",
      "训练次数：4684, loss:30.55058479309082\n",
      "训练次数：4685, loss:27.056365966796875\n",
      "训练次数：4686, loss:32.671363830566406\n",
      "训练次数：4687, loss:28.051769256591797\n",
      "训练次数：4688, loss:27.88780975341797\n",
      "训练次数：4689, loss:41.598392486572266\n",
      "训练次数：4690, loss:37.924560546875\n",
      "训练次数：4691, loss:38.59920120239258\n",
      "训练次数：4692, loss:27.146175384521484\n",
      "----------第6轮训练开始----------\n",
      "训练次数：4693, loss:34.27985382080078\n",
      "训练次数：4694, loss:43.724281311035156\n",
      "训练次数：4695, loss:40.425228118896484\n",
      "训练次数：4696, loss:27.232471466064453\n",
      "训练次数：4697, loss:39.55486297607422\n",
      "训练次数：4698, loss:36.73588562011719\n",
      "训练次数：4699, loss:34.50481033325195\n",
      "训练次数：4700, loss:36.856178283691406\n",
      "训练次数：4701, loss:36.6529541015625\n",
      "训练次数：4702, loss:35.87344741821289\n",
      "训练次数：4703, loss:39.3636589050293\n",
      "训练次数：4704, loss:35.527801513671875\n",
      "训练次数：4705, loss:49.22569274902344\n",
      "训练次数：4706, loss:30.18299674987793\n",
      "训练次数：4707, loss:28.526016235351562\n",
      "训练次数：4708, loss:29.663572311401367\n",
      "训练次数：4709, loss:31.389753341674805\n",
      "训练次数：4710, loss:27.97079086303711\n",
      "训练次数：4711, loss:29.03154945373535\n",
      "训练次数：4712, loss:30.954736709594727\n",
      "训练次数：4713, loss:28.842243194580078\n",
      "训练次数：4714, loss:42.30146026611328\n",
      "训练次数：4715, loss:34.67720031738281\n",
      "训练次数：4716, loss:37.64152908325195\n",
      "训练次数：4717, loss:31.05714988708496\n",
      "训练次数：4718, loss:39.80915832519531\n",
      "训练次数：4719, loss:36.91335678100586\n",
      "训练次数：4720, loss:26.734445571899414\n",
      "训练次数：4721, loss:34.94070053100586\n",
      "训练次数：4722, loss:35.9409294128418\n",
      "训练次数：4723, loss:34.94382858276367\n",
      "训练次数：4724, loss:34.5838623046875\n",
      "训练次数：4725, loss:56.034828186035156\n",
      "训练次数：4726, loss:22.062288284301758\n",
      "训练次数：4727, loss:42.380096435546875\n",
      "训练次数：4728, loss:29.764245986938477\n",
      "训练次数：4729, loss:44.397247314453125\n",
      "训练次数：4730, loss:29.458633422851562\n",
      "训练次数：4731, loss:44.350914001464844\n",
      "训练次数：4732, loss:38.76216506958008\n",
      "训练次数：4733, loss:37.73394775390625\n",
      "训练次数：4734, loss:34.57286071777344\n",
      "训练次数：4735, loss:30.603300094604492\n",
      "训练次数：4736, loss:38.387481689453125\n",
      "训练次数：4737, loss:25.880455017089844\n",
      "训练次数：4738, loss:46.874473571777344\n",
      "训练次数：4739, loss:31.481992721557617\n",
      "训练次数：4740, loss:42.22542953491211\n",
      "训练次数：4741, loss:50.90892791748047\n",
      "训练次数：4742, loss:41.82346725463867\n",
      "训练次数：4743, loss:37.909507751464844\n",
      "训练次数：4744, loss:35.62607955932617\n",
      "训练次数：4745, loss:28.321048736572266\n",
      "训练次数：4746, loss:32.82671356201172\n",
      "训练次数：4747, loss:41.53220748901367\n",
      "训练次数：4748, loss:35.43068313598633\n",
      "训练次数：4749, loss:38.75187683105469\n",
      "训练次数：4750, loss:48.1341552734375\n",
      "训练次数：4751, loss:40.564552307128906\n",
      "训练次数：4752, loss:27.662418365478516\n",
      "训练次数：4753, loss:43.01739501953125\n",
      "训练次数：4754, loss:32.27936553955078\n",
      "训练次数：4755, loss:32.738990783691406\n",
      "训练次数：4756, loss:28.22430419921875\n",
      "训练次数：4757, loss:27.53601837158203\n",
      "训练次数：4758, loss:25.46714973449707\n",
      "训练次数：4759, loss:22.84052085876465\n",
      "训练次数：4760, loss:27.461647033691406\n",
      "训练次数：4761, loss:34.914764404296875\n",
      "训练次数：4762, loss:21.04249382019043\n",
      "训练次数：4763, loss:31.777469635009766\n",
      "训练次数：4764, loss:26.415634155273438\n",
      "训练次数：4765, loss:22.774728775024414\n",
      "训练次数：4766, loss:28.74003791809082\n",
      "训练次数：4767, loss:22.49524688720703\n",
      "训练次数：4768, loss:36.055171966552734\n",
      "训练次数：4769, loss:26.534290313720703\n",
      "训练次数：4770, loss:25.484514236450195\n",
      "训练次数：4771, loss:25.32565689086914\n",
      "训练次数：4772, loss:37.78044128417969\n",
      "训练次数：4773, loss:30.092060089111328\n",
      "训练次数：4774, loss:33.93350601196289\n",
      "训练次数：4775, loss:23.77981948852539\n",
      "训练次数：4776, loss:28.899246215820312\n",
      "训练次数：4777, loss:33.108089447021484\n",
      "训练次数：4778, loss:20.249448776245117\n",
      "训练次数：4779, loss:26.726072311401367\n",
      "训练次数：4780, loss:21.839853286743164\n",
      "训练次数：4781, loss:27.277267456054688\n",
      "训练次数：4782, loss:32.06364059448242\n",
      "训练次数：4783, loss:31.97764015197754\n",
      "训练次数：4784, loss:25.80257797241211\n",
      "训练次数：4785, loss:26.32537078857422\n",
      "训练次数：4786, loss:25.317293167114258\n",
      "训练次数：4787, loss:26.904054641723633\n",
      "训练次数：4788, loss:25.614721298217773\n",
      "训练次数：4789, loss:27.491973876953125\n",
      "训练次数：4790, loss:26.922706604003906\n",
      "训练次数：4791, loss:22.994935989379883\n",
      "训练次数：4792, loss:25.572378158569336\n",
      "训练次数：4793, loss:34.659202575683594\n",
      "训练次数：4794, loss:29.672565460205078\n",
      "训练次数：4795, loss:30.97075843811035\n",
      "训练次数：4796, loss:29.713834762573242\n",
      "训练次数：4797, loss:26.926137924194336\n",
      "训练次数：4798, loss:25.678293228149414\n",
      "训练次数：4799, loss:30.416200637817383\n",
      "训练次数：4800, loss:29.868398666381836\n",
      "训练次数：4801, loss:31.316532135009766\n",
      "训练次数：4802, loss:24.159320831298828\n",
      "训练次数：4803, loss:28.295034408569336\n",
      "训练次数：4804, loss:36.045997619628906\n",
      "训练次数：4805, loss:32.54838180541992\n",
      "训练次数：4806, loss:32.6715087890625\n",
      "训练次数：4807, loss:29.679906845092773\n",
      "训练次数：4808, loss:31.912839889526367\n",
      "训练次数：4809, loss:29.208744049072266\n",
      "训练次数：4810, loss:33.32982635498047\n",
      "训练次数：4811, loss:29.808021545410156\n",
      "训练次数：4812, loss:35.82596969604492\n",
      "训练次数：4813, loss:29.179624557495117\n",
      "训练次数：4814, loss:23.313232421875\n",
      "训练次数：4815, loss:22.067646026611328\n",
      "训练次数：4816, loss:23.130558013916016\n",
      "训练次数：4817, loss:24.320247650146484\n",
      "训练次数：4818, loss:23.7083683013916\n",
      "训练次数：4819, loss:30.88459014892578\n",
      "训练次数：4820, loss:24.23833465576172\n",
      "训练次数：4821, loss:19.399328231811523\n",
      "训练次数：4822, loss:26.95348358154297\n",
      "训练次数：4823, loss:23.667980194091797\n",
      "训练次数：4824, loss:26.73801040649414\n",
      "训练次数：4825, loss:32.82541275024414\n",
      "训练次数：4826, loss:23.10138702392578\n",
      "训练次数：4827, loss:19.72572135925293\n",
      "训练次数：4828, loss:25.478012084960938\n",
      "训练次数：4829, loss:26.269981384277344\n",
      "训练次数：4830, loss:27.325876235961914\n",
      "训练次数：4831, loss:22.7078914642334\n",
      "训练次数：4832, loss:33.44230270385742\n",
      "训练次数：4833, loss:28.28221893310547\n",
      "训练次数：4834, loss:32.38914108276367\n",
      "训练次数：4835, loss:20.206275939941406\n",
      "训练次数：4836, loss:28.2929630279541\n",
      "训练次数：4837, loss:25.344573974609375\n",
      "训练次数：4838, loss:23.444334030151367\n",
      "训练次数：4839, loss:26.19989013671875\n",
      "训练次数：4840, loss:27.43250846862793\n",
      "训练次数：4841, loss:18.17239761352539\n",
      "训练次数：4842, loss:26.853654861450195\n",
      "训练次数：4843, loss:24.984392166137695\n",
      "训练次数：4844, loss:26.312835693359375\n",
      "训练次数：4845, loss:26.5468692779541\n",
      "训练次数：4846, loss:23.026391983032227\n",
      "训练次数：4847, loss:23.510215759277344\n",
      "训练次数：4848, loss:23.301658630371094\n",
      "训练次数：4849, loss:25.871267318725586\n",
      "训练次数：4850, loss:23.90009307861328\n",
      "训练次数：4851, loss:20.599178314208984\n",
      "训练次数：4852, loss:25.094772338867188\n",
      "训练次数：4853, loss:25.122386932373047\n",
      "训练次数：4854, loss:21.299745559692383\n",
      "训练次数：4855, loss:20.937101364135742\n",
      "训练次数：4856, loss:30.790908813476562\n",
      "训练次数：4857, loss:24.67804718017578\n",
      "训练次数：4858, loss:28.968637466430664\n",
      "训练次数：4859, loss:26.52760124206543\n",
      "训练次数：4860, loss:37.36181640625\n",
      "训练次数：4861, loss:22.791168212890625\n",
      "训练次数：4862, loss:24.332427978515625\n",
      "训练次数：4863, loss:31.59412956237793\n",
      "训练次数：4864, loss:30.764253616333008\n",
      "训练次数：4865, loss:25.278003692626953\n",
      "训练次数：4866, loss:30.03893280029297\n",
      "训练次数：4867, loss:22.961238861083984\n",
      "训练次数：4868, loss:25.561927795410156\n",
      "训练次数：4869, loss:35.54952621459961\n",
      "训练次数：4870, loss:27.26145362854004\n",
      "训练次数：4871, loss:30.628198623657227\n",
      "训练次数：4872, loss:31.613534927368164\n",
      "训练次数：4873, loss:39.23472213745117\n",
      "训练次数：4874, loss:22.619693756103516\n",
      "训练次数：4875, loss:22.024667739868164\n",
      "训练次数：4876, loss:19.506166458129883\n",
      "训练次数：4877, loss:26.891355514526367\n",
      "训练次数：4878, loss:26.88430404663086\n",
      "训练次数：4879, loss:21.981046676635742\n",
      "训练次数：4880, loss:23.594676971435547\n",
      "训练次数：4881, loss:18.01831817626953\n",
      "训练次数：4882, loss:22.170652389526367\n",
      "训练次数：4883, loss:11.073362350463867\n",
      "训练次数：4884, loss:25.93018913269043\n",
      "训练次数：4885, loss:22.745624542236328\n",
      "训练次数：4886, loss:25.139057159423828\n",
      "训练次数：4887, loss:19.739315032958984\n",
      "训练次数：4888, loss:23.534276962280273\n",
      "训练次数：4889, loss:17.315185546875\n",
      "训练次数：4890, loss:18.940393447875977\n",
      "训练次数：4891, loss:23.498085021972656\n",
      "训练次数：4892, loss:19.523221969604492\n",
      "训练次数：4893, loss:16.12247657775879\n",
      "训练次数：4894, loss:26.164817810058594\n",
      "训练次数：4895, loss:25.00553321838379\n",
      "训练次数：4896, loss:22.545957565307617\n",
      "训练次数：4897, loss:22.15336036682129\n",
      "训练次数：4898, loss:28.722179412841797\n",
      "训练次数：4899, loss:17.69230842590332\n",
      "训练次数：4900, loss:21.270353317260742\n",
      "训练次数：4901, loss:20.081850051879883\n",
      "训练次数：4902, loss:23.60600471496582\n",
      "训练次数：4903, loss:26.710893630981445\n",
      "训练次数：4904, loss:22.78194236755371\n",
      "训练次数：4905, loss:21.66407585144043\n",
      "训练次数：4906, loss:21.2182559967041\n",
      "训练次数：4907, loss:22.387033462524414\n",
      "训练次数：4908, loss:22.60443687438965\n",
      "训练次数：4909, loss:16.22475814819336\n",
      "训练次数：4910, loss:19.1289005279541\n",
      "训练次数：4911, loss:19.173381805419922\n",
      "训练次数：4912, loss:18.438196182250977\n",
      "训练次数：4913, loss:16.17310905456543\n",
      "训练次数：4914, loss:18.24797248840332\n",
      "训练次数：4915, loss:21.854887008666992\n",
      "训练次数：4916, loss:22.39166259765625\n",
      "训练次数：4917, loss:22.53432273864746\n",
      "训练次数：4918, loss:21.631282806396484\n",
      "训练次数：4919, loss:16.241050720214844\n",
      "训练次数：4920, loss:17.798324584960938\n",
      "训练次数：4921, loss:21.369552612304688\n",
      "训练次数：4922, loss:20.806713104248047\n",
      "训练次数：4923, loss:24.829450607299805\n",
      "训练次数：4924, loss:26.81944465637207\n",
      "训练次数：4925, loss:28.05824851989746\n",
      "训练次数：4926, loss:31.959871292114258\n",
      "训练次数：4927, loss:19.544565200805664\n",
      "训练次数：4928, loss:38.890769958496094\n",
      "训练次数：4929, loss:26.809391021728516\n",
      "训练次数：4930, loss:38.1127815246582\n",
      "训练次数：4931, loss:16.726991653442383\n",
      "训练次数：4932, loss:25.086822509765625\n",
      "训练次数：4933, loss:25.360065460205078\n",
      "训练次数：4934, loss:17.74334716796875\n",
      "训练次数：4935, loss:23.07254409790039\n",
      "训练次数：4936, loss:28.40721893310547\n",
      "训练次数：4937, loss:17.44536018371582\n",
      "训练次数：4938, loss:21.02916145324707\n",
      "训练次数：4939, loss:20.795642852783203\n",
      "训练次数：4940, loss:25.63713836669922\n",
      "训练次数：4941, loss:23.518701553344727\n",
      "训练次数：4942, loss:22.818519592285156\n",
      "训练次数：4943, loss:15.432449340820312\n",
      "训练次数：4944, loss:23.188308715820312\n",
      "训练次数：4945, loss:20.371173858642578\n",
      "训练次数：4946, loss:20.958507537841797\n",
      "训练次数：4947, loss:24.474767684936523\n",
      "训练次数：4948, loss:21.196075439453125\n",
      "训练次数：4949, loss:17.244165420532227\n",
      "训练次数：4950, loss:17.516721725463867\n",
      "训练次数：4951, loss:21.42660140991211\n",
      "训练次数：4952, loss:19.079029083251953\n",
      "训练次数：4953, loss:17.234912872314453\n",
      "训练次数：4954, loss:18.59989356994629\n",
      "训练次数：4955, loss:24.41935157775879\n",
      "训练次数：4956, loss:16.138355255126953\n",
      "训练次数：4957, loss:18.4837589263916\n",
      "训练次数：4958, loss:19.5168514251709\n",
      "训练次数：4959, loss:20.559669494628906\n",
      "训练次数：4960, loss:17.97264289855957\n",
      "训练次数：4961, loss:22.192895889282227\n",
      "训练次数：4962, loss:15.019265174865723\n",
      "训练次数：4963, loss:23.09181785583496\n",
      "训练次数：4964, loss:17.30592155456543\n",
      "训练次数：4965, loss:18.41858673095703\n",
      "训练次数：4966, loss:18.81962776184082\n",
      "训练次数：4967, loss:20.941425323486328\n",
      "训练次数：4968, loss:19.65053939819336\n",
      "训练次数：4969, loss:30.484189987182617\n",
      "训练次数：4970, loss:16.157163619995117\n",
      "训练次数：4971, loss:32.45503234863281\n",
      "训练次数：4972, loss:23.62799835205078\n",
      "训练次数：4973, loss:26.291858673095703\n",
      "训练次数：4974, loss:17.26982307434082\n",
      "训练次数：4975, loss:16.361446380615234\n",
      "训练次数：4976, loss:17.76677703857422\n",
      "训练次数：4977, loss:12.332845687866211\n",
      "训练次数：4978, loss:23.094839096069336\n",
      "训练次数：4979, loss:18.307832717895508\n",
      "训练次数：4980, loss:19.00510025024414\n",
      "训练次数：4981, loss:23.535186767578125\n",
      "训练次数：4982, loss:18.754131317138672\n",
      "训练次数：4983, loss:13.578317642211914\n",
      "训练次数：4984, loss:21.69529151916504\n",
      "训练次数：4985, loss:17.83966827392578\n",
      "训练次数：4986, loss:17.745189666748047\n",
      "训练次数：4987, loss:19.653228759765625\n",
      "训练次数：4988, loss:15.221266746520996\n",
      "训练次数：4989, loss:19.995059967041016\n",
      "训练次数：4990, loss:18.41671371459961\n",
      "训练次数：4991, loss:15.186822891235352\n",
      "训练次数：4992, loss:16.427824020385742\n",
      "训练次数：4993, loss:16.273935317993164\n",
      "训练次数：4994, loss:16.664369583129883\n",
      "训练次数：4995, loss:18.220422744750977\n",
      "训练次数：4996, loss:16.290817260742188\n",
      "训练次数：4997, loss:20.007877349853516\n",
      "训练次数：4998, loss:21.65191650390625\n",
      "训练次数：4999, loss:15.334242820739746\n",
      "训练次数：5000, loss:16.05956268310547\n",
      "训练次数：5001, loss:23.025970458984375\n",
      "训练次数：5002, loss:21.578094482421875\n",
      "训练次数：5003, loss:19.180755615234375\n",
      "训练次数：5004, loss:16.179851531982422\n",
      "训练次数：5005, loss:21.61731719970703\n",
      "训练次数：5006, loss:23.59853172302246\n",
      "训练次数：5007, loss:30.59259605407715\n",
      "训练次数：5008, loss:18.667583465576172\n",
      "训练次数：5009, loss:18.656213760375977\n",
      "训练次数：5010, loss:22.73654556274414\n",
      "训练次数：5011, loss:23.49169158935547\n",
      "训练次数：5012, loss:25.265079498291016\n",
      "训练次数：5013, loss:18.400522232055664\n",
      "训练次数：5014, loss:20.275312423706055\n",
      "训练次数：5015, loss:21.59210968017578\n",
      "训练次数：5016, loss:19.623241424560547\n",
      "训练次数：5017, loss:18.946535110473633\n",
      "训练次数：5018, loss:15.424612998962402\n",
      "训练次数：5019, loss:16.935392379760742\n",
      "训练次数：5020, loss:21.330806732177734\n",
      "训练次数：5021, loss:21.98497772216797\n",
      "训练次数：5022, loss:16.350183486938477\n",
      "训练次数：5023, loss:16.36020278930664\n",
      "训练次数：5024, loss:12.427492141723633\n",
      "训练次数：5025, loss:13.800067901611328\n",
      "训练次数：5026, loss:17.632020950317383\n",
      "训练次数：5027, loss:22.020187377929688\n",
      "训练次数：5028, loss:23.05718231201172\n",
      "训练次数：5029, loss:21.556249618530273\n",
      "训练次数：5030, loss:19.078371047973633\n",
      "训练次数：5031, loss:19.603696823120117\n",
      "训练次数：5032, loss:17.635173797607422\n",
      "训练次数：5033, loss:19.003023147583008\n",
      "训练次数：5034, loss:17.874530792236328\n",
      "训练次数：5035, loss:25.81772232055664\n",
      "训练次数：5036, loss:16.23042869567871\n",
      "训练次数：5037, loss:23.477203369140625\n",
      "训练次数：5038, loss:16.43683624267578\n",
      "训练次数：5039, loss:20.113569259643555\n",
      "训练次数：5040, loss:20.10946273803711\n",
      "训练次数：5041, loss:15.75969409942627\n",
      "训练次数：5042, loss:12.645838737487793\n",
      "训练次数：5043, loss:17.00802993774414\n",
      "训练次数：5044, loss:24.290637969970703\n",
      "训练次数：5045, loss:15.984757423400879\n",
      "训练次数：5046, loss:18.087003707885742\n",
      "训练次数：5047, loss:16.01640510559082\n",
      "训练次数：5048, loss:17.800710678100586\n",
      "训练次数：5049, loss:15.93330192565918\n",
      "训练次数：5050, loss:18.459148406982422\n",
      "训练次数：5051, loss:14.711602210998535\n",
      "训练次数：5052, loss:14.551734924316406\n",
      "训练次数：5053, loss:24.1488037109375\n",
      "训练次数：5054, loss:14.697938919067383\n",
      "训练次数：5055, loss:19.04978370666504\n",
      "训练次数：5056, loss:15.551753044128418\n",
      "训练次数：5057, loss:14.299132347106934\n",
      "训练次数：5058, loss:13.068071365356445\n",
      "训练次数：5059, loss:19.075698852539062\n",
      "训练次数：5060, loss:19.209943771362305\n",
      "训练次数：5061, loss:9.789731979370117\n",
      "训练次数：5062, loss:17.456073760986328\n",
      "训练次数：5063, loss:13.410371780395508\n",
      "训练次数：5064, loss:18.865802764892578\n",
      "训练次数：5065, loss:14.648621559143066\n",
      "训练次数：5066, loss:13.73127269744873\n",
      "训练次数：5067, loss:19.82494354248047\n",
      "训练次数：5068, loss:13.130318641662598\n",
      "训练次数：5069, loss:18.245914459228516\n",
      "训练次数：5070, loss:19.098617553710938\n",
      "训练次数：5071, loss:25.618194580078125\n",
      "训练次数：5072, loss:18.85399627685547\n",
      "训练次数：5073, loss:13.581995010375977\n",
      "训练次数：5074, loss:18.579017639160156\n",
      "训练次数：5075, loss:15.980114936828613\n",
      "训练次数：5076, loss:19.465974807739258\n",
      "训练次数：5077, loss:19.252017974853516\n",
      "训练次数：5078, loss:12.822771072387695\n",
      "训练次数：5079, loss:20.715059280395508\n",
      "训练次数：5080, loss:14.826345443725586\n",
      "训练次数：5081, loss:16.43230438232422\n",
      "训练次数：5082, loss:11.845683097839355\n",
      "训练次数：5083, loss:18.076919555664062\n",
      "训练次数：5084, loss:10.432392120361328\n",
      "训练次数：5085, loss:15.460124015808105\n",
      "训练次数：5086, loss:17.135202407836914\n",
      "训练次数：5087, loss:17.070096969604492\n",
      "训练次数：5088, loss:19.653846740722656\n",
      "训练次数：5089, loss:15.718708038330078\n",
      "训练次数：5090, loss:18.41426658630371\n",
      "训练次数：5091, loss:12.10130786895752\n",
      "训练次数：5092, loss:23.44950294494629\n",
      "训练次数：5093, loss:21.38436508178711\n",
      "训练次数：5094, loss:15.693585395812988\n",
      "训练次数：5095, loss:16.88475227355957\n",
      "训练次数：5096, loss:18.873781204223633\n",
      "训练次数：5097, loss:23.907146453857422\n",
      "训练次数：5098, loss:20.26333999633789\n",
      "训练次数：5099, loss:18.64781379699707\n",
      "训练次数：5100, loss:12.075311660766602\n",
      "训练次数：5101, loss:17.37169647216797\n",
      "训练次数：5102, loss:19.052005767822266\n",
      "训练次数：5103, loss:19.510730743408203\n",
      "训练次数：5104, loss:15.731470108032227\n",
      "训练次数：5105, loss:20.393787384033203\n",
      "训练次数：5106, loss:19.459375381469727\n",
      "训练次数：5107, loss:20.936695098876953\n",
      "训练次数：5108, loss:13.04575252532959\n",
      "训练次数：5109, loss:17.357210159301758\n",
      "训练次数：5110, loss:19.08784294128418\n",
      "训练次数：5111, loss:20.513580322265625\n",
      "训练次数：5112, loss:17.024635314941406\n",
      "训练次数：5113, loss:18.20386505126953\n",
      "训练次数：5114, loss:17.257568359375\n",
      "训练次数：5115, loss:24.439802169799805\n",
      "训练次数：5116, loss:19.56534767150879\n",
      "训练次数：5117, loss:22.49814224243164\n",
      "训练次数：5118, loss:20.80533790588379\n",
      "训练次数：5119, loss:26.86555290222168\n",
      "训练次数：5120, loss:14.712742805480957\n",
      "训练次数：5121, loss:20.058753967285156\n",
      "训练次数：5122, loss:17.217052459716797\n",
      "训练次数：5123, loss:23.64647102355957\n",
      "训练次数：5124, loss:12.527326583862305\n",
      "训练次数：5125, loss:19.97972297668457\n",
      "训练次数：5126, loss:14.25173282623291\n",
      "训练次数：5127, loss:19.880191802978516\n",
      "训练次数：5128, loss:16.708810806274414\n",
      "训练次数：5129, loss:15.467602729797363\n",
      "训练次数：5130, loss:15.529460906982422\n",
      "训练次数：5131, loss:15.109865188598633\n",
      "训练次数：5132, loss:12.076654434204102\n",
      "训练次数：5133, loss:17.49949836730957\n",
      "训练次数：5134, loss:15.731554985046387\n",
      "训练次数：5135, loss:12.790387153625488\n",
      "训练次数：5136, loss:10.146738052368164\n",
      "训练次数：5137, loss:17.784156799316406\n",
      "训练次数：5138, loss:14.326900482177734\n",
      "训练次数：5139, loss:17.96964454650879\n",
      "训练次数：5140, loss:10.886897087097168\n",
      "训练次数：5141, loss:12.7044677734375\n",
      "训练次数：5142, loss:14.365165710449219\n",
      "训练次数：5143, loss:12.562393188476562\n",
      "训练次数：5144, loss:10.369242668151855\n",
      "训练次数：5145, loss:16.655799865722656\n",
      "训练次数：5146, loss:13.935332298278809\n",
      "训练次数：5147, loss:12.45556640625\n",
      "训练次数：5148, loss:13.82007884979248\n",
      "训练次数：5149, loss:15.319931983947754\n",
      "训练次数：5150, loss:14.063742637634277\n",
      "训练次数：5151, loss:14.882552146911621\n",
      "训练次数：5152, loss:12.785470008850098\n",
      "训练次数：5153, loss:15.433245658874512\n",
      "训练次数：5154, loss:15.730264663696289\n",
      "训练次数：5155, loss:10.711402893066406\n",
      "训练次数：5156, loss:18.673507690429688\n",
      "训练次数：5157, loss:17.110511779785156\n",
      "训练次数：5158, loss:10.884297370910645\n",
      "训练次数：5159, loss:15.488883018493652\n",
      "训练次数：5160, loss:12.772201538085938\n",
      "训练次数：5161, loss:13.610149383544922\n",
      "训练次数：5162, loss:20.329137802124023\n",
      "训练次数：5163, loss:11.820582389831543\n",
      "训练次数：5164, loss:12.051713943481445\n",
      "训练次数：5165, loss:11.518265724182129\n",
      "训练次数：5166, loss:15.725798606872559\n",
      "训练次数：5167, loss:11.905560493469238\n",
      "训练次数：5168, loss:15.156599998474121\n",
      "训练次数：5169, loss:17.007932662963867\n",
      "训练次数：5170, loss:13.553349494934082\n",
      "训练次数：5171, loss:18.240989685058594\n",
      "训练次数：5172, loss:14.00727653503418\n",
      "训练次数：5173, loss:18.70055389404297\n",
      "训练次数：5174, loss:9.346846580505371\n",
      "训练次数：5175, loss:13.472847938537598\n",
      "训练次数：5176, loss:13.055038452148438\n",
      "训练次数：5177, loss:12.822016716003418\n",
      "训练次数：5178, loss:12.056670188903809\n",
      "训练次数：5179, loss:17.076786041259766\n",
      "训练次数：5180, loss:15.497358322143555\n",
      "训练次数：5181, loss:17.740583419799805\n",
      "训练次数：5182, loss:10.506292343139648\n",
      "训练次数：5183, loss:16.771141052246094\n",
      "训练次数：5184, loss:15.606350898742676\n",
      "训练次数：5185, loss:11.653409957885742\n",
      "训练次数：5186, loss:11.58256721496582\n",
      "训练次数：5187, loss:12.486992835998535\n",
      "训练次数：5188, loss:17.219139099121094\n",
      "训练次数：5189, loss:12.574919700622559\n",
      "训练次数：5190, loss:12.397892951965332\n",
      "训练次数：5191, loss:13.805161476135254\n",
      "训练次数：5192, loss:13.387685775756836\n",
      "训练次数：5193, loss:12.78549575805664\n",
      "训练次数：5194, loss:10.657021522521973\n",
      "训练次数：5195, loss:16.468669891357422\n",
      "训练次数：5196, loss:16.908367156982422\n",
      "训练次数：5197, loss:15.351329803466797\n",
      "训练次数：5198, loss:14.257061004638672\n",
      "训练次数：5199, loss:13.524438858032227\n",
      "训练次数：5200, loss:17.501739501953125\n",
      "训练次数：5201, loss:13.121160507202148\n",
      "训练次数：5202, loss:16.940732955932617\n",
      "训练次数：5203, loss:16.788291931152344\n",
      "训练次数：5204, loss:16.199451446533203\n",
      "训练次数：5205, loss:15.884490013122559\n",
      "训练次数：5206, loss:12.414929389953613\n",
      "训练次数：5207, loss:16.959320068359375\n",
      "训练次数：5208, loss:13.866832733154297\n",
      "训练次数：5209, loss:19.36587905883789\n",
      "训练次数：5210, loss:14.277364730834961\n",
      "训练次数：5211, loss:14.43634033203125\n",
      "训练次数：5212, loss:8.779096603393555\n",
      "训练次数：5213, loss:11.882516860961914\n",
      "训练次数：5214, loss:14.804993629455566\n",
      "训练次数：5215, loss:12.403889656066895\n",
      "训练次数：5216, loss:13.716160774230957\n",
      "训练次数：5217, loss:12.304593086242676\n",
      "训练次数：5218, loss:17.692846298217773\n",
      "训练次数：5219, loss:10.545515060424805\n",
      "训练次数：5220, loss:11.724985122680664\n",
      "训练次数：5221, loss:12.950509071350098\n",
      "训练次数：5222, loss:12.3622465133667\n",
      "训练次数：5223, loss:13.879355430603027\n",
      "训练次数：5224, loss:14.861772537231445\n",
      "训练次数：5225, loss:11.208693504333496\n",
      "训练次数：5226, loss:10.646347999572754\n",
      "训练次数：5227, loss:10.442033767700195\n",
      "训练次数：5228, loss:8.768521308898926\n",
      "训练次数：5229, loss:13.965923309326172\n",
      "训练次数：5230, loss:11.48825454711914\n",
      "训练次数：5231, loss:13.014155387878418\n",
      "训练次数：5232, loss:10.560301780700684\n",
      "训练次数：5233, loss:13.576909065246582\n",
      "训练次数：5234, loss:16.530061721801758\n",
      "训练次数：5235, loss:11.239115715026855\n",
      "训练次数：5236, loss:9.822751998901367\n",
      "训练次数：5237, loss:12.246347427368164\n",
      "训练次数：5238, loss:12.909648895263672\n",
      "训练次数：5239, loss:15.327224731445312\n",
      "训练次数：5240, loss:13.85118293762207\n",
      "训练次数：5241, loss:13.163787841796875\n",
      "训练次数：5242, loss:10.868341445922852\n",
      "训练次数：5243, loss:12.836254119873047\n",
      "训练次数：5244, loss:16.4809627532959\n",
      "训练次数：5245, loss:9.990449905395508\n",
      "训练次数：5246, loss:12.788747787475586\n",
      "训练次数：5247, loss:9.966266632080078\n",
      "训练次数：5248, loss:11.103862762451172\n",
      "训练次数：5249, loss:14.452648162841797\n",
      "训练次数：5250, loss:13.781384468078613\n",
      "训练次数：5251, loss:15.731215476989746\n",
      "训练次数：5252, loss:10.887978553771973\n",
      "训练次数：5253, loss:11.474288940429688\n",
      "训练次数：5254, loss:11.609976768493652\n",
      "训练次数：5255, loss:16.360496520996094\n",
      "训练次数：5256, loss:13.051203727722168\n",
      "训练次数：5257, loss:14.946967124938965\n",
      "训练次数：5258, loss:12.922282218933105\n",
      "训练次数：5259, loss:13.445491790771484\n",
      "训练次数：5260, loss:15.201910972595215\n",
      "训练次数：5261, loss:11.938828468322754\n",
      "训练次数：5262, loss:14.214019775390625\n",
      "训练次数：5263, loss:18.20063018798828\n",
      "训练次数：5264, loss:12.991374015808105\n",
      "训练次数：5265, loss:11.424938201904297\n",
      "训练次数：5266, loss:12.909529685974121\n",
      "训练次数：5267, loss:12.019230842590332\n",
      "训练次数：5268, loss:13.31292724609375\n",
      "训练次数：5269, loss:12.503003120422363\n",
      "训练次数：5270, loss:15.56741714477539\n",
      "训练次数：5271, loss:9.886348724365234\n",
      "训练次数：5272, loss:13.657185554504395\n",
      "训练次数：5273, loss:12.386387825012207\n",
      "训练次数：5274, loss:12.894485473632812\n",
      "训练次数：5275, loss:12.290861129760742\n",
      "训练次数：5276, loss:15.029114723205566\n",
      "训练次数：5277, loss:15.935782432556152\n",
      "训练次数：5278, loss:14.020852088928223\n",
      "训练次数：5279, loss:12.247767448425293\n",
      "训练次数：5280, loss:11.55585765838623\n",
      "训练次数：5281, loss:12.048965454101562\n",
      "训练次数：5282, loss:12.06318473815918\n",
      "训练次数：5283, loss:13.849328994750977\n",
      "训练次数：5284, loss:8.06685733795166\n",
      "训练次数：5285, loss:12.243144989013672\n",
      "训练次数：5286, loss:10.532066345214844\n",
      "训练次数：5287, loss:15.494241714477539\n",
      "训练次数：5288, loss:16.974451065063477\n",
      "训练次数：5289, loss:13.399748802185059\n",
      "训练次数：5290, loss:12.756450653076172\n",
      "训练次数：5291, loss:9.444355010986328\n",
      "训练次数：5292, loss:9.720157623291016\n",
      "训练次数：5293, loss:12.046518325805664\n",
      "训练次数：5294, loss:9.500139236450195\n",
      "训练次数：5295, loss:10.031357765197754\n",
      "训练次数：5296, loss:13.217451095581055\n",
      "训练次数：5297, loss:12.181727409362793\n",
      "训练次数：5298, loss:8.816304206848145\n",
      "训练次数：5299, loss:10.26750373840332\n",
      "训练次数：5300, loss:11.157428741455078\n",
      "训练次数：5301, loss:11.701268196105957\n",
      "训练次数：5302, loss:12.198675155639648\n",
      "训练次数：5303, loss:12.620247840881348\n",
      "训练次数：5304, loss:12.867467880249023\n",
      "训练次数：5305, loss:10.56201171875\n",
      "训练次数：5306, loss:11.684378623962402\n",
      "训练次数：5307, loss:13.71243667602539\n",
      "训练次数：5308, loss:11.652817726135254\n",
      "训练次数：5309, loss:17.675737380981445\n",
      "训练次数：5310, loss:11.661906242370605\n",
      "训练次数：5311, loss:14.267378807067871\n",
      "训练次数：5312, loss:11.248870849609375\n",
      "训练次数：5313, loss:16.256864547729492\n",
      "训练次数：5314, loss:19.472375869750977\n",
      "训练次数：5315, loss:12.220269203186035\n",
      "训练次数：5316, loss:10.768472671508789\n",
      "训练次数：5317, loss:13.969079971313477\n",
      "训练次数：5318, loss:12.33891773223877\n",
      "训练次数：5319, loss:15.177961349487305\n",
      "训练次数：5320, loss:15.968222618103027\n",
      "训练次数：5321, loss:11.393820762634277\n",
      "训练次数：5322, loss:12.521881103515625\n",
      "训练次数：5323, loss:12.258578300476074\n",
      "训练次数：5324, loss:16.58456802368164\n",
      "训练次数：5325, loss:10.983052253723145\n",
      "训练次数：5326, loss:8.929713249206543\n",
      "训练次数：5327, loss:10.274179458618164\n",
      "训练次数：5328, loss:11.695096969604492\n",
      "训练次数：5329, loss:11.517424583435059\n",
      "训练次数：5330, loss:13.753124237060547\n",
      "训练次数：5331, loss:13.011131286621094\n",
      "训练次数：5332, loss:11.309821128845215\n",
      "训练次数：5333, loss:16.27701759338379\n",
      "训练次数：5334, loss:15.577573776245117\n",
      "训练次数：5335, loss:13.484639167785645\n",
      "训练次数：5336, loss:10.1078519821167\n",
      "训练次数：5337, loss:17.12009620666504\n",
      "训练次数：5338, loss:15.462090492248535\n",
      "训练次数：5339, loss:19.050954818725586\n",
      "训练次数：5340, loss:9.840496063232422\n",
      "训练次数：5341, loss:16.82769775390625\n",
      "训练次数：5342, loss:21.837488174438477\n",
      "训练次数：5343, loss:11.120392799377441\n",
      "训练次数：5344, loss:14.175505638122559\n",
      "训练次数：5345, loss:8.394369125366211\n",
      "训练次数：5346, loss:10.796998977661133\n",
      "训练次数：5347, loss:14.859868049621582\n",
      "训练次数：5348, loss:16.188861846923828\n",
      "训练次数：5349, loss:13.233426094055176\n",
      "训练次数：5350, loss:11.0687837600708\n",
      "训练次数：5351, loss:16.583860397338867\n",
      "训练次数：5352, loss:10.030571937561035\n",
      "训练次数：5353, loss:10.82607650756836\n",
      "训练次数：5354, loss:11.008625030517578\n",
      "训练次数：5355, loss:9.434354782104492\n",
      "训练次数：5356, loss:13.201762199401855\n",
      "训练次数：5357, loss:11.185604095458984\n",
      "训练次数：5358, loss:11.832839965820312\n",
      "训练次数：5359, loss:12.656468391418457\n",
      "训练次数：5360, loss:15.429250717163086\n",
      "训练次数：5361, loss:16.840675354003906\n",
      "训练次数：5362, loss:12.342158317565918\n",
      "训练次数：5363, loss:12.070162773132324\n",
      "训练次数：5364, loss:13.331865310668945\n",
      "训练次数：5365, loss:10.641796112060547\n",
      "训练次数：5366, loss:15.751847267150879\n",
      "训练次数：5367, loss:17.242170333862305\n",
      "训练次数：5368, loss:14.135594367980957\n",
      "训练次数：5369, loss:11.987136840820312\n",
      "训练次数：5370, loss:13.265130996704102\n",
      "训练次数：5371, loss:12.752405166625977\n",
      "训练次数：5372, loss:15.731051445007324\n",
      "训练次数：5373, loss:16.78813934326172\n",
      "训练次数：5374, loss:7.654821872711182\n",
      "训练次数：5375, loss:10.43471908569336\n",
      "训练次数：5376, loss:13.689457893371582\n",
      "训练次数：5377, loss:15.271647453308105\n",
      "训练次数：5378, loss:15.179699897766113\n",
      "训练次数：5379, loss:10.079426765441895\n",
      "训练次数：5380, loss:11.4308443069458\n",
      "训练次数：5381, loss:10.147673606872559\n",
      "训练次数：5382, loss:14.48813247680664\n",
      "训练次数：5383, loss:13.570265769958496\n",
      "训练次数：5384, loss:11.972724914550781\n",
      "训练次数：5385, loss:14.432413101196289\n",
      "训练次数：5386, loss:12.673797607421875\n",
      "训练次数：5387, loss:17.163318634033203\n",
      "训练次数：5388, loss:11.743144989013672\n",
      "训练次数：5389, loss:11.51551342010498\n",
      "训练次数：5390, loss:12.90237045288086\n",
      "训练次数：5391, loss:8.085685729980469\n",
      "训练次数：5392, loss:17.25747299194336\n",
      "训练次数：5393, loss:12.312065124511719\n",
      "训练次数：5394, loss:12.686680793762207\n",
      "训练次数：5395, loss:14.32291030883789\n",
      "训练次数：5396, loss:10.545132637023926\n",
      "训练次数：5397, loss:13.965287208557129\n",
      "训练次数：5398, loss:14.677276611328125\n",
      "训练次数：5399, loss:11.15459156036377\n",
      "训练次数：5400, loss:10.89453125\n",
      "训练次数：5401, loss:8.73718547821045\n",
      "训练次数：5402, loss:12.174468040466309\n",
      "训练次数：5403, loss:9.858541488647461\n",
      "训练次数：5404, loss:7.579301834106445\n",
      "训练次数：5405, loss:9.2921724319458\n",
      "训练次数：5406, loss:12.095185279846191\n",
      "训练次数：5407, loss:10.793685913085938\n",
      "训练次数：5408, loss:10.574864387512207\n",
      "训练次数：5409, loss:12.332233428955078\n",
      "训练次数：5410, loss:14.355833053588867\n",
      "训练次数：5411, loss:8.79801082611084\n",
      "训练次数：5412, loss:11.440281867980957\n",
      "训练次数：5413, loss:10.970892906188965\n",
      "训练次数：5414, loss:10.143460273742676\n",
      "训练次数：5415, loss:9.76404857635498\n",
      "训练次数：5416, loss:9.723955154418945\n",
      "训练次数：5417, loss:12.133626937866211\n",
      "训练次数：5418, loss:9.823896408081055\n",
      "训练次数：5419, loss:10.436156272888184\n",
      "训练次数：5420, loss:16.83623695373535\n",
      "训练次数：5421, loss:11.654435157775879\n",
      "训练次数：5422, loss:16.4217472076416\n",
      "训练次数：5423, loss:9.221734046936035\n",
      "训练次数：5424, loss:11.464316368103027\n",
      "训练次数：5425, loss:12.82834243774414\n",
      "训练次数：5426, loss:10.470046997070312\n",
      "训练次数：5427, loss:11.835005760192871\n",
      "训练次数：5428, loss:11.638830184936523\n",
      "训练次数：5429, loss:8.055999755859375\n",
      "训练次数：5430, loss:14.67797565460205\n",
      "训练次数：5431, loss:11.225674629211426\n",
      "训练次数：5432, loss:9.897866249084473\n",
      "训练次数：5433, loss:7.404188632965088\n",
      "训练次数：5434, loss:11.310564041137695\n",
      "训练次数：5435, loss:14.72463321685791\n",
      "训练次数：5436, loss:12.340155601501465\n",
      "训练次数：5437, loss:7.372325897216797\n",
      "训练次数：5438, loss:12.277491569519043\n",
      "训练次数：5439, loss:11.973793983459473\n",
      "训练次数：5440, loss:11.924098014831543\n",
      "训练次数：5441, loss:12.682204246520996\n",
      "训练次数：5442, loss:10.149738311767578\n",
      "训练次数：5443, loss:15.82732105255127\n",
      "训练次数：5444, loss:12.57796859741211\n",
      "训练次数：5445, loss:8.221954345703125\n",
      "训练次数：5446, loss:11.7911376953125\n",
      "训练次数：5447, loss:11.660659790039062\n",
      "训练次数：5448, loss:12.468754768371582\n",
      "训练次数：5449, loss:16.723012924194336\n",
      "训练次数：5450, loss:8.548250198364258\n",
      "训练次数：5451, loss:12.11685562133789\n",
      "训练次数：5452, loss:11.543285369873047\n",
      "训练次数：5453, loss:10.59343433380127\n",
      "训练次数：5454, loss:14.550249099731445\n",
      "训练次数：5455, loss:10.67674732208252\n",
      "训练次数：5456, loss:14.666915893554688\n",
      "训练次数：5457, loss:15.82866382598877\n",
      "训练次数：5458, loss:12.578644752502441\n",
      "训练次数：5459, loss:15.692778587341309\n",
      "训练次数：5460, loss:13.5762939453125\n",
      "训练次数：5461, loss:8.539651870727539\n",
      "训练次数：5462, loss:15.560920715332031\n",
      "训练次数：5463, loss:12.817374229431152\n",
      "训练次数：5464, loss:11.805829048156738\n",
      "训练次数：5465, loss:14.66012191772461\n",
      "训练次数：5466, loss:13.824670791625977\n",
      "训练次数：5467, loss:10.813529968261719\n",
      "训练次数：5468, loss:10.119546890258789\n",
      "训练次数：5469, loss:15.16002082824707\n",
      "训练次数：5470, loss:11.00412654876709\n",
      "训练次数：5471, loss:13.351508140563965\n",
      "训练次数：5472, loss:11.242413520812988\n",
      "训练次数：5473, loss:16.355079650878906\n",
      "训练次数：5474, loss:21.213058471679688\n",
      "----------第7轮训练开始----------\n",
      "训练次数：5475, loss:15.942964553833008\n",
      "训练次数：5476, loss:9.716021537780762\n",
      "训练次数：5477, loss:10.863614082336426\n",
      "训练次数：5478, loss:9.377425193786621\n",
      "训练次数：5479, loss:11.693007469177246\n",
      "训练次数：5480, loss:10.596315383911133\n",
      "训练次数：5481, loss:11.16069507598877\n",
      "训练次数：5482, loss:11.27215576171875\n",
      "训练次数：5483, loss:11.124211311340332\n",
      "训练次数：5484, loss:14.376758575439453\n",
      "训练次数：5485, loss:16.21668243408203\n",
      "训练次数：5486, loss:10.682770729064941\n",
      "训练次数：5487, loss:15.146541595458984\n",
      "训练次数：5488, loss:11.297137260437012\n",
      "训练次数：5489, loss:8.792498588562012\n",
      "训练次数：5490, loss:8.713388442993164\n",
      "训练次数：5491, loss:15.895045280456543\n",
      "训练次数：5492, loss:8.095039367675781\n",
      "训练次数：5493, loss:9.827743530273438\n",
      "训练次数：5494, loss:9.896988868713379\n",
      "训练次数：5495, loss:11.373395919799805\n",
      "训练次数：5496, loss:8.553468704223633\n",
      "训练次数：5497, loss:9.103129386901855\n",
      "训练次数：5498, loss:10.905556678771973\n",
      "训练次数：5499, loss:10.633991241455078\n",
      "训练次数：5500, loss:9.957867622375488\n",
      "训练次数：5501, loss:9.958333969116211\n",
      "训练次数：5502, loss:9.047856330871582\n",
      "训练次数：5503, loss:10.268229484558105\n",
      "训练次数：5504, loss:8.553467750549316\n",
      "训练次数：5505, loss:8.501886367797852\n",
      "训练次数：5506, loss:12.593259811401367\n",
      "训练次数：5507, loss:10.449862480163574\n",
      "训练次数：5508, loss:9.731119155883789\n",
      "训练次数：5509, loss:9.447530746459961\n",
      "训练次数：5510, loss:10.688347816467285\n",
      "训练次数：5511, loss:10.093463897705078\n",
      "训练次数：5512, loss:9.745878219604492\n",
      "训练次数：5513, loss:9.745452880859375\n",
      "训练次数：5514, loss:12.942473411560059\n",
      "训练次数：5515, loss:8.201722145080566\n",
      "训练次数：5516, loss:10.370224952697754\n",
      "训练次数：5517, loss:10.270487785339355\n",
      "训练次数：5518, loss:9.33496379852295\n",
      "训练次数：5519, loss:6.010189056396484\n",
      "训练次数：5520, loss:7.948186874389648\n",
      "训练次数：5521, loss:11.519000053405762\n",
      "训练次数：5522, loss:10.467171669006348\n",
      "训练次数：5523, loss:11.495372772216797\n",
      "训练次数：5524, loss:9.172149658203125\n",
      "训练次数：5525, loss:10.449265480041504\n",
      "训练次数：5526, loss:12.825478553771973\n",
      "训练次数：5527, loss:8.648852348327637\n",
      "训练次数：5528, loss:12.797103881835938\n",
      "训练次数：5529, loss:9.917659759521484\n",
      "训练次数：5530, loss:11.952056884765625\n",
      "训练次数：5531, loss:7.839028835296631\n",
      "训练次数：5532, loss:9.522196769714355\n",
      "训练次数：5533, loss:11.325248718261719\n",
      "训练次数：5534, loss:10.242764472961426\n",
      "训练次数：5535, loss:10.13351821899414\n",
      "训练次数：5536, loss:10.42497730255127\n",
      "训练次数：5537, loss:9.100028038024902\n",
      "训练次数：5538, loss:9.003494262695312\n",
      "训练次数：5539, loss:10.256570816040039\n",
      "训练次数：5540, loss:9.933152198791504\n",
      "训练次数：5541, loss:9.338785171508789\n",
      "训练次数：5542, loss:11.258645057678223\n",
      "训练次数：5543, loss:11.612979888916016\n",
      "训练次数：5544, loss:8.573838233947754\n",
      "训练次数：5545, loss:9.847489356994629\n",
      "训练次数：5546, loss:9.98880672454834\n",
      "训练次数：5547, loss:7.8306779861450195\n",
      "训练次数：5548, loss:10.129838943481445\n",
      "训练次数：5549, loss:9.7683687210083\n",
      "训练次数：5550, loss:12.774704933166504\n",
      "训练次数：5551, loss:12.77334213256836\n",
      "训练次数：5552, loss:11.398119926452637\n",
      "训练次数：5553, loss:8.40835189819336\n",
      "训练次数：5554, loss:15.596609115600586\n",
      "训练次数：5555, loss:11.305484771728516\n",
      "训练次数：5556, loss:11.040815353393555\n",
      "训练次数：5557, loss:10.42391586303711\n",
      "训练次数：5558, loss:9.757960319519043\n",
      "训练次数：5559, loss:11.579548835754395\n",
      "训练次数：5560, loss:6.5539326667785645\n",
      "训练次数：5561, loss:10.074625968933105\n",
      "训练次数：5562, loss:8.353529930114746\n",
      "训练次数：5563, loss:8.396021842956543\n",
      "训练次数：5564, loss:9.975387573242188\n",
      "训练次数：5565, loss:13.075289726257324\n",
      "训练次数：5566, loss:9.88625717163086\n",
      "训练次数：5567, loss:10.623872756958008\n",
      "训练次数：5568, loss:9.729181289672852\n",
      "训练次数：5569, loss:9.513358116149902\n",
      "训练次数：5570, loss:9.54684829711914\n",
      "训练次数：5571, loss:8.457645416259766\n",
      "训练次数：5572, loss:9.987818717956543\n",
      "训练次数：5573, loss:10.436272621154785\n",
      "训练次数：5574, loss:10.076629638671875\n",
      "训练次数：5575, loss:12.324288368225098\n",
      "训练次数：5576, loss:10.722235679626465\n",
      "训练次数：5577, loss:11.038102149963379\n",
      "训练次数：5578, loss:11.246850967407227\n",
      "训练次数：5579, loss:9.171527862548828\n",
      "训练次数：5580, loss:9.93315601348877\n",
      "训练次数：5581, loss:11.261714935302734\n",
      "训练次数：5582, loss:11.00833797454834\n",
      "训练次数：5583, loss:9.003617286682129\n",
      "训练次数：5584, loss:8.692834854125977\n",
      "训练次数：5585, loss:6.033742904663086\n",
      "训练次数：5586, loss:12.204449653625488\n",
      "训练次数：5587, loss:8.840243339538574\n",
      "训练次数：5588, loss:11.877498626708984\n",
      "训练次数：5589, loss:9.16599178314209\n",
      "训练次数：5590, loss:10.822562217712402\n",
      "训练次数：5591, loss:9.44989013671875\n",
      "训练次数：5592, loss:10.957508087158203\n",
      "训练次数：5593, loss:10.1206636428833\n",
      "训练次数：5594, loss:12.302450180053711\n",
      "训练次数：5595, loss:9.553021430969238\n",
      "训练次数：5596, loss:9.017678260803223\n",
      "训练次数：5597, loss:10.570337295532227\n",
      "训练次数：5598, loss:10.26411247253418\n",
      "训练次数：5599, loss:9.853852272033691\n",
      "训练次数：5600, loss:12.687093734741211\n",
      "训练次数：5601, loss:9.229130744934082\n",
      "训练次数：5602, loss:9.903239250183105\n",
      "训练次数：5603, loss:9.04954719543457\n",
      "训练次数：5604, loss:8.329346656799316\n",
      "训练次数：5605, loss:13.874945640563965\n",
      "训练次数：5606, loss:11.142939567565918\n",
      "训练次数：5607, loss:11.432252883911133\n",
      "训练次数：5608, loss:7.582093238830566\n",
      "训练次数：5609, loss:10.92401123046875\n",
      "训练次数：5610, loss:10.388309478759766\n",
      "训练次数：5611, loss:15.778188705444336\n",
      "训练次数：5612, loss:17.765819549560547\n",
      "训练次数：5613, loss:9.989744186401367\n",
      "训练次数：5614, loss:13.171724319458008\n",
      "训练次数：5615, loss:15.335914611816406\n",
      "训练次数：5616, loss:10.661643028259277\n",
      "训练次数：5617, loss:14.047955513000488\n",
      "训练次数：5618, loss:9.228748321533203\n",
      "训练次数：5619, loss:10.905425071716309\n",
      "训练次数：5620, loss:11.58332347869873\n",
      "训练次数：5621, loss:11.348883628845215\n",
      "训练次数：5622, loss:12.220351219177246\n",
      "训练次数：5623, loss:8.955260276794434\n",
      "训练次数：5624, loss:8.784613609313965\n",
      "训练次数：5625, loss:11.050619125366211\n",
      "训练次数：5626, loss:9.172626495361328\n",
      "训练次数：5627, loss:11.124387741088867\n",
      "训练次数：5628, loss:8.816259384155273\n",
      "训练次数：5629, loss:10.946290969848633\n",
      "训练次数：5630, loss:9.08643913269043\n",
      "训练次数：5631, loss:10.74353313446045\n",
      "训练次数：5632, loss:9.148615837097168\n",
      "训练次数：5633, loss:9.312064170837402\n",
      "训练次数：5634, loss:10.158002853393555\n",
      "训练次数：5635, loss:11.141919136047363\n",
      "训练次数：5636, loss:9.698555946350098\n",
      "训练次数：5637, loss:9.00169563293457\n",
      "训练次数：5638, loss:15.807500839233398\n",
      "训练次数：5639, loss:12.159867286682129\n",
      "训练次数：5640, loss:7.641201972961426\n",
      "训练次数：5641, loss:11.565178871154785\n",
      "训练次数：5642, loss:7.026366233825684\n",
      "训练次数：5643, loss:8.227670669555664\n",
      "训练次数：5644, loss:9.546947479248047\n",
      "训练次数：5645, loss:9.830425262451172\n",
      "训练次数：5646, loss:10.058497428894043\n",
      "训练次数：5647, loss:8.377298355102539\n",
      "训练次数：5648, loss:8.30752182006836\n",
      "训练次数：5649, loss:8.522710800170898\n",
      "训练次数：5650, loss:6.570950984954834\n",
      "训练次数：5651, loss:9.156448364257812\n",
      "训练次数：5652, loss:11.887067794799805\n",
      "训练次数：5653, loss:10.449317932128906\n",
      "训练次数：5654, loss:11.109505653381348\n",
      "训练次数：5655, loss:9.662435531616211\n",
      "训练次数：5656, loss:10.23190689086914\n",
      "训练次数：5657, loss:9.342412948608398\n",
      "训练次数：5658, loss:7.684708118438721\n",
      "训练次数：5659, loss:7.763862133026123\n",
      "训练次数：5660, loss:9.0325288772583\n",
      "训练次数：5661, loss:7.509232044219971\n",
      "训练次数：5662, loss:9.137344360351562\n",
      "训练次数：5663, loss:8.750541687011719\n",
      "训练次数：5664, loss:9.447718620300293\n",
      "训练次数：5665, loss:5.232029914855957\n",
      "训练次数：5666, loss:7.7695417404174805\n",
      "训练次数：5667, loss:8.948095321655273\n",
      "训练次数：5668, loss:8.636871337890625\n",
      "训练次数：5669, loss:6.968976974487305\n",
      "训练次数：5670, loss:9.01939868927002\n",
      "训练次数：5671, loss:6.997012615203857\n",
      "训练次数：5672, loss:9.786223411560059\n",
      "训练次数：5673, loss:8.937515258789062\n",
      "训练次数：5674, loss:8.813054084777832\n",
      "训练次数：5675, loss:6.476007461547852\n",
      "训练次数：5676, loss:9.461570739746094\n",
      "训练次数：5677, loss:13.128703117370605\n",
      "训练次数：5678, loss:9.921294212341309\n",
      "训练次数：5679, loss:9.166913032531738\n",
      "训练次数：5680, loss:12.787535667419434\n",
      "训练次数：5681, loss:9.706670761108398\n",
      "训练次数：5682, loss:8.765971183776855\n",
      "训练次数：5683, loss:11.089521408081055\n",
      "训练次数：5684, loss:11.964093208312988\n",
      "训练次数：5685, loss:11.156469345092773\n",
      "训练次数：5686, loss:11.440925598144531\n",
      "训练次数：5687, loss:10.836349487304688\n",
      "训练次数：5688, loss:9.906871795654297\n",
      "训练次数：5689, loss:12.59677505493164\n",
      "训练次数：5690, loss:12.913110733032227\n",
      "训练次数：5691, loss:7.223139762878418\n",
      "训练次数：5692, loss:8.830217361450195\n",
      "训练次数：5693, loss:9.615307807922363\n",
      "训练次数：5694, loss:7.4773945808410645\n",
      "训练次数：5695, loss:7.5792975425720215\n",
      "训练次数：5696, loss:9.419018745422363\n",
      "训练次数：5697, loss:10.104915618896484\n",
      "训练次数：5698, loss:9.219882011413574\n",
      "训练次数：5699, loss:8.413294792175293\n",
      "训练次数：5700, loss:8.47922420501709\n",
      "训练次数：5701, loss:7.120434761047363\n",
      "训练次数：5702, loss:7.355713844299316\n",
      "训练次数：5703, loss:11.211748123168945\n",
      "训练次数：5704, loss:9.090145111083984\n",
      "训练次数：5705, loss:9.144899368286133\n",
      "训练次数：5706, loss:10.38700008392334\n",
      "训练次数：5707, loss:11.348315238952637\n",
      "训练次数：5708, loss:9.275386810302734\n",
      "训练次数：5709, loss:9.547811508178711\n",
      "训练次数：5710, loss:9.566274642944336\n",
      "训练次数：5711, loss:10.56448745727539\n",
      "训练次数：5712, loss:9.44277286529541\n",
      "训练次数：5713, loss:7.560964107513428\n",
      "训练次数：5714, loss:8.873602867126465\n",
      "训练次数：5715, loss:9.891040802001953\n",
      "训练次数：5716, loss:7.207983493804932\n",
      "训练次数：5717, loss:9.581640243530273\n",
      "训练次数：5718, loss:11.703398704528809\n",
      "训练次数：5719, loss:7.535299301147461\n",
      "训练次数：5720, loss:10.142210960388184\n",
      "训练次数：5721, loss:8.241365432739258\n",
      "训练次数：5722, loss:9.471722602844238\n",
      "训练次数：5723, loss:9.841367721557617\n",
      "训练次数：5724, loss:8.212300300598145\n",
      "训练次数：5725, loss:7.192018985748291\n",
      "训练次数：5726, loss:9.218790054321289\n",
      "训练次数：5727, loss:9.478726387023926\n",
      "训练次数：5728, loss:10.968572616577148\n",
      "训练次数：5729, loss:9.59951400756836\n",
      "训练次数：5730, loss:9.636903762817383\n",
      "训练次数：5731, loss:7.348874092102051\n",
      "训练次数：5732, loss:9.71652889251709\n",
      "训练次数：5733, loss:8.492105484008789\n",
      "训练次数：5734, loss:8.91998291015625\n",
      "训练次数：5735, loss:8.896271705627441\n",
      "训练次数：5736, loss:10.094757080078125\n",
      "训练次数：5737, loss:9.90571117401123\n",
      "训练次数：5738, loss:9.245335578918457\n",
      "训练次数：5739, loss:6.629401683807373\n",
      "训练次数：5740, loss:9.081659317016602\n",
      "训练次数：5741, loss:10.404102325439453\n",
      "训练次数：5742, loss:8.228649139404297\n",
      "训练次数：5743, loss:9.167600631713867\n",
      "训练次数：5744, loss:6.345935344696045\n",
      "训练次数：5745, loss:9.476865768432617\n",
      "训练次数：5746, loss:8.042696952819824\n",
      "训练次数：5747, loss:6.494049549102783\n",
      "训练次数：5748, loss:9.224150657653809\n",
      "训练次数：5749, loss:8.582664489746094\n",
      "训练次数：5750, loss:8.337267875671387\n",
      "训练次数：5751, loss:10.333608627319336\n",
      "训练次数：5752, loss:7.244034767150879\n",
      "训练次数：5753, loss:10.664024353027344\n",
      "训练次数：5754, loss:9.400863647460938\n",
      "训练次数：5755, loss:11.145501136779785\n",
      "训练次数：5756, loss:8.175408363342285\n",
      "训练次数：5757, loss:6.275180816650391\n",
      "训练次数：5758, loss:8.73654556274414\n",
      "训练次数：5759, loss:6.0446014404296875\n",
      "训练次数：5760, loss:9.846963882446289\n",
      "训练次数：5761, loss:8.155980110168457\n",
      "训练次数：5762, loss:9.470131874084473\n",
      "训练次数：5763, loss:10.796236038208008\n",
      "训练次数：5764, loss:9.370898246765137\n",
      "训练次数：5765, loss:7.784045696258545\n",
      "训练次数：5766, loss:9.399956703186035\n",
      "训练次数：5767, loss:10.911718368530273\n",
      "训练次数：5768, loss:9.495572090148926\n",
      "训练次数：5769, loss:8.696316719055176\n",
      "训练次数：5770, loss:6.115784645080566\n",
      "训练次数：5771, loss:10.055249214172363\n",
      "训练次数：5772, loss:8.419513702392578\n",
      "训练次数：5773, loss:6.197320938110352\n",
      "训练次数：5774, loss:7.699305534362793\n",
      "训练次数：5775, loss:5.978514194488525\n",
      "训练次数：5776, loss:8.971466064453125\n",
      "训练次数：5777, loss:9.117791175842285\n",
      "训练次数：5778, loss:6.532174110412598\n",
      "训练次数：5779, loss:7.3675384521484375\n",
      "训练次数：5780, loss:11.300300598144531\n",
      "训练次数：5781, loss:7.213197231292725\n",
      "训练次数：5782, loss:7.066554069519043\n",
      "训练次数：5783, loss:10.217494010925293\n",
      "训练次数：5784, loss:11.503416061401367\n",
      "训练次数：5785, loss:8.298616409301758\n",
      "训练次数：5786, loss:9.822790145874023\n",
      "训练次数：5787, loss:9.324935913085938\n",
      "训练次数：5788, loss:8.348316192626953\n",
      "训练次数：5789, loss:13.698884010314941\n",
      "训练次数：5790, loss:8.56778621673584\n",
      "训练次数：5791, loss:7.891477108001709\n",
      "训练次数：5792, loss:7.045318126678467\n",
      "训练次数：5793, loss:8.945032119750977\n",
      "训练次数：5794, loss:9.175189971923828\n",
      "训练次数：5795, loss:9.071290969848633\n",
      "训练次数：5796, loss:7.589193344116211\n",
      "训练次数：5797, loss:6.892997741699219\n",
      "训练次数：5798, loss:9.00579833984375\n",
      "训练次数：5799, loss:7.917642116546631\n",
      "训练次数：5800, loss:6.700375080108643\n",
      "训练次数：5801, loss:7.27373743057251\n",
      "训练次数：5802, loss:7.118781566619873\n",
      "训练次数：5803, loss:9.042966842651367\n",
      "训练次数：5804, loss:7.6229987144470215\n",
      "训练次数：5805, loss:8.131793975830078\n",
      "训练次数：5806, loss:7.7653398513793945\n",
      "训练次数：5807, loss:6.1698994636535645\n",
      "训练次数：5808, loss:7.694516181945801\n",
      "训练次数：5809, loss:12.100567817687988\n",
      "训练次数：5810, loss:14.57170581817627\n",
      "训练次数：5811, loss:8.48772144317627\n",
      "训练次数：5812, loss:7.494697570800781\n",
      "训练次数：5813, loss:8.889630317687988\n",
      "训练次数：5814, loss:7.383622169494629\n",
      "训练次数：5815, loss:8.581673622131348\n",
      "训练次数：5816, loss:10.37399673461914\n",
      "训练次数：5817, loss:7.656957626342773\n",
      "训练次数：5818, loss:7.304905414581299\n",
      "训练次数：5819, loss:8.036116600036621\n",
      "训练次数：5820, loss:7.255472183227539\n",
      "训练次数：5821, loss:9.481411933898926\n",
      "训练次数：5822, loss:7.413612365722656\n",
      "训练次数：5823, loss:6.446012496948242\n",
      "训练次数：5824, loss:6.335275173187256\n",
      "训练次数：5825, loss:7.087640285491943\n",
      "训练次数：5826, loss:11.430920600891113\n",
      "训练次数：5827, loss:7.070215225219727\n",
      "训练次数：5828, loss:7.355611324310303\n",
      "训练次数：5829, loss:7.070846080780029\n",
      "训练次数：5830, loss:9.283585548400879\n",
      "训练次数：5831, loss:8.148168563842773\n",
      "训练次数：5832, loss:8.733475685119629\n",
      "训练次数：5833, loss:6.128065586090088\n",
      "训练次数：5834, loss:6.850879192352295\n",
      "训练次数：5835, loss:12.318595886230469\n",
      "训练次数：5836, loss:6.11685848236084\n",
      "训练次数：5837, loss:7.319039344787598\n",
      "训练次数：5838, loss:7.538764953613281\n",
      "训练次数：5839, loss:7.39402961730957\n",
      "训练次数：5840, loss:7.779129981994629\n",
      "训练次数：5841, loss:9.50686264038086\n",
      "训练次数：5842, loss:8.840312004089355\n",
      "训练次数：5843, loss:6.64606237411499\n",
      "训练次数：5844, loss:9.169209480285645\n",
      "训练次数：5845, loss:8.239592552185059\n",
      "训练次数：5846, loss:8.075079917907715\n",
      "训练次数：5847, loss:7.400089263916016\n",
      "训练次数：5848, loss:7.67160177230835\n",
      "训练次数：5849, loss:9.99726676940918\n",
      "训练次数：5850, loss:6.078371047973633\n",
      "训练次数：5851, loss:8.247074127197266\n",
      "训练次数：5852, loss:10.093978881835938\n",
      "训练次数：5853, loss:9.837592124938965\n",
      "训练次数：5854, loss:9.062613487243652\n",
      "训练次数：5855, loss:8.441556930541992\n",
      "训练次数：5856, loss:8.979384422302246\n",
      "训练次数：5857, loss:7.3206281661987305\n",
      "训练次数：5858, loss:8.3793363571167\n",
      "训练次数：5859, loss:7.704630374908447\n",
      "训练次数：5860, loss:6.329977035522461\n",
      "训练次数：5861, loss:9.382918357849121\n",
      "训练次数：5862, loss:7.6679534912109375\n",
      "训练次数：5863, loss:8.641854286193848\n",
      "训练次数：5864, loss:6.890608310699463\n",
      "训练次数：5865, loss:7.8803534507751465\n",
      "训练次数：5866, loss:4.37256383895874\n",
      "训练次数：5867, loss:7.66613245010376\n",
      "训练次数：5868, loss:10.412318229675293\n",
      "训练次数：5869, loss:9.646644592285156\n",
      "训练次数：5870, loss:8.162663459777832\n",
      "训练次数：5871, loss:10.341277122497559\n",
      "训练次数：5872, loss:8.351921081542969\n",
      "训练次数：5873, loss:7.743320465087891\n",
      "训练次数：5874, loss:10.665572166442871\n",
      "训练次数：5875, loss:9.81470012664795\n",
      "训练次数：5876, loss:9.466740608215332\n",
      "训练次数：5877, loss:9.312943458557129\n",
      "训练次数：5878, loss:11.542879104614258\n",
      "训练次数：5879, loss:10.874251365661621\n",
      "训练次数：5880, loss:8.10024356842041\n",
      "训练次数：5881, loss:10.464625358581543\n",
      "训练次数：5882, loss:4.698054790496826\n",
      "训练次数：5883, loss:8.9279203414917\n",
      "训练次数：5884, loss:9.053121566772461\n",
      "训练次数：5885, loss:9.532219886779785\n",
      "训练次数：5886, loss:6.99181604385376\n",
      "训练次数：5887, loss:6.782919406890869\n",
      "训练次数：5888, loss:8.896675109863281\n",
      "训练次数：5889, loss:11.007603645324707\n",
      "训练次数：5890, loss:8.197395324707031\n",
      "训练次数：5891, loss:8.068124771118164\n",
      "训练次数：5892, loss:7.857580661773682\n",
      "训练次数：5893, loss:8.423815727233887\n",
      "训练次数：5894, loss:7.051874160766602\n",
      "训练次数：5895, loss:9.869606018066406\n",
      "训练次数：5896, loss:5.812129020690918\n",
      "训练次数：5897, loss:10.041946411132812\n",
      "训练次数：5898, loss:7.67489767074585\n",
      "训练次数：5899, loss:6.507203102111816\n",
      "训练次数：5900, loss:7.666802406311035\n",
      "训练次数：5901, loss:10.249253273010254\n",
      "训练次数：5902, loss:5.3392133712768555\n",
      "训练次数：5903, loss:7.129201889038086\n",
      "训练次数：5904, loss:6.813656806945801\n",
      "训练次数：5905, loss:8.46419906616211\n",
      "训练次数：5906, loss:5.386578559875488\n",
      "训练次数：5907, loss:6.895996570587158\n",
      "训练次数：5908, loss:5.820858955383301\n",
      "训练次数：5909, loss:9.002156257629395\n",
      "训练次数：5910, loss:8.080446243286133\n",
      "训练次数：5911, loss:7.369958877563477\n",
      "训练次数：5912, loss:8.318499565124512\n",
      "训练次数：5913, loss:7.113652229309082\n",
      "训练次数：5914, loss:6.27197265625\n",
      "训练次数：5915, loss:8.613322257995605\n",
      "训练次数：5916, loss:8.536691665649414\n",
      "训练次数：5917, loss:6.9490838050842285\n",
      "训练次数：5918, loss:5.424407482147217\n",
      "训练次数：5919, loss:9.362480163574219\n",
      "训练次数：5920, loss:6.815122127532959\n",
      "训练次数：5921, loss:9.284727096557617\n",
      "训练次数：5922, loss:6.225338459014893\n",
      "训练次数：5923, loss:7.63950777053833\n",
      "训练次数：5924, loss:8.230632781982422\n",
      "训练次数：5925, loss:5.654914855957031\n",
      "训练次数：5926, loss:4.132286071777344\n",
      "训练次数：5927, loss:7.650792598724365\n",
      "训练次数：5928, loss:6.416965484619141\n",
      "训练次数：5929, loss:6.2399067878723145\n",
      "训练次数：5930, loss:6.9586310386657715\n",
      "训练次数：5931, loss:9.077520370483398\n",
      "训练次数：5932, loss:6.106294631958008\n",
      "训练次数：5933, loss:7.021684646606445\n",
      "训练次数：5934, loss:5.876070022583008\n",
      "训练次数：5935, loss:8.771178245544434\n",
      "训练次数：5936, loss:7.061498165130615\n",
      "训练次数：5937, loss:6.157508850097656\n",
      "训练次数：5938, loss:8.764698028564453\n",
      "训练次数：5939, loss:7.851327896118164\n",
      "训练次数：5940, loss:5.236589431762695\n",
      "训练次数：5941, loss:6.841750621795654\n",
      "训练次数：5942, loss:6.134425163269043\n",
      "训练次数：5943, loss:6.909591197967529\n",
      "训练次数：5944, loss:9.86666488647461\n",
      "训练次数：5945, loss:6.135547161102295\n",
      "训练次数：5946, loss:6.573470115661621\n",
      "训练次数：5947, loss:5.559412002563477\n",
      "训练次数：5948, loss:6.785626411437988\n",
      "训练次数：5949, loss:6.013056755065918\n",
      "训练次数：5950, loss:8.172311782836914\n",
      "训练次数：5951, loss:8.595122337341309\n",
      "训练次数：5952, loss:7.573748588562012\n",
      "训练次数：5953, loss:8.03719711303711\n",
      "训练次数：5954, loss:6.187267780303955\n",
      "训练次数：5955, loss:7.8038763999938965\n",
      "训练次数：5956, loss:5.421294212341309\n",
      "训练次数：5957, loss:6.485398292541504\n",
      "训练次数：5958, loss:6.5786027908325195\n",
      "训练次数：5959, loss:7.220014572143555\n",
      "训练次数：5960, loss:5.529508590698242\n",
      "训练次数：5961, loss:8.520594596862793\n",
      "训练次数：5962, loss:8.59525203704834\n",
      "训练次数：5963, loss:8.31994915008545\n",
      "训练次数：5964, loss:5.955763339996338\n",
      "训练次数：5965, loss:8.815701484680176\n",
      "训练次数：5966, loss:8.382095336914062\n",
      "训练次数：5967, loss:5.940158843994141\n",
      "训练次数：5968, loss:5.247244358062744\n",
      "训练次数：5969, loss:6.092987060546875\n",
      "训练次数：5970, loss:9.224420547485352\n",
      "训练次数：5971, loss:6.603835105895996\n",
      "训练次数：5972, loss:5.958530902862549\n",
      "训练次数：5973, loss:7.8392157554626465\n",
      "训练次数：5974, loss:5.366975784301758\n",
      "训练次数：5975, loss:6.116844654083252\n",
      "训练次数：5976, loss:6.36487340927124\n",
      "训练次数：5977, loss:7.6322021484375\n",
      "训练次数：5978, loss:7.7259440422058105\n",
      "训练次数：5979, loss:8.701552391052246\n",
      "训练次数：5980, loss:8.3691987991333\n",
      "训练次数：5981, loss:6.3904008865356445\n",
      "训练次数：5982, loss:7.6778883934021\n",
      "训练次数：5983, loss:7.370169639587402\n",
      "训练次数：5984, loss:8.294506072998047\n",
      "训练次数：5985, loss:7.747348308563232\n",
      "训练次数：5986, loss:6.930954933166504\n",
      "训练次数：5987, loss:6.636592388153076\n",
      "训练次数：5988, loss:7.390162944793701\n",
      "训练次数：5989, loss:5.922757625579834\n",
      "训练次数：5990, loss:5.389925003051758\n",
      "训练次数：5991, loss:9.885661125183105\n",
      "训练次数：5992, loss:7.560464859008789\n",
      "训练次数：5993, loss:6.57746696472168\n",
      "训练次数：5994, loss:4.659042835235596\n",
      "训练次数：5995, loss:7.528409004211426\n",
      "训练次数：5996, loss:8.188488960266113\n",
      "训练次数：5997, loss:6.145328998565674\n",
      "训练次数：5998, loss:8.048684120178223\n",
      "训练次数：5999, loss:6.914221286773682\n",
      "训练次数：6000, loss:9.289078712463379\n",
      "训练次数：6001, loss:6.060732841491699\n",
      "训练次数：6002, loss:5.606076717376709\n",
      "训练次数：6003, loss:7.410392761230469\n",
      "训练次数：6004, loss:8.599628448486328\n",
      "训练次数：6005, loss:7.785560131072998\n",
      "训练次数：6006, loss:7.452911853790283\n",
      "训练次数：6007, loss:6.017615795135498\n",
      "训练次数：6008, loss:4.540804386138916\n",
      "训练次数：6009, loss:6.027403831481934\n",
      "训练次数：6010, loss:4.9641642570495605\n",
      "训练次数：6011, loss:6.7076897621154785\n",
      "训练次数：6012, loss:6.568684101104736\n",
      "训练次数：6013, loss:6.802044868469238\n",
      "训练次数：6014, loss:5.907097339630127\n",
      "训练次数：6015, loss:6.161965370178223\n",
      "训练次数：6016, loss:9.579094886779785\n",
      "训练次数：6017, loss:6.014766216278076\n",
      "训练次数：6018, loss:5.97973108291626\n",
      "训练次数：6019, loss:6.83445930480957\n",
      "训练次数：6020, loss:7.779053688049316\n",
      "训练次数：6021, loss:7.8851542472839355\n",
      "训练次数：6022, loss:7.4247355461120605\n",
      "训练次数：6023, loss:6.913141250610352\n",
      "训练次数：6024, loss:6.868201732635498\n",
      "训练次数：6025, loss:5.725527286529541\n",
      "训练次数：6026, loss:10.32430362701416\n",
      "训练次数：6027, loss:5.514349937438965\n",
      "训练次数：6028, loss:7.763670921325684\n",
      "训练次数：6029, loss:5.85016393661499\n",
      "训练次数：6030, loss:7.314267635345459\n",
      "训练次数：6031, loss:9.07184886932373\n",
      "训练次数：6032, loss:7.6684980392456055\n",
      "训练次数：6033, loss:9.446724891662598\n",
      "训练次数：6034, loss:7.236629009246826\n",
      "训练次数：6035, loss:4.910433769226074\n",
      "训练次数：6036, loss:7.174665927886963\n",
      "训练次数：6037, loss:8.941190719604492\n",
      "训练次数：6038, loss:6.937558650970459\n",
      "训练次数：6039, loss:7.015920639038086\n",
      "训练次数：6040, loss:7.915305137634277\n",
      "训练次数：6041, loss:7.434421539306641\n",
      "训练次数：6042, loss:7.4174065589904785\n",
      "训练次数：6043, loss:7.323176860809326\n",
      "训练次数：6044, loss:5.593801021575928\n",
      "训练次数：6045, loss:8.9187593460083\n",
      "训练次数：6046, loss:6.7754693031311035\n",
      "训练次数：6047, loss:5.889820575714111\n",
      "训练次数：6048, loss:7.203612327575684\n",
      "训练次数：6049, loss:7.96533203125\n",
      "训练次数：6050, loss:6.000729084014893\n",
      "训练次数：6051, loss:6.6694111824035645\n",
      "训练次数：6052, loss:9.593724250793457\n",
      "训练次数：6053, loss:5.535189628601074\n",
      "训练次数：6054, loss:8.752925872802734\n",
      "训练次数：6055, loss:6.137511253356934\n",
      "训练次数：6056, loss:7.344849586486816\n",
      "训练次数：6057, loss:7.039931774139404\n",
      "训练次数：6058, loss:6.037595272064209\n",
      "训练次数：6059, loss:8.633872985839844\n",
      "训练次数：6060, loss:6.2161359786987305\n",
      "训练次数：6061, loss:7.3177313804626465\n",
      "训练次数：6062, loss:4.785406112670898\n",
      "训练次数：6063, loss:7.654224395751953\n",
      "训练次数：6064, loss:7.234835624694824\n",
      "训练次数：6065, loss:6.660950660705566\n",
      "训练次数：6066, loss:5.095637321472168\n",
      "训练次数：6067, loss:6.542985916137695\n",
      "训练次数：6068, loss:7.7241926193237305\n",
      "训练次数：6069, loss:8.77646541595459\n",
      "训练次数：6070, loss:10.758296966552734\n",
      "训练次数：6071, loss:7.053951263427734\n",
      "训练次数：6072, loss:7.4543137550354\n",
      "训练次数：6073, loss:5.611477375030518\n",
      "训练次数：6074, loss:6.99010705947876\n",
      "训练次数：6075, loss:6.306794166564941\n",
      "训练次数：6076, loss:5.579428672790527\n",
      "训练次数：6077, loss:6.498409271240234\n",
      "训练次数：6078, loss:9.387625694274902\n",
      "训练次数：6079, loss:9.275716781616211\n",
      "训练次数：6080, loss:4.950631618499756\n",
      "训练次数：6081, loss:5.663573265075684\n",
      "训练次数：6082, loss:8.029706001281738\n",
      "训练次数：6083, loss:6.928168773651123\n",
      "训练次数：6084, loss:6.62455940246582\n",
      "训练次数：6085, loss:6.766007900238037\n",
      "训练次数：6086, loss:7.870532035827637\n",
      "训练次数：6087, loss:6.941561698913574\n",
      "训练次数：6088, loss:6.6747894287109375\n",
      "训练次数：6089, loss:8.622209548950195\n",
      "训练次数：6090, loss:6.253901958465576\n",
      "训练次数：6091, loss:9.302286148071289\n",
      "训练次数：6092, loss:6.648024559020996\n",
      "训练次数：6093, loss:7.162781715393066\n",
      "训练次数：6094, loss:6.585278511047363\n",
      "训练次数：6095, loss:7.989095687866211\n",
      "训练次数：6096, loss:10.796671867370605\n",
      "训练次数：6097, loss:6.323841571807861\n",
      "训练次数：6098, loss:7.3906168937683105\n",
      "训练次数：6099, loss:7.768057346343994\n",
      "训练次数：6100, loss:6.317043781280518\n",
      "训练次数：6101, loss:7.4248175621032715\n",
      "训练次数：6102, loss:7.888434410095215\n",
      "训练次数：6103, loss:6.49530553817749\n",
      "训练次数：6104, loss:5.5428595542907715\n",
      "训练次数：6105, loss:6.526148796081543\n",
      "训练次数：6106, loss:9.484550476074219\n",
      "训练次数：6107, loss:7.368056297302246\n",
      "训练次数：6108, loss:5.87839412689209\n",
      "训练次数：6109, loss:6.363909721374512\n",
      "训练次数：6110, loss:5.559784889221191\n",
      "训练次数：6111, loss:7.109673976898193\n",
      "训练次数：6112, loss:8.565230369567871\n",
      "训练次数：6113, loss:6.424545764923096\n",
      "训练次数：6114, loss:6.4605584144592285\n",
      "训练次数：6115, loss:9.238199234008789\n",
      "训练次数：6116, loss:8.186208724975586\n",
      "训练次数：6117, loss:6.142963409423828\n",
      "训练次数：6118, loss:5.715231895446777\n",
      "训练次数：6119, loss:8.885673522949219\n",
      "训练次数：6120, loss:7.5575737953186035\n",
      "训练次数：6121, loss:8.314607620239258\n",
      "训练次数：6122, loss:6.336601257324219\n",
      "训练次数：6123, loss:8.159985542297363\n",
      "训练次数：6124, loss:7.967093467712402\n",
      "训练次数：6125, loss:5.627570629119873\n",
      "训练次数：6126, loss:5.214011192321777\n",
      "训练次数：6127, loss:4.169336795806885\n",
      "训练次数：6128, loss:5.4585113525390625\n",
      "训练次数：6129, loss:7.902217388153076\n",
      "训练次数：6130, loss:6.30381965637207\n",
      "训练次数：6131, loss:4.3252787590026855\n",
      "训练次数：6132, loss:4.9521284103393555\n",
      "训练次数：6133, loss:6.532034873962402\n",
      "训练次数：6134, loss:4.897575855255127\n",
      "训练次数：6135, loss:6.102336406707764\n",
      "训练次数：6136, loss:4.084591865539551\n",
      "训练次数：6137, loss:6.366607666015625\n",
      "训练次数：6138, loss:5.232126712799072\n",
      "训练次数：6139, loss:7.611039638519287\n",
      "训练次数：6140, loss:7.403763771057129\n",
      "训练次数：6141, loss:5.5373406410217285\n",
      "训练次数：6142, loss:8.383679389953613\n",
      "训练次数：6143, loss:8.21984577178955\n",
      "训练次数：6144, loss:8.176446914672852\n",
      "训练次数：6145, loss:7.9142351150512695\n",
      "训练次数：6146, loss:6.8010358810424805\n",
      "训练次数：6147, loss:7.847367286682129\n",
      "训练次数：6148, loss:7.436456203460693\n",
      "训练次数：6149, loss:8.797094345092773\n",
      "训练次数：6150, loss:7.463165760040283\n",
      "训练次数：6151, loss:7.2357354164123535\n",
      "训练次数：6152, loss:8.027050971984863\n",
      "训练次数：6153, loss:5.573452472686768\n",
      "训练次数：6154, loss:7.696752071380615\n",
      "训练次数：6155, loss:9.33697509765625\n",
      "训练次数：6156, loss:3.837454080581665\n",
      "训练次数：6157, loss:5.769678115844727\n",
      "训练次数：6158, loss:7.636557579040527\n",
      "训练次数：6159, loss:7.684909820556641\n",
      "训练次数：6160, loss:7.6258463859558105\n",
      "训练次数：6161, loss:5.407679557800293\n",
      "训练次数：6162, loss:3.8092715740203857\n",
      "训练次数：6163, loss:4.93947172164917\n",
      "训练次数：6164, loss:9.784002304077148\n",
      "训练次数：6165, loss:7.844876289367676\n",
      "训练次数：6166, loss:7.110063076019287\n",
      "训练次数：6167, loss:7.1755452156066895\n",
      "训练次数：6168, loss:5.5318732261657715\n",
      "训练次数：6169, loss:7.438803672790527\n",
      "训练次数：6170, loss:6.049393177032471\n",
      "训练次数：6171, loss:5.6636786460876465\n",
      "训练次数：6172, loss:6.252889633178711\n",
      "训练次数：6173, loss:4.428333282470703\n",
      "训练次数：6174, loss:6.913354396820068\n",
      "训练次数：6175, loss:6.221236228942871\n",
      "训练次数：6176, loss:7.045079708099365\n",
      "训练次数：6177, loss:6.005218029022217\n",
      "训练次数：6178, loss:5.690332889556885\n",
      "训练次数：6179, loss:7.245283126831055\n",
      "训练次数：6180, loss:6.790307521820068\n",
      "训练次数：6181, loss:6.323211669921875\n",
      "训练次数：6182, loss:5.049924850463867\n",
      "训练次数：6183, loss:6.54838752746582\n",
      "训练次数：6184, loss:7.901830196380615\n",
      "训练次数：6185, loss:5.373065948486328\n",
      "训练次数：6186, loss:4.75181245803833\n",
      "训练次数：6187, loss:5.172547817230225\n",
      "训练次数：6188, loss:7.99669075012207\n",
      "训练次数：6189, loss:6.1685566902160645\n",
      "训练次数：6190, loss:6.701897144317627\n",
      "训练次数：6191, loss:6.020415782928467\n",
      "训练次数：6192, loss:7.059550762176514\n",
      "训练次数：6193, loss:5.506932735443115\n",
      "训练次数：6194, loss:6.257728099822998\n",
      "训练次数：6195, loss:5.94401741027832\n",
      "训练次数：6196, loss:6.185116291046143\n",
      "训练次数：6197, loss:4.3648810386657715\n",
      "训练次数：6198, loss:6.430886745452881\n",
      "训练次数：6199, loss:7.726254940032959\n",
      "训练次数：6200, loss:5.680611610412598\n",
      "训练次数：6201, loss:6.423113822937012\n",
      "训练次数：6202, loss:9.168781280517578\n",
      "训练次数：6203, loss:6.317488193511963\n",
      "训练次数：6204, loss:6.471674919128418\n",
      "训练次数：6205, loss:3.8438711166381836\n",
      "训练次数：6206, loss:6.110344409942627\n",
      "训练次数：6207, loss:4.523428440093994\n",
      "训练次数：6208, loss:6.353221893310547\n",
      "训练次数：6209, loss:5.960902690887451\n",
      "训练次数：6210, loss:5.067318916320801\n",
      "训练次数：6211, loss:4.368089199066162\n",
      "训练次数：6212, loss:6.204253673553467\n",
      "训练次数：6213, loss:5.395215034484863\n",
      "训练次数：6214, loss:4.829652786254883\n",
      "训练次数：6215, loss:4.912491798400879\n",
      "训练次数：6216, loss:5.599484443664551\n",
      "训练次数：6217, loss:7.424349308013916\n",
      "训练次数：6218, loss:5.199112892150879\n",
      "训练次数：6219, loss:3.866722583770752\n",
      "训练次数：6220, loss:5.825162410736084\n",
      "训练次数：6221, loss:6.049992084503174\n",
      "训练次数：6222, loss:6.787703037261963\n",
      "训练次数：6223, loss:6.477080345153809\n",
      "训练次数：6224, loss:5.760115623474121\n",
      "训练次数：6225, loss:7.545434474945068\n",
      "训练次数：6226, loss:6.228298187255859\n",
      "训练次数：6227, loss:5.14046573638916\n",
      "训练次数：6228, loss:6.102385997772217\n",
      "训练次数：6229, loss:5.692544937133789\n",
      "训练次数：6230, loss:4.981942176818848\n",
      "训练次数：6231, loss:8.130096435546875\n",
      "训练次数：6232, loss:5.302024841308594\n",
      "训练次数：6233, loss:5.156087398529053\n",
      "训练次数：6234, loss:6.090054035186768\n",
      "训练次数：6235, loss:6.676079750061035\n",
      "训练次数：6236, loss:4.978209972381592\n",
      "训练次数：6237, loss:5.28121280670166\n",
      "训练次数：6238, loss:5.70268440246582\n",
      "训练次数：6239, loss:8.646245002746582\n",
      "训练次数：6240, loss:6.211249828338623\n",
      "训练次数：6241, loss:5.893446922302246\n",
      "训练次数：6242, loss:6.062447547912598\n",
      "训练次数：6243, loss:4.072712421417236\n",
      "训练次数：6244, loss:6.7181396484375\n",
      "训练次数：6245, loss:5.692469120025635\n",
      "训练次数：6246, loss:6.603558540344238\n",
      "训练次数：6247, loss:6.508145332336426\n",
      "训练次数：6248, loss:6.184427261352539\n",
      "训练次数：6249, loss:5.684957981109619\n",
      "训练次数：6250, loss:6.210054397583008\n",
      "训练次数：6251, loss:5.972386837005615\n",
      "训练次数：6252, loss:5.709249019622803\n",
      "训练次数：6253, loss:5.125138282775879\n",
      "训练次数：6254, loss:5.054764270782471\n",
      "训练次数：6255, loss:9.389737129211426\n",
      "训练次数：6256, loss:8.26682186126709\n",
      "----------第8轮训练开始----------\n",
      "训练次数：6257, loss:9.069778442382812\n",
      "训练次数：6258, loss:5.68130350112915\n",
      "训练次数：6259, loss:5.663028240203857\n",
      "训练次数：6260, loss:5.23194694519043\n",
      "训练次数：6261, loss:5.922883033752441\n",
      "训练次数：6262, loss:5.857434272766113\n",
      "训练次数：6263, loss:4.547554016113281\n",
      "训练次数：6264, loss:5.913843631744385\n",
      "训练次数：6265, loss:4.917579174041748\n",
      "训练次数：6266, loss:7.263731956481934\n",
      "训练次数：6267, loss:8.253710746765137\n",
      "训练次数：6268, loss:5.310171604156494\n",
      "训练次数：6269, loss:6.509549140930176\n",
      "训练次数：6270, loss:6.862942218780518\n",
      "训练次数：6271, loss:4.921510219573975\n",
      "训练次数：6272, loss:4.945694923400879\n",
      "训练次数：6273, loss:6.574464321136475\n",
      "训练次数：6274, loss:4.927794456481934\n",
      "训练次数：6275, loss:4.922518730163574\n",
      "训练次数：6276, loss:5.072197914123535\n",
      "训练次数：6277, loss:5.50935697555542\n",
      "训练次数：6278, loss:4.705717086791992\n",
      "训练次数：6279, loss:5.644532203674316\n",
      "训练次数：6280, loss:7.102578163146973\n",
      "训练次数：6281, loss:5.321887493133545\n",
      "训练次数：6282, loss:7.8632283210754395\n",
      "训练次数：6283, loss:8.184550285339355\n",
      "训练次数：6284, loss:6.349262237548828\n",
      "训练次数：6285, loss:5.704055309295654\n",
      "训练次数：6286, loss:5.789032936096191\n",
      "训练次数：6287, loss:5.732171535491943\n",
      "训练次数：6288, loss:6.957501411437988\n",
      "训练次数：6289, loss:7.0784010887146\n",
      "训练次数：6290, loss:3.7422094345092773\n",
      "训练次数：6291, loss:6.036038398742676\n",
      "训练次数：6292, loss:5.34393310546875\n",
      "训练次数：6293, loss:5.847145080566406\n",
      "训练次数：6294, loss:5.804542541503906\n",
      "训练次数：6295, loss:5.517433166503906\n",
      "训练次数：6296, loss:7.08941125869751\n",
      "训练次数：6297, loss:5.034491539001465\n",
      "训练次数：6298, loss:5.534875869750977\n",
      "训练次数：6299, loss:6.7663726806640625\n",
      "训练次数：6300, loss:5.906854629516602\n",
      "训练次数：6301, loss:3.878175735473633\n",
      "训练次数：6302, loss:4.210225582122803\n",
      "训练次数：6303, loss:6.919108867645264\n",
      "训练次数：6304, loss:6.079244136810303\n",
      "训练次数：6305, loss:6.409582138061523\n",
      "训练次数：6306, loss:5.49700403213501\n",
      "训练次数：6307, loss:6.062780857086182\n",
      "训练次数：6308, loss:6.030547618865967\n",
      "训练次数：6309, loss:5.793585300445557\n",
      "训练次数：6310, loss:6.679994583129883\n",
      "训练次数：6311, loss:6.143649578094482\n",
      "训练次数：6312, loss:5.763090133666992\n",
      "训练次数：6313, loss:4.224295139312744\n",
      "训练次数：6314, loss:5.045149803161621\n",
      "训练次数：6315, loss:5.865856647491455\n",
      "训练次数：6316, loss:4.80440616607666\n",
      "训练次数：6317, loss:5.539175987243652\n",
      "训练次数：6318, loss:5.085441589355469\n",
      "训练次数：6319, loss:4.516418933868408\n",
      "训练次数：6320, loss:4.763855457305908\n",
      "训练次数：6321, loss:6.172512054443359\n",
      "训练次数：6322, loss:5.342363357543945\n",
      "训练次数：6323, loss:4.548602104187012\n",
      "训练次数：6324, loss:5.950212478637695\n",
      "训练次数：6325, loss:6.710407257080078\n",
      "训练次数：6326, loss:6.102440357208252\n",
      "训练次数：6327, loss:5.381921291351318\n",
      "训练次数：6328, loss:4.929965972900391\n",
      "训练次数：6329, loss:5.055877208709717\n",
      "训练次数：6330, loss:5.132814407348633\n",
      "训练次数：6331, loss:6.260808944702148\n",
      "训练次数：6332, loss:6.409657001495361\n",
      "训练次数：6333, loss:7.924598217010498\n",
      "训练次数：6334, loss:7.218465805053711\n",
      "训练次数：6335, loss:6.108415603637695\n",
      "训练次数：6336, loss:8.34520149230957\n",
      "训练次数：6337, loss:6.76564884185791\n",
      "训练次数：6338, loss:5.742313385009766\n",
      "训练次数：6339, loss:5.658194065093994\n",
      "训练次数：6340, loss:6.371888637542725\n",
      "训练次数：6341, loss:6.605663776397705\n",
      "训练次数：6342, loss:5.0286664962768555\n",
      "训练次数：6343, loss:6.613593101501465\n",
      "训练次数：6344, loss:4.647924423217773\n",
      "训练次数：6345, loss:5.503942012786865\n",
      "训练次数：6346, loss:5.480428218841553\n",
      "训练次数：6347, loss:7.238138675689697\n",
      "训练次数：6348, loss:5.492411136627197\n",
      "训练次数：6349, loss:5.884099006652832\n",
      "训练次数：6350, loss:4.981080532073975\n",
      "训练次数：6351, loss:4.854452610015869\n",
      "训练次数：6352, loss:5.734613418579102\n",
      "训练次数：6353, loss:4.699221611022949\n",
      "训练次数：6354, loss:6.21475076675415\n",
      "训练次数：6355, loss:5.6548662185668945\n",
      "训练次数：6356, loss:5.8850998878479\n",
      "训练次数：6357, loss:6.585783004760742\n",
      "训练次数：6358, loss:6.833438873291016\n",
      "训练次数：6359, loss:5.826471328735352\n",
      "训练次数：6360, loss:6.662888050079346\n",
      "训练次数：6361, loss:5.202264308929443\n",
      "训练次数：6362, loss:6.112062931060791\n",
      "训练次数：6363, loss:5.707851409912109\n",
      "训练次数：6364, loss:6.64072322845459\n",
      "训练次数：6365, loss:4.559857368469238\n",
      "训练次数：6366, loss:5.770256042480469\n",
      "训练次数：6367, loss:4.647881984710693\n",
      "训练次数：6368, loss:6.4091410636901855\n",
      "训练次数：6369, loss:4.808905601501465\n",
      "训练次数：6370, loss:6.364500999450684\n",
      "训练次数：6371, loss:5.578090667724609\n",
      "训练次数：6372, loss:6.809981346130371\n",
      "训练次数：6373, loss:6.221991539001465\n",
      "训练次数：6374, loss:5.4849324226379395\n",
      "训练次数：6375, loss:6.141421318054199\n",
      "训练次数：6376, loss:6.219057083129883\n",
      "训练次数：6377, loss:6.276941776275635\n",
      "训练次数：6378, loss:5.239574909210205\n",
      "训练次数：6379, loss:5.161132335662842\n",
      "训练次数：6380, loss:6.2078423500061035\n",
      "训练次数：6381, loss:5.614553928375244\n",
      "训练次数：6382, loss:6.140848159790039\n",
      "训练次数：6383, loss:5.761994361877441\n",
      "训练次数：6384, loss:4.679615020751953\n",
      "训练次数：6385, loss:4.223852634429932\n",
      "训练次数：6386, loss:5.327892780303955\n",
      "训练次数：6387, loss:5.96050500869751\n",
      "训练次数：6388, loss:4.349432945251465\n",
      "训练次数：6389, loss:6.773633003234863\n",
      "训练次数：6390, loss:3.5167396068573\n",
      "训练次数：6391, loss:3.652470111846924\n",
      "训练次数：6392, loss:5.408458709716797\n",
      "训练次数：6393, loss:7.033382892608643\n",
      "训练次数：6394, loss:6.602890491485596\n",
      "训练次数：6395, loss:5.563329219818115\n",
      "训练次数：6396, loss:6.127007007598877\n",
      "训练次数：6397, loss:7.99016809463501\n",
      "训练次数：6398, loss:5.405544757843018\n",
      "训练次数：6399, loss:5.327031135559082\n",
      "训练次数：6400, loss:5.085250377655029\n",
      "训练次数：6401, loss:5.911827087402344\n",
      "训练次数：6402, loss:5.011678695678711\n",
      "训练次数：6403, loss:7.076509475708008\n",
      "训练次数：6404, loss:5.521366596221924\n",
      "训练次数：6405, loss:4.349313259124756\n",
      "训练次数：6406, loss:4.866243362426758\n",
      "训练次数：6407, loss:5.836752414703369\n",
      "训练次数：6408, loss:4.866621971130371\n",
      "训练次数：6409, loss:5.846989154815674\n",
      "训练次数：6410, loss:4.381496906280518\n",
      "训练次数：6411, loss:5.339478015899658\n",
      "训练次数：6412, loss:5.525378704071045\n",
      "训练次数：6413, loss:6.30043888092041\n",
      "训练次数：6414, loss:4.555240154266357\n",
      "训练次数：6415, loss:4.424241065979004\n",
      "训练次数：6416, loss:6.372234344482422\n",
      "训练次数：6417, loss:6.7572712898254395\n",
      "训练次数：6418, loss:5.329431056976318\n",
      "训练次数：6419, loss:6.363144397735596\n",
      "训练次数：6420, loss:9.392865180969238\n",
      "训练次数：6421, loss:5.640925884246826\n",
      "训练次数：6422, loss:5.062512397766113\n",
      "训练次数：6423, loss:6.249182224273682\n",
      "训练次数：6424, loss:4.568292617797852\n",
      "训练次数：6425, loss:5.144512176513672\n",
      "训练次数：6426, loss:5.731686115264893\n",
      "训练次数：6427, loss:6.591598033905029\n",
      "训练次数：6428, loss:6.8043928146362305\n",
      "训练次数：6429, loss:5.647634506225586\n",
      "训练次数：6430, loss:6.39376163482666\n",
      "训练次数：6431, loss:5.42871618270874\n",
      "训练次数：6432, loss:4.331380844116211\n",
      "训练次数：6433, loss:5.572298526763916\n",
      "训练次数：6434, loss:7.226432800292969\n",
      "训练次数：6435, loss:7.51587438583374\n",
      "训练次数：6436, loss:6.821800231933594\n",
      "训练次数：6437, loss:6.045632362365723\n",
      "训练次数：6438, loss:6.120726108551025\n",
      "训练次数：6439, loss:5.892906665802002\n",
      "训练次数：6440, loss:4.469679832458496\n",
      "训练次数：6441, loss:5.09127140045166\n",
      "训练次数：6442, loss:6.6570611000061035\n",
      "训练次数：6443, loss:4.242439270019531\n",
      "训练次数：6444, loss:6.333294868469238\n",
      "训练次数：6445, loss:5.6442108154296875\n",
      "训练次数：6446, loss:7.121379852294922\n",
      "训练次数：6447, loss:3.4031834602355957\n",
      "训练次数：6448, loss:5.091339111328125\n",
      "训练次数：6449, loss:6.12514066696167\n",
      "训练次数：6450, loss:6.127905368804932\n",
      "训练次数：6451, loss:4.811264991760254\n",
      "训练次数：6452, loss:5.572351455688477\n",
      "训练次数：6453, loss:5.184521198272705\n",
      "训练次数：6454, loss:6.516454696655273\n",
      "训练次数：6455, loss:5.173989295959473\n",
      "训练次数：6456, loss:4.564520835876465\n",
      "训练次数：6457, loss:4.3285698890686035\n",
      "训练次数：6458, loss:5.644390106201172\n",
      "训练次数：6459, loss:6.914320945739746\n",
      "训练次数：6460, loss:6.763282775878906\n",
      "训练次数：6461, loss:5.3683319091796875\n",
      "训练次数：6462, loss:7.941116809844971\n",
      "训练次数：6463, loss:4.903042316436768\n",
      "训练次数：6464, loss:5.696035385131836\n",
      "训练次数：6465, loss:4.762962341308594\n",
      "训练次数：6466, loss:7.006807804107666\n",
      "训练次数：6467, loss:6.802816390991211\n",
      "训练次数：6468, loss:4.961700439453125\n",
      "训练次数：6469, loss:3.9624295234680176\n",
      "训练次数：6470, loss:5.989993572235107\n",
      "训练次数：6471, loss:7.484661102294922\n",
      "训练次数：6472, loss:5.970278739929199\n",
      "训练次数：6473, loss:4.827457427978516\n",
      "训练次数：6474, loss:4.721744060516357\n",
      "训练次数：6475, loss:3.5271077156066895\n",
      "训练次数：6476, loss:5.174293041229248\n",
      "训练次数：6477, loss:4.510213851928711\n",
      "训练次数：6478, loss:5.0954155921936035\n",
      "训练次数：6479, loss:5.0204596519470215\n",
      "训练次数：6480, loss:6.196897029876709\n",
      "训练次数：6481, loss:5.089599132537842\n",
      "训练次数：6482, loss:4.901243686676025\n",
      "训练次数：6483, loss:5.406765460968018\n",
      "训练次数：6484, loss:5.639820575714111\n",
      "训练次数：6485, loss:5.408658504486084\n",
      "训练次数：6486, loss:5.2445068359375\n",
      "训练次数：6487, loss:5.1162638664245605\n",
      "训练次数：6488, loss:5.226234436035156\n",
      "训练次数：6489, loss:6.142762184143066\n",
      "训练次数：6490, loss:5.024868488311768\n",
      "训练次数：6491, loss:4.543398380279541\n",
      "训练次数：6492, loss:6.034876346588135\n",
      "训练次数：6493, loss:6.264244079589844\n",
      "训练次数：6494, loss:5.499918460845947\n",
      "训练次数：6495, loss:3.8983170986175537\n",
      "训练次数：6496, loss:5.037398338317871\n",
      "训练次数：6497, loss:6.164336204528809\n",
      "训练次数：6498, loss:3.9284324645996094\n",
      "训练次数：6499, loss:6.68389892578125\n",
      "训练次数：6500, loss:6.2196221351623535\n",
      "训练次数：6501, loss:3.716918706893921\n",
      "训练次数：6502, loss:5.447114944458008\n",
      "训练次数：6503, loss:4.494604110717773\n",
      "训练次数：6504, loss:6.2132344245910645\n",
      "训练次数：6505, loss:5.483814239501953\n",
      "训练次数：6506, loss:5.076049327850342\n",
      "训练次数：6507, loss:3.956378936767578\n",
      "训练次数：6508, loss:5.317890167236328\n",
      "训练次数：6509, loss:5.655518054962158\n",
      "训练次数：6510, loss:7.612215518951416\n",
      "训练次数：6511, loss:6.281683444976807\n",
      "训练次数：6512, loss:5.097024440765381\n",
      "训练次数：6513, loss:3.7549281120300293\n",
      "训练次数：6514, loss:6.54986047744751\n",
      "训练次数：6515, loss:5.3930182456970215\n",
      "训练次数：6516, loss:6.196976184844971\n",
      "训练次数：6517, loss:5.396904945373535\n",
      "训练次数：6518, loss:5.82990837097168\n",
      "训练次数：6519, loss:5.755371570587158\n",
      "训练次数：6520, loss:5.602503776550293\n",
      "训练次数：6521, loss:5.17128324508667\n",
      "训练次数：6522, loss:4.88543176651001\n",
      "训练次数：6523, loss:6.624279022216797\n",
      "训练次数：6524, loss:5.669589519500732\n",
      "训练次数：6525, loss:5.271978378295898\n",
      "训练次数：6526, loss:4.542514324188232\n",
      "训练次数：6527, loss:6.420050144195557\n",
      "训练次数：6528, loss:6.086507320404053\n",
      "训练次数：6529, loss:4.420688152313232\n",
      "训练次数：6530, loss:5.709718227386475\n",
      "训练次数：6531, loss:5.127270698547363\n",
      "训练次数：6532, loss:5.652920246124268\n",
      "训练次数：6533, loss:7.062245845794678\n",
      "训练次数：6534, loss:7.377467632293701\n",
      "训练次数：6535, loss:8.516225814819336\n",
      "训练次数：6536, loss:6.31448221206665\n",
      "训练次数：6537, loss:6.828916072845459\n",
      "训练次数：6538, loss:6.751734256744385\n",
      "训练次数：6539, loss:4.330000877380371\n",
      "训练次数：6540, loss:6.965136528015137\n",
      "训练次数：6541, loss:4.551062107086182\n",
      "训练次数：6542, loss:6.229735851287842\n",
      "训练次数：6543, loss:5.227812767028809\n",
      "训练次数：6544, loss:7.134898662567139\n",
      "训练次数：6545, loss:6.959068298339844\n",
      "训练次数：6546, loss:5.9151153564453125\n",
      "训练次数：6547, loss:3.8167502880096436\n",
      "训练次数：6548, loss:5.940215587615967\n",
      "训练次数：6549, loss:6.520462989807129\n",
      "训练次数：6550, loss:5.477476119995117\n",
      "训练次数：6551, loss:5.656530380249023\n",
      "训练次数：6552, loss:4.166253089904785\n",
      "训练次数：6553, loss:6.388627052307129\n",
      "训练次数：6554, loss:5.160976886749268\n",
      "训练次数：6555, loss:4.773700714111328\n",
      "训练次数：6556, loss:4.977175712585449\n",
      "训练次数：6557, loss:3.7449493408203125\n",
      "训练次数：6558, loss:5.203814506530762\n",
      "训练次数：6559, loss:6.559123992919922\n",
      "训练次数：6560, loss:4.126644134521484\n",
      "训练次数：6561, loss:4.719745635986328\n",
      "训练次数：6562, loss:6.8886284828186035\n",
      "训练次数：6563, loss:4.491499423980713\n",
      "训练次数：6564, loss:4.74191951751709\n",
      "训练次数：6565, loss:7.248591899871826\n",
      "训练次数：6566, loss:7.272352695465088\n",
      "训练次数：6567, loss:4.655059337615967\n",
      "训练次数：6568, loss:4.4781928062438965\n",
      "训练次数：6569, loss:5.327973365783691\n",
      "训练次数：6570, loss:5.627419471740723\n",
      "训练次数：6571, loss:7.1215128898620605\n",
      "训练次数：6572, loss:4.916442394256592\n",
      "训练次数：6573, loss:5.417886257171631\n",
      "训练次数：6574, loss:4.954432010650635\n",
      "训练次数：6575, loss:5.530617713928223\n",
      "训练次数：6576, loss:5.545015811920166\n",
      "训练次数：6577, loss:4.702681541442871\n",
      "训练次数：6578, loss:5.015984535217285\n",
      "训练次数：6579, loss:4.212204456329346\n",
      "训练次数：6580, loss:5.62013578414917\n",
      "训练次数：6581, loss:5.128620147705078\n",
      "训练次数：6582, loss:5.44167423248291\n",
      "训练次数：6583, loss:4.086594581604004\n",
      "训练次数：6584, loss:5.041612148284912\n",
      "训练次数：6585, loss:5.5296196937561035\n",
      "训练次数：6586, loss:3.928978204727173\n",
      "训练次数：6587, loss:4.124199867248535\n",
      "训练次数：6588, loss:5.13886022567749\n",
      "训练次数：6589, loss:5.06485652923584\n",
      "训练次数：6590, loss:5.31854772567749\n",
      "训练次数：6591, loss:6.2389936447143555\n",
      "训练次数：6592, loss:8.54601001739502\n",
      "训练次数：6593, loss:5.0094990730285645\n",
      "训练次数：6594, loss:4.818925380706787\n",
      "训练次数：6595, loss:5.530153274536133\n",
      "训练次数：6596, loss:4.6406569480896\n",
      "训练次数：6597, loss:5.604823112487793\n",
      "训练次数：6598, loss:5.164267539978027\n",
      "训练次数：6599, loss:4.4775214195251465\n",
      "训练次数：6600, loss:4.940685272216797\n",
      "训练次数：6601, loss:5.047898292541504\n",
      "训练次数：6602, loss:4.171304702758789\n",
      "训练次数：6603, loss:4.1722636222839355\n",
      "训练次数：6604, loss:4.502449035644531\n",
      "训练次数：6605, loss:5.234611511230469\n",
      "训练次数：6606, loss:3.659494400024414\n",
      "训练次数：6607, loss:5.26025390625\n",
      "训练次数：6608, loss:6.28562593460083\n",
      "训练次数：6609, loss:3.9725735187530518\n",
      "训练次数：6610, loss:5.5777459144592285\n",
      "训练次数：6611, loss:3.892836809158325\n",
      "训练次数：6612, loss:5.383191108703613\n",
      "训练次数：6613, loss:6.136726379394531\n",
      "训练次数：6614, loss:6.6729655265808105\n",
      "训练次数：6615, loss:3.670869827270508\n",
      "训练次数：6616, loss:4.081443786621094\n",
      "训练次数：6617, loss:7.030937194824219\n",
      "训练次数：6618, loss:5.136268138885498\n",
      "训练次数：6619, loss:5.654883861541748\n",
      "训练次数：6620, loss:5.376769065856934\n",
      "训练次数：6621, loss:3.007474899291992\n",
      "训练次数：6622, loss:4.786262512207031\n",
      "训练次数：6623, loss:5.4256720542907715\n",
      "训练次数：6624, loss:5.612093448638916\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8712/2953061496.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m### 开始优化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moptimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0moptimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mtotal_train_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Miniconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\Miniconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "loss1 = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimer\n",
    "learning_rate = 0.01\n",
    "optimer = torch.optim.Adam(fanfan.parameters(),lr=learning_rate)\n",
    "\n",
    "# train_model parameters setting\n",
    "epoch = 10\n",
    "total_train_step = 0\n",
    "total_test_step = 0\n",
    "\n",
    "# tensorboard start\n",
    "writer = SummaryWriter('../loss_train')\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(\"----------第{}轮训练开始----------\".format(i))\n",
    "    \n",
    "    ## train start\n",
    "    for img, label in train_dataloader:\n",
    "        output = fanfan(img)\n",
    "        loss = loss1(output,label)\n",
    "\n",
    "        ### 开始优化\n",
    "        optimer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimer.step()\n",
    "\n",
    "        total_train_step += 1\n",
    "        if total_train_step % 50 == 0:\n",
    "            print(\"训练次数：{}, loss:{}\".format(total_train_step, loss.item()))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 更准确的说是验证，计算的是整个数据集的loss\n",
    "total_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for img, label in test_dataloader:\n",
    "        output = fanfan(img)\n",
    "        loss = loss1(output, label)\n",
    "        optimer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimer.step()\n",
    "        total_test_step += 1\n",
    "        total_test_loss += loss\n",
    "\n",
    "print(\"整体测试集上的Loss:{}\".format(total_test_loss))\n",
    "writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
    "total_test_step += 1\n",
    "\n",
    "torch.save(fanfan, \"fanfan_{}.pt\".format(i))\n",
    "print(\"model save already\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, AUC, Recall, F1, Pricision\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b749f76f95eccfe306708669d82109f7e5c80c589b5c5e4336c9e8a3ef8a3f2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
